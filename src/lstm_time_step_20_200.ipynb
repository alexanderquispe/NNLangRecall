{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with time_steps: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch Loss: 1.1178, Batch MAE: 0.8431\n",
      "Batch Loss: 0.8620, Batch MAE: 0.7288\n",
      "Batch Loss: 0.6354, Batch MAE: 0.5812\n",
      "Batch Loss: 0.4412, Batch MAE: 0.4015\n",
      "Batch Loss: 0.2713, Batch MAE: 0.2525\n",
      "Batch Loss: 0.2244, Batch MAE: 0.3060\n",
      "Batch Loss: 0.3261, Batch MAE: 0.2680\n",
      "Batch Loss: 0.2814, Batch MAE: 0.2167\n",
      "Batch Loss: 0.2127, Batch MAE: 0.2304\n",
      "Batch Loss: 0.1927, Batch MAE: 0.2542\n",
      "Batch Loss: 0.1936, Batch MAE: 0.2942\n",
      "Batch Loss: 0.2271, Batch MAE: 0.3465\n",
      "Batch Loss: 0.2330, Batch MAE: 0.3424\n",
      "Batch Loss: 0.2185, Batch MAE: 0.3077\n",
      "Batch Loss: 0.1993, Batch MAE: 0.2593\n",
      "Batch Loss: 0.1783, Batch MAE: 0.2166\n",
      "Batch Loss: 0.1816, Batch MAE: 0.2064\n",
      "Epoch 2/10\n",
      "Batch Loss: 0.1939, Batch MAE: 0.1855\n",
      "Batch Loss: 0.1757, Batch MAE: 0.1492\n",
      "Batch Loss: 0.2018, Batch MAE: 0.1690\n",
      "Batch Loss: 0.1866, Batch MAE: 0.1470\n",
      "Batch Loss: 0.1797, Batch MAE: 0.1550\n",
      "Batch Loss: 0.1711, Batch MAE: 0.1656\n",
      "Batch Loss: 0.1686, Batch MAE: 0.1900\n",
      "Batch Loss: 0.1597, Batch MAE: 0.2028\n",
      "Batch Loss: 0.1671, Batch MAE: 0.2224\n",
      "Batch Loss: 0.1634, Batch MAE: 0.2212\n",
      "Batch Loss: 0.1582, Batch MAE: 0.2144\n",
      "Batch Loss: 0.1713, Batch MAE: 0.2294\n",
      "Batch Loss: 0.1625, Batch MAE: 0.2107\n",
      "Batch Loss: 0.1521, Batch MAE: 0.1851\n",
      "Batch Loss: 0.1576, Batch MAE: 0.1668\n",
      "Batch Loss: 0.1503, Batch MAE: 0.1415\n",
      "Batch Loss: 0.1627, Batch MAE: 0.1460\n",
      "Epoch 3/10\n",
      "Batch Loss: 0.1674, Batch MAE: 0.1313\n",
      "Batch Loss: 0.1603, Batch MAE: 0.1261\n",
      "Batch Loss: 0.1813, Batch MAE: 0.1595\n",
      "Batch Loss: 0.1657, Batch MAE: 0.1552\n",
      "Batch Loss: 0.1617, Batch MAE: 0.1748\n",
      "Batch Loss: 0.1581, Batch MAE: 0.1869\n",
      "Batch Loss: 0.1601, Batch MAE: 0.2003\n",
      "Batch Loss: 0.1514, Batch MAE: 0.1987\n",
      "Batch Loss: 0.1578, Batch MAE: 0.2035\n",
      "Batch Loss: 0.1532, Batch MAE: 0.1977\n",
      "Batch Loss: 0.1442, Batch MAE: 0.1796\n",
      "Batch Loss: 0.1575, Batch MAE: 0.1875\n",
      "Batch Loss: 0.1477, Batch MAE: 0.1683\n",
      "Batch Loss: 0.1410, Batch MAE: 0.1527\n",
      "Batch Loss: 0.1502, Batch MAE: 0.1493\n",
      "Batch Loss: 0.1436, Batch MAE: 0.1365\n",
      "Batch Loss: 0.1552, Batch MAE: 0.1518\n",
      "Epoch 4/10\n",
      "Batch Loss: 0.1547, Batch MAE: 0.1535\n",
      "Batch Loss: 0.1501, Batch MAE: 0.1552\n",
      "Batch Loss: 0.1699, Batch MAE: 0.1858\n",
      "Batch Loss: 0.1550, Batch MAE: 0.1815\n",
      "Batch Loss: 0.1541, Batch MAE: 0.1914\n",
      "Batch Loss: 0.1520, Batch MAE: 0.1925\n",
      "Batch Loss: 0.1526, Batch MAE: 0.1938\n",
      "Batch Loss: 0.1441, Batch MAE: 0.1826\n",
      "Batch Loss: 0.1491, Batch MAE: 0.1802\n",
      "Batch Loss: 0.1448, Batch MAE: 0.1727\n",
      "Batch Loss: 0.1347, Batch MAE: 0.1545\n",
      "Batch Loss: 0.1503, Batch MAE: 0.1680\n",
      "Batch Loss: 0.1412, Batch MAE: 0.1549\n",
      "Batch Loss: 0.1340, Batch MAE: 0.1489\n",
      "Batch Loss: 0.1453, Batch MAE: 0.1563\n",
      "Batch Loss: 0.1369, Batch MAE: 0.1514\n",
      "Batch Loss: 0.1473, Batch MAE: 0.1705\n",
      "Epoch 5/10\n",
      "Batch Loss: 0.1485, Batch MAE: 0.1752\n",
      "Batch Loss: 0.1431, Batch MAE: 0.1742\n",
      "Batch Loss: 0.1616, Batch MAE: 0.1962\n",
      "Batch Loss: 0.1488, Batch MAE: 0.1851\n",
      "Batch Loss: 0.1477, Batch MAE: 0.1863\n",
      "Batch Loss: 0.1458, Batch MAE: 0.1805\n",
      "Batch Loss: 0.1458, Batch MAE: 0.1772\n",
      "Batch Loss: 0.1373, Batch MAE: 0.1659\n",
      "Batch Loss: 0.1422, Batch MAE: 0.1653\n",
      "Batch Loss: 0.1389, Batch MAE: 0.1617\n",
      "Batch Loss: 0.1294, Batch MAE: 0.1492\n",
      "Batch Loss: 0.1447, Batch MAE: 0.1687\n",
      "Batch Loss: 0.1345, Batch MAE: 0.1605\n",
      "Batch Loss: 0.1279, Batch MAE: 0.1573\n",
      "Batch Loss: 0.1389, Batch MAE: 0.1667\n",
      "Batch Loss: 0.1306, Batch MAE: 0.1614\n",
      "Batch Loss: 0.1420, Batch MAE: 0.1783\n",
      "Epoch 6/10\n",
      "Batch Loss: 0.1421, Batch MAE: 0.1790\n",
      "Batch Loss: 0.1378, Batch MAE: 0.1736\n",
      "Batch Loss: 0.1549, Batch MAE: 0.1919\n",
      "Batch Loss: 0.1430, Batch MAE: 0.1773\n",
      "Batch Loss: 0.1423, Batch MAE: 0.1772\n",
      "Batch Loss: 0.1394, Batch MAE: 0.1710\n",
      "Batch Loss: 0.1400, Batch MAE: 0.1687\n",
      "Batch Loss: 0.1310, Batch MAE: 0.1599\n",
      "Batch Loss: 0.1364, Batch MAE: 0.1621\n",
      "Batch Loss: 0.1330, Batch MAE: 0.1617\n",
      "Batch Loss: 0.1237, Batch MAE: 0.1519\n",
      "Batch Loss: 0.1385, Batch MAE: 0.1726\n",
      "Batch Loss: 0.1303, Batch MAE: 0.1648\n",
      "Batch Loss: 0.1226, Batch MAE: 0.1608\n",
      "Batch Loss: 0.1334, Batch MAE: 0.1686\n",
      "Batch Loss: 0.1251, Batch MAE: 0.1610\n",
      "Batch Loss: 0.1348, Batch MAE: 0.1759\n",
      "Epoch 7/10\n",
      "Batch Loss: 0.1352, Batch MAE: 0.1739\n",
      "Batch Loss: 0.1316, Batch MAE: 0.1666\n",
      "Batch Loss: 0.1503, Batch MAE: 0.1860\n",
      "Batch Loss: 0.1376, Batch MAE: 0.1721\n",
      "Batch Loss: 0.1370, Batch MAE: 0.1740\n",
      "Batch Loss: 0.1352, Batch MAE: 0.1701\n",
      "Batch Loss: 0.1335, Batch MAE: 0.1693\n",
      "Batch Loss: 0.1257, Batch MAE: 0.1612\n",
      "Batch Loss: 0.1307, Batch MAE: 0.1630\n",
      "Batch Loss: 0.1271, Batch MAE: 0.1618\n",
      "Batch Loss: 0.1178, Batch MAE: 0.1506\n",
      "Batch Loss: 0.1336, Batch MAE: 0.1705\n",
      "Batch Loss: 0.1242, Batch MAE: 0.1619\n",
      "Batch Loss: 0.1180, Batch MAE: 0.1577\n",
      "Batch Loss: 0.1271, Batch MAE: 0.1650\n",
      "Batch Loss: 0.1195, Batch MAE: 0.1580\n",
      "Batch Loss: 0.1307, Batch MAE: 0.1743\n",
      "Epoch 8/10\n",
      "Batch Loss: 0.1301, Batch MAE: 0.1728\n",
      "Batch Loss: 0.1266, Batch MAE: 0.1654\n",
      "Batch Loss: 0.1445, Batch MAE: 0.1852\n",
      "Batch Loss: 0.1317, Batch MAE: 0.1715\n",
      "Batch Loss: 0.1311, Batch MAE: 0.1727\n",
      "Batch Loss: 0.1292, Batch MAE: 0.1678\n",
      "Batch Loss: 0.1288, Batch MAE: 0.1658\n",
      "Batch Loss: 0.1208, Batch MAE: 0.1571\n",
      "Batch Loss: 0.1259, Batch MAE: 0.1588\n",
      "Batch Loss: 0.1228, Batch MAE: 0.1595\n",
      "Batch Loss: 0.1127, Batch MAE: 0.1500\n",
      "Batch Loss: 0.1280, Batch MAE: 0.1710\n",
      "Batch Loss: 0.1191, Batch MAE: 0.1628\n",
      "Batch Loss: 0.1125, Batch MAE: 0.1583\n",
      "Batch Loss: 0.1224, Batch MAE: 0.1648\n",
      "Batch Loss: 0.1144, Batch MAE: 0.1558\n",
      "Batch Loss: 0.1256, Batch MAE: 0.1704\n",
      "Epoch 9/10\n",
      "Batch Loss: 0.1245, Batch MAE: 0.1673\n",
      "Batch Loss: 0.1219, Batch MAE: 0.1586\n",
      "Batch Loss: 0.1399, Batch MAE: 0.1807\n",
      "Batch Loss: 0.1263, Batch MAE: 0.1698\n",
      "Batch Loss: 0.1258, Batch MAE: 0.1734\n",
      "Batch Loss: 0.1240, Batch MAE: 0.1692\n",
      "Batch Loss: 0.1241, Batch MAE: 0.1653\n",
      "Batch Loss: 0.1159, Batch MAE: 0.1546\n",
      "Batch Loss: 0.1220, Batch MAE: 0.1551\n",
      "Batch Loss: 0.1169, Batch MAE: 0.1563\n",
      "Batch Loss: 0.1080, Batch MAE: 0.1473\n",
      "Batch Loss: 0.1235, Batch MAE: 0.1707\n",
      "Batch Loss: 0.1152, Batch MAE: 0.1652\n",
      "Batch Loss: 0.1090, Batch MAE: 0.1608\n",
      "Batch Loss: 0.1172, Batch MAE: 0.1662\n",
      "Batch Loss: 0.1106, Batch MAE: 0.1546\n",
      "Batch Loss: 0.1202, Batch MAE: 0.1680\n",
      "Epoch 10/10\n",
      "Batch Loss: 0.1201, Batch MAE: 0.1630\n",
      "Batch Loss: 0.1170, Batch MAE: 0.1541\n",
      "Batch Loss: 0.1345, Batch MAE: 0.1807\n",
      "Batch Loss: 0.1222, Batch MAE: 0.1738\n",
      "Batch Loss: 0.1210, Batch MAE: 0.1778\n",
      "Batch Loss: 0.1198, Batch MAE: 0.1713\n",
      "Batch Loss: 0.1192, Batch MAE: 0.1628\n",
      "Batch Loss: 0.1113, Batch MAE: 0.1486\n",
      "Batch Loss: 0.1164, Batch MAE: 0.1488\n",
      "Batch Loss: 0.1141, Batch MAE: 0.1541\n",
      "Batch Loss: 0.1037, Batch MAE: 0.1496\n",
      "Batch Loss: 0.1189, Batch MAE: 0.1763\n",
      "Batch Loss: 0.1102, Batch MAE: 0.1710\n",
      "Batch Loss: 0.1042, Batch MAE: 0.1629\n",
      "Batch Loss: 0.1136, Batch MAE: 0.1630\n",
      "Batch Loss: 0.1069, Batch MAE: 0.1470\n",
      "Batch Loss: 0.1172, Batch MAE: 0.1606\n",
      "tf.Tensor(0.16279997, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1648243, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15909614, shape=(), dtype=float32)\n",
      "tf.Tensor(0.15873107, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16152689, shape=(), dtype=float32)\n",
      "Final Test MAE: 0.161527\n",
      "Training model with time_steps: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch Loss: 1.2090, Batch MAE: 0.8256\n",
      "Batch Loss: 0.8994, Batch MAE: 0.6947\n",
      "Batch Loss: 0.6591, Batch MAE: 0.5515\n",
      "Batch Loss: 0.4346, Batch MAE: 0.3738\n",
      "Batch Loss: 0.2765, Batch MAE: 0.2450\n",
      "Batch Loss: 0.2519, Batch MAE: 0.3200\n",
      "Batch Loss: 0.3043, Batch MAE: 0.2658\n",
      "Batch Loss: 0.3214, Batch MAE: 0.2422\n",
      "Batch Loss: 0.2504, Batch MAE: 0.2348\n",
      "Batch Loss: 0.2228, Batch MAE: 0.3266\n",
      "Batch Loss: 0.2418, Batch MAE: 0.3447\n",
      "Batch Loss: 0.2415, Batch MAE: 0.3265\n",
      "Batch Loss: 0.2208, Batch MAE: 0.2743\n",
      "Batch Loss: 0.2128, Batch MAE: 0.2607\n",
      "Epoch 2/10\n",
      "Batch Loss: 0.2278, Batch MAE: 0.2500\n",
      "Batch Loss: 0.1984, Batch MAE: 0.1997\n",
      "Batch Loss: 0.1928, Batch MAE: 0.1795\n",
      "Batch Loss: 0.2079, Batch MAE: 0.1801\n",
      "Batch Loss: 0.1971, Batch MAE: 0.1699\n",
      "Batch Loss: 0.1968, Batch MAE: 0.1748\n",
      "Batch Loss: 0.1713, Batch MAE: 0.1678\n",
      "Batch Loss: 0.1922, Batch MAE: 0.1969\n",
      "Batch Loss: 0.1914, Batch MAE: 0.1993\n",
      "Batch Loss: 0.1784, Batch MAE: 0.2188\n",
      "Batch Loss: 0.1718, Batch MAE: 0.2092\n",
      "Batch Loss: 0.1754, Batch MAE: 0.1959\n",
      "Batch Loss: 0.1766, Batch MAE: 0.1674\n",
      "Batch Loss: 0.1631, Batch MAE: 0.1519\n",
      "Epoch 3/10\n",
      "Batch Loss: 0.1953, Batch MAE: 0.1653\n",
      "Batch Loss: 0.1816, Batch MAE: 0.1535\n",
      "Batch Loss: 0.1766, Batch MAE: 0.1576\n",
      "Batch Loss: 0.1897, Batch MAE: 0.1851\n",
      "Batch Loss: 0.1803, Batch MAE: 0.1961\n",
      "Batch Loss: 0.1814, Batch MAE: 0.2122\n",
      "Batch Loss: 0.1644, Batch MAE: 0.1998\n",
      "Batch Loss: 0.1826, Batch MAE: 0.2135\n",
      "Batch Loss: 0.1819, Batch MAE: 0.2089\n",
      "Batch Loss: 0.1641, Batch MAE: 0.1884\n",
      "Batch Loss: 0.1579, Batch MAE: 0.1655\n",
      "Batch Loss: 0.1630, Batch MAE: 0.1554\n",
      "Batch Loss: 0.1683, Batch MAE: 0.1390\n",
      "Batch Loss: 0.1543, Batch MAE: 0.1280\n",
      "Epoch 4/10\n",
      "Batch Loss: 0.1857, Batch MAE: 0.1615\n",
      "Batch Loss: 0.1731, Batch MAE: 0.1649\n",
      "Batch Loss: 0.1685, Batch MAE: 0.1745\n",
      "Batch Loss: 0.1822, Batch MAE: 0.2024\n",
      "Batch Loss: 0.1745, Batch MAE: 0.2058\n",
      "Batch Loss: 0.1765, Batch MAE: 0.2127\n",
      "Batch Loss: 0.1570, Batch MAE: 0.1894\n",
      "Batch Loss: 0.1740, Batch MAE: 0.1965\n",
      "Batch Loss: 0.1765, Batch MAE: 0.1909\n",
      "Batch Loss: 0.1566, Batch MAE: 0.1631\n",
      "Batch Loss: 0.1503, Batch MAE: 0.1467\n",
      "Batch Loss: 0.1580, Batch MAE: 0.1501\n",
      "Batch Loss: 0.1621, Batch MAE: 0.1488\n",
      "Batch Loss: 0.1479, Batch MAE: 0.1441\n",
      "Epoch 5/10\n",
      "Batch Loss: 0.1757, Batch MAE: 0.1818\n",
      "Batch Loss: 0.1652, Batch MAE: 0.1810\n",
      "Batch Loss: 0.1624, Batch MAE: 0.1827\n",
      "Batch Loss: 0.1768, Batch MAE: 0.2015\n",
      "Batch Loss: 0.1679, Batch MAE: 0.1966\n",
      "Batch Loss: 0.1707, Batch MAE: 0.1971\n",
      "Batch Loss: 0.1494, Batch MAE: 0.1712\n",
      "Batch Loss: 0.1692, Batch MAE: 0.1806\n",
      "Batch Loss: 0.1707, Batch MAE: 0.1806\n",
      "Batch Loss: 0.1498, Batch MAE: 0.1593\n",
      "Batch Loss: 0.1449, Batch MAE: 0.1502\n",
      "Batch Loss: 0.1512, Batch MAE: 0.1599\n",
      "Batch Loss: 0.1549, Batch MAE: 0.1617\n",
      "Batch Loss: 0.1428, Batch MAE: 0.1552\n",
      "Epoch 6/10\n",
      "Batch Loss: 0.1674, Batch MAE: 0.1885\n",
      "Batch Loss: 0.1596, Batch MAE: 0.1822\n",
      "Batch Loss: 0.1576, Batch MAE: 0.1776\n",
      "Batch Loss: 0.1707, Batch MAE: 0.1918\n",
      "Batch Loss: 0.1622, Batch MAE: 0.1846\n",
      "Batch Loss: 0.1636, Batch MAE: 0.1856\n",
      "Batch Loss: 0.1440, Batch MAE: 0.1631\n",
      "Batch Loss: 0.1642, Batch MAE: 0.1777\n",
      "Batch Loss: 0.1645, Batch MAE: 0.1824\n",
      "Batch Loss: 0.1459, Batch MAE: 0.1663\n",
      "Batch Loss: 0.1386, Batch MAE: 0.1587\n",
      "Batch Loss: 0.1456, Batch MAE: 0.1668\n",
      "Batch Loss: 0.1486, Batch MAE: 0.1645\n",
      "Batch Loss: 0.1371, Batch MAE: 0.1541\n",
      "Epoch 7/10\n",
      "Batch Loss: 0.1618, Batch MAE: 0.1834\n",
      "Batch Loss: 0.1537, Batch MAE: 0.1749\n",
      "Batch Loss: 0.1511, Batch MAE: 0.1697\n",
      "Batch Loss: 0.1643, Batch MAE: 0.1856\n",
      "Batch Loss: 0.1562, Batch MAE: 0.1814\n",
      "Batch Loss: 0.1594, Batch MAE: 0.1849\n",
      "Batch Loss: 0.1386, Batch MAE: 0.1641\n",
      "Batch Loss: 0.1571, Batch MAE: 0.1790\n",
      "Batch Loss: 0.1585, Batch MAE: 0.1836\n",
      "Batch Loss: 0.1396, Batch MAE: 0.1659\n",
      "Batch Loss: 0.1339, Batch MAE: 0.1570\n",
      "Batch Loss: 0.1405, Batch MAE: 0.1647\n",
      "Batch Loss: 0.1449, Batch MAE: 0.1631\n",
      "Batch Loss: 0.1309, Batch MAE: 0.1533\n",
      "Epoch 8/10\n",
      "Batch Loss: 0.1573, Batch MAE: 0.1836\n",
      "Batch Loss: 0.1478, Batch MAE: 0.1753\n",
      "Batch Loss: 0.1455, Batch MAE: 0.1700\n",
      "Batch Loss: 0.1592, Batch MAE: 0.1856\n",
      "Batch Loss: 0.1515, Batch MAE: 0.1803\n",
      "Batch Loss: 0.1547, Batch MAE: 0.1830\n",
      "Batch Loss: 0.1341, Batch MAE: 0.1614\n",
      "Batch Loss: 0.1523, Batch MAE: 0.1763\n",
      "Batch Loss: 0.1540, Batch MAE: 0.1826\n",
      "Batch Loss: 0.1354, Batch MAE: 0.1649\n",
      "Batch Loss: 0.1286, Batch MAE: 0.1576\n",
      "Batch Loss: 0.1360, Batch MAE: 0.1664\n",
      "Batch Loss: 0.1378, Batch MAE: 0.1642\n",
      "Batch Loss: 0.1271, Batch MAE: 0.1530\n",
      "Epoch 9/10\n",
      "Batch Loss: 0.1512, Batch MAE: 0.1822\n",
      "Batch Loss: 0.1421, Batch MAE: 0.1720\n",
      "Batch Loss: 0.1392, Batch MAE: 0.1644\n",
      "Batch Loss: 0.1534, Batch MAE: 0.1792\n",
      "Batch Loss: 0.1472, Batch MAE: 0.1742\n",
      "Batch Loss: 0.1486, Batch MAE: 0.1783\n",
      "Batch Loss: 0.1283, Batch MAE: 0.1579\n",
      "Batch Loss: 0.1487, Batch MAE: 0.1760\n",
      "Batch Loss: 0.1506, Batch MAE: 0.1853\n",
      "Batch Loss: 0.1304, Batch MAE: 0.1675\n",
      "Batch Loss: 0.1246, Batch MAE: 0.1596\n",
      "Batch Loss: 0.1321, Batch MAE: 0.1673\n",
      "Batch Loss: 0.1337, Batch MAE: 0.1632\n",
      "Batch Loss: 0.1218, Batch MAE: 0.1496\n",
      "Epoch 10/10\n",
      "Batch Loss: 0.1470, Batch MAE: 0.1787\n",
      "Batch Loss: 0.1387, Batch MAE: 0.1685\n",
      "Batch Loss: 0.1348, Batch MAE: 0.1617\n",
      "Batch Loss: 0.1483, Batch MAE: 0.1792\n",
      "Batch Loss: 0.1413, Batch MAE: 0.1767\n",
      "Batch Loss: 0.1435, Batch MAE: 0.1821\n",
      "Batch Loss: 0.1243, Batch MAE: 0.1602\n",
      "Batch Loss: 0.1425, Batch MAE: 0.1756\n",
      "Batch Loss: 0.1457, Batch MAE: 0.1835\n",
      "Batch Loss: 0.1262, Batch MAE: 0.1634\n",
      "Batch Loss: 0.1203, Batch MAE: 0.1553\n",
      "Batch Loss: 0.1277, Batch MAE: 0.1642\n",
      "Batch Loss: 0.1287, Batch MAE: 0.1619\n",
      "Batch Loss: 0.1169, Batch MAE: 0.1506\n",
      "tf.Tensor(0.16323607, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16970399, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16807811, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16735059, shape=(), dtype=float32)\n",
      "Final Test MAE: 0.167351\n",
      "Training model with time_steps: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch Loss: 1.0444, Batch MAE: 0.7741\n",
      "Batch Loss: 0.7578, Batch MAE: 0.6020\n",
      "Batch Loss: 0.5049, Batch MAE: 0.3985\n",
      "Batch Loss: 0.2884, Batch MAE: 0.2007\n",
      "Batch Loss: 0.2212, Batch MAE: 0.3542\n",
      "Batch Loss: 0.3091, Batch MAE: 0.3000\n",
      "Batch Loss: 0.3208, Batch MAE: 0.2301\n",
      "Batch Loss: 0.2496, Batch MAE: 0.2127\n",
      "Batch Loss: 0.2074, Batch MAE: 0.2944\n",
      "Batch Loss: 0.2289, Batch MAE: 0.3321\n",
      "Batch Loss: 0.2017, Batch MAE: 0.2696\n",
      "Batch Loss: 0.2310, Batch MAE: 0.3010\n",
      "Epoch 2/10\n",
      "Batch Loss: 0.2155, Batch MAE: 0.2635\n",
      "Batch Loss: 0.2094, Batch MAE: 0.2470\n",
      "Batch Loss: 0.1995, Batch MAE: 0.2179\n",
      "Batch Loss: 0.1825, Batch MAE: 0.1817\n",
      "Batch Loss: 0.1953, Batch MAE: 0.1843\n",
      "Batch Loss: 0.1723, Batch MAE: 0.1518\n",
      "Batch Loss: 0.1863, Batch MAE: 0.1628\n",
      "Batch Loss: 0.1969, Batch MAE: 0.1681\n",
      "Batch Loss: 0.1813, Batch MAE: 0.1806\n",
      "Batch Loss: 0.1789, Batch MAE: 0.1782\n",
      "Batch Loss: 0.1742, Batch MAE: 0.1346\n",
      "Batch Loss: 0.1814, Batch MAE: 0.1616\n",
      "Epoch 3/10\n",
      "Batch Loss: 0.1848, Batch MAE: 0.1477\n",
      "Batch Loss: 0.1884, Batch MAE: 0.1545\n",
      "Batch Loss: 0.1825, Batch MAE: 0.1603\n",
      "Batch Loss: 0.1677, Batch MAE: 0.1630\n",
      "Batch Loss: 0.1818, Batch MAE: 0.1985\n",
      "Batch Loss: 0.1655, Batch MAE: 0.1916\n",
      "Batch Loss: 0.1781, Batch MAE: 0.2097\n",
      "Batch Loss: 0.1849, Batch MAE: 0.2172\n",
      "Batch Loss: 0.1789, Batch MAE: 0.2182\n",
      "Batch Loss: 0.1788, Batch MAE: 0.2078\n",
      "Batch Loss: 0.1624, Batch MAE: 0.1526\n",
      "Batch Loss: 0.1651, Batch MAE: 0.1459\n",
      "Epoch 4/10\n",
      "Batch Loss: 0.1665, Batch MAE: 0.1126\n",
      "Batch Loss: 0.1779, Batch MAE: 0.1144\n",
      "Batch Loss: 0.1791, Batch MAE: 0.1103\n",
      "Batch Loss: 0.1653, Batch MAE: 0.1055\n",
      "Batch Loss: 0.1779, Batch MAE: 0.1443\n",
      "Batch Loss: 0.1537, Batch MAE: 0.1427\n",
      "Batch Loss: 0.1667, Batch MAE: 0.1733\n",
      "Batch Loss: 0.1753, Batch MAE: 0.1981\n",
      "Batch Loss: 0.1651, Batch MAE: 0.1981\n",
      "Batch Loss: 0.1676, Batch MAE: 0.1998\n",
      "Batch Loss: 0.1596, Batch MAE: 0.1755\n",
      "Batch Loss: 0.1605, Batch MAE: 0.1686\n",
      "Epoch 5/10\n",
      "Batch Loss: 0.1607, Batch MAE: 0.1491\n",
      "Batch Loss: 0.1677, Batch MAE: 0.1396\n",
      "Batch Loss: 0.1672, Batch MAE: 0.1265\n",
      "Batch Loss: 0.1569, Batch MAE: 0.1100\n",
      "Batch Loss: 0.1720, Batch MAE: 0.1320\n",
      "Batch Loss: 0.1500, Batch MAE: 0.1174\n",
      "Batch Loss: 0.1625, Batch MAE: 0.1428\n",
      "Batch Loss: 0.1724, Batch MAE: 0.1709\n",
      "Batch Loss: 0.1575, Batch MAE: 0.1712\n",
      "Batch Loss: 0.1606, Batch MAE: 0.1811\n",
      "Batch Loss: 0.1542, Batch MAE: 0.1739\n",
      "Batch Loss: 0.1567, Batch MAE: 0.1739\n",
      "Epoch 6/10\n",
      "Batch Loss: 0.1572, Batch MAE: 0.1681\n",
      "Batch Loss: 0.1642, Batch MAE: 0.1610\n",
      "Batch Loss: 0.1620, Batch MAE: 0.1465\n",
      "Batch Loss: 0.1518, Batch MAE: 0.1236\n",
      "Batch Loss: 0.1661, Batch MAE: 0.1351\n",
      "Batch Loss: 0.1451, Batch MAE: 0.1114\n",
      "Batch Loss: 0.1583, Batch MAE: 0.1308\n",
      "Batch Loss: 0.1696, Batch MAE: 0.1565\n",
      "Batch Loss: 0.1542, Batch MAE: 0.1590\n",
      "Batch Loss: 0.1558, Batch MAE: 0.1714\n",
      "Batch Loss: 0.1482, Batch MAE: 0.1660\n",
      "Batch Loss: 0.1500, Batch MAE: 0.1722\n",
      "Epoch 7/10\n",
      "Batch Loss: 0.1525, Batch MAE: 0.1702\n",
      "Batch Loss: 0.1605, Batch MAE: 0.1682\n",
      "Batch Loss: 0.1588, Batch MAE: 0.1561\n",
      "Batch Loss: 0.1477, Batch MAE: 0.1340\n",
      "Batch Loss: 0.1607, Batch MAE: 0.1435\n",
      "Batch Loss: 0.1401, Batch MAE: 0.1161\n",
      "Batch Loss: 0.1535, Batch MAE: 0.1306\n",
      "Batch Loss: 0.1648, Batch MAE: 0.1516\n",
      "Batch Loss: 0.1491, Batch MAE: 0.1542\n",
      "Batch Loss: 0.1504, Batch MAE: 0.1645\n",
      "Batch Loss: 0.1436, Batch MAE: 0.1551\n",
      "Batch Loss: 0.1450, Batch MAE: 0.1643\n",
      "Epoch 8/10\n",
      "Batch Loss: 0.1468, Batch MAE: 0.1621\n",
      "Batch Loss: 0.1543, Batch MAE: 0.1650\n",
      "Batch Loss: 0.1537, Batch MAE: 0.1571\n",
      "Batch Loss: 0.1435, Batch MAE: 0.1388\n",
      "Batch Loss: 0.1556, Batch MAE: 0.1493\n",
      "Batch Loss: 0.1358, Batch MAE: 0.1223\n",
      "Batch Loss: 0.1484, Batch MAE: 0.1346\n",
      "Batch Loss: 0.1598, Batch MAE: 0.1530\n",
      "Batch Loss: 0.1450, Batch MAE: 0.1541\n",
      "Batch Loss: 0.1473, Batch MAE: 0.1622\n",
      "Batch Loss: 0.1397, Batch MAE: 0.1495\n",
      "Batch Loss: 0.1409, Batch MAE: 0.1582\n",
      "Epoch 9/10\n",
      "Batch Loss: 0.1420, Batch MAE: 0.1548\n",
      "Batch Loss: 0.1504, Batch MAE: 0.1590\n",
      "Batch Loss: 0.1501, Batch MAE: 0.1526\n",
      "Batch Loss: 0.1393, Batch MAE: 0.1366\n",
      "Batch Loss: 0.1513, Batch MAE: 0.1495\n",
      "Batch Loss: 0.1308, Batch MAE: 0.1252\n",
      "Batch Loss: 0.1444, Batch MAE: 0.1386\n",
      "Batch Loss: 0.1558, Batch MAE: 0.1575\n",
      "Batch Loss: 0.1403, Batch MAE: 0.1568\n",
      "Batch Loss: 0.1421, Batch MAE: 0.1639\n",
      "Batch Loss: 0.1349, Batch MAE: 0.1512\n",
      "Batch Loss: 0.1361, Batch MAE: 0.1566\n",
      "Epoch 10/10\n",
      "Batch Loss: 0.1368, Batch MAE: 0.1511\n",
      "Batch Loss: 0.1464, Batch MAE: 0.1525\n",
      "Batch Loss: 0.1431, Batch MAE: 0.1455\n",
      "Batch Loss: 0.1351, Batch MAE: 0.1302\n",
      "Batch Loss: 0.1465, Batch MAE: 0.1449\n",
      "Batch Loss: 0.1273, Batch MAE: 0.1239\n",
      "Batch Loss: 0.1396, Batch MAE: 0.1407\n",
      "Batch Loss: 0.1499, Batch MAE: 0.1633\n",
      "Batch Loss: 0.1374, Batch MAE: 0.1621\n",
      "Batch Loss: 0.1376, Batch MAE: 0.1696\n",
      "Batch Loss: 0.1315, Batch MAE: 0.1581\n",
      "Batch Loss: 0.1323, Batch MAE: 0.1577\n",
      "tf.Tensor(0.17508551, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17302057, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17162506, shape=(), dtype=float32)\n",
      "Final Test MAE: 0.171625\n",
      "Training model with time_steps: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch Loss: 1.0080, Batch MAE: 0.7828\n",
      "Batch Loss: 0.7758, Batch MAE: 0.6616\n",
      "Batch Loss: 0.5741, Batch MAE: 0.4976\n",
      "Batch Loss: 0.3690, Batch MAE: 0.3203\n",
      "Batch Loss: 0.2445, Batch MAE: 0.3192\n",
      "Batch Loss: 0.4199, Batch MAE: 0.2551\n",
      "Batch Loss: 0.2735, Batch MAE: 0.2446\n",
      "Batch Loss: 0.2189, Batch MAE: 0.2900\n",
      "Batch Loss: 0.2104, Batch MAE: 0.2683\n",
      "Batch Loss: 0.1952, Batch MAE: 0.2426\n",
      "Epoch 2/10\n",
      "Batch Loss: 0.2303, Batch MAE: 0.2643\n",
      "Batch Loss: 0.2028, Batch MAE: 0.2397\n",
      "Batch Loss: 0.1912, Batch MAE: 0.2256\n",
      "Batch Loss: 0.1946, Batch MAE: 0.2303\n",
      "Batch Loss: 0.1782, Batch MAE: 0.2150\n",
      "Batch Loss: 0.1741, Batch MAE: 0.2059\n",
      "Batch Loss: 0.1863, Batch MAE: 0.2125\n",
      "Batch Loss: 0.1829, Batch MAE: 0.2269\n",
      "Batch Loss: 0.1770, Batch MAE: 0.1927\n",
      "Batch Loss: 0.1576, Batch MAE: 0.1465\n",
      "Epoch 3/10\n",
      "Batch Loss: 0.1929, Batch MAE: 0.1645\n",
      "Batch Loss: 0.1913, Batch MAE: 0.1532\n",
      "Batch Loss: 0.1811, Batch MAE: 0.1465\n",
      "Batch Loss: 0.1842, Batch MAE: 0.1732\n",
      "Batch Loss: 0.1598, Batch MAE: 0.1767\n",
      "Batch Loss: 0.1569, Batch MAE: 0.1932\n",
      "Batch Loss: 0.1752, Batch MAE: 0.2212\n",
      "Batch Loss: 0.1717, Batch MAE: 0.2349\n",
      "Batch Loss: 0.1714, Batch MAE: 0.2248\n",
      "Batch Loss: 0.1535, Batch MAE: 0.1848\n",
      "Epoch 4/10\n",
      "Batch Loss: 0.1697, Batch MAE: 0.1814\n",
      "Batch Loss: 0.1697, Batch MAE: 0.1674\n",
      "Batch Loss: 0.1646, Batch MAE: 0.1452\n",
      "Batch Loss: 0.1743, Batch MAE: 0.1517\n",
      "Batch Loss: 0.1524, Batch MAE: 0.1289\n",
      "Batch Loss: 0.1504, Batch MAE: 0.1315\n",
      "Batch Loss: 0.1701, Batch MAE: 0.1596\n",
      "Batch Loss: 0.1555, Batch MAE: 0.1686\n",
      "Batch Loss: 0.1590, Batch MAE: 0.1779\n",
      "Batch Loss: 0.1398, Batch MAE: 0.1582\n",
      "Epoch 5/10\n",
      "Batch Loss: 0.1628, Batch MAE: 0.1796\n",
      "Batch Loss: 0.1647, Batch MAE: 0.1844\n",
      "Batch Loss: 0.1598, Batch MAE: 0.1786\n",
      "Batch Loss: 0.1689, Batch MAE: 0.1892\n",
      "Batch Loss: 0.1488, Batch MAE: 0.1662\n",
      "Batch Loss: 0.1450, Batch MAE: 0.1579\n",
      "Batch Loss: 0.1646, Batch MAE: 0.1728\n",
      "Batch Loss: 0.1507, Batch MAE: 0.1627\n",
      "Batch Loss: 0.1548, Batch MAE: 0.1655\n",
      "Batch Loss: 0.1350, Batch MAE: 0.1398\n",
      "Epoch 6/10\n",
      "Batch Loss: 0.1569, Batch MAE: 0.1587\n",
      "Batch Loss: 0.1590, Batch MAE: 0.1637\n",
      "Batch Loss: 0.1553, Batch MAE: 0.1605\n",
      "Batch Loss: 0.1650, Batch MAE: 0.1782\n",
      "Batch Loss: 0.1443, Batch MAE: 0.1621\n",
      "Batch Loss: 0.1415, Batch MAE: 0.1613\n",
      "Batch Loss: 0.1596, Batch MAE: 0.1820\n",
      "Batch Loss: 0.1471, Batch MAE: 0.1729\n",
      "Batch Loss: 0.1501, Batch MAE: 0.1769\n",
      "Batch Loss: 0.1328, Batch MAE: 0.1515\n",
      "Epoch 7/10\n",
      "Batch Loss: 0.1520, Batch MAE: 0.1660\n",
      "Batch Loss: 0.1563, Batch MAE: 0.1638\n",
      "Batch Loss: 0.1511, Batch MAE: 0.1548\n",
      "Batch Loss: 0.1610, Batch MAE: 0.1682\n",
      "Batch Loss: 0.1393, Batch MAE: 0.1485\n",
      "Batch Loss: 0.1365, Batch MAE: 0.1477\n",
      "Batch Loss: 0.1563, Batch MAE: 0.1718\n",
      "Batch Loss: 0.1425, Batch MAE: 0.1672\n",
      "Batch Loss: 0.1466, Batch MAE: 0.1760\n",
      "Batch Loss: 0.1285, Batch MAE: 0.1547\n",
      "Epoch 8/10\n",
      "Batch Loss: 0.1477, Batch MAE: 0.1717\n",
      "Batch Loss: 0.1507, Batch MAE: 0.1696\n",
      "Batch Loss: 0.1474, Batch MAE: 0.1594\n",
      "Batch Loss: 0.1564, Batch MAE: 0.1698\n",
      "Batch Loss: 0.1355, Batch MAE: 0.1474\n",
      "Batch Loss: 0.1322, Batch MAE: 0.1440\n",
      "Batch Loss: 0.1533, Batch MAE: 0.1675\n",
      "Batch Loss: 0.1384, Batch MAE: 0.1636\n",
      "Batch Loss: 0.1426, Batch MAE: 0.1730\n",
      "Batch Loss: 0.1252, Batch MAE: 0.1513\n",
      "Epoch 9/10\n",
      "Batch Loss: 0.1438, Batch MAE: 0.1685\n",
      "Batch Loss: 0.1469, Batch MAE: 0.1674\n",
      "Batch Loss: 0.1425, Batch MAE: 0.1582\n",
      "Batch Loss: 0.1519, Batch MAE: 0.1701\n",
      "Batch Loss: 0.1316, Batch MAE: 0.1484\n",
      "Batch Loss: 0.1284, Batch MAE: 0.1449\n",
      "Batch Loss: 0.1483, Batch MAE: 0.1679\n",
      "Batch Loss: 0.1341, Batch MAE: 0.1642\n",
      "Batch Loss: 0.1382, Batch MAE: 0.1723\n",
      "Batch Loss: 0.1204, Batch MAE: 0.1486\n",
      "Epoch 10/10\n",
      "Batch Loss: 0.1398, Batch MAE: 0.1640\n",
      "Batch Loss: 0.1424, Batch MAE: 0.1627\n",
      "Batch Loss: 0.1391, Batch MAE: 0.1544\n",
      "Batch Loss: 0.1476, Batch MAE: 0.1682\n",
      "Batch Loss: 0.1268, Batch MAE: 0.1480\n",
      "Batch Loss: 0.1245, Batch MAE: 0.1455\n",
      "Batch Loss: 0.1444, Batch MAE: 0.1690\n",
      "Batch Loss: 0.1308, Batch MAE: 0.1653\n",
      "Batch Loss: 0.1345, Batch MAE: 0.1719\n",
      "Batch Loss: 0.1167, Batch MAE: 0.1461\n",
      "tf.Tensor(0.16694278, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16727188, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16880393, shape=(), dtype=float32)\n",
      "Final Test MAE: 0.168804\n",
      "Training model with time_steps: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch Loss: 0.9828, Batch MAE: 0.7630\n",
      "Batch Loss: 0.7550, Batch MAE: 0.6340\n",
      "Batch Loss: 0.5199, Batch MAE: 0.4402\n",
      "Batch Loss: 0.3328, Batch MAE: 0.2710\n",
      "Batch Loss: 0.2354, Batch MAE: 0.2943\n",
      "Batch Loss: 0.4942, Batch MAE: 0.2344\n",
      "Batch Loss: 0.2104, Batch MAE: 0.2775\n",
      "Batch Loss: 0.1953, Batch MAE: 0.2574\n",
      "Batch Loss: 0.2088, Batch MAE: 0.2778\n",
      "Epoch 2/10\n",
      "Batch Loss: 0.2217, Batch MAE: 0.2803\n",
      "Batch Loss: 0.1990, Batch MAE: 0.2682\n",
      "Batch Loss: 0.1986, Batch MAE: 0.2590\n",
      "Batch Loss: 0.1993, Batch MAE: 0.2548\n",
      "Batch Loss: 0.1983, Batch MAE: 0.2479\n",
      "Batch Loss: 0.1944, Batch MAE: 0.2349\n",
      "Batch Loss: 0.1936, Batch MAE: 0.2392\n",
      "Batch Loss: 0.1761, Batch MAE: 0.1904\n",
      "Batch Loss: 0.1730, Batch MAE: 0.1693\n",
      "Epoch 3/10\n",
      "Batch Loss: 0.1969, Batch MAE: 0.1632\n",
      "Batch Loss: 0.1801, Batch MAE: 0.1313\n",
      "Batch Loss: 0.1870, Batch MAE: 0.1362\n",
      "Batch Loss: 0.1839, Batch MAE: 0.1255\n",
      "Batch Loss: 0.1890, Batch MAE: 0.1401\n",
      "Batch Loss: 0.1810, Batch MAE: 0.1460\n",
      "Batch Loss: 0.1677, Batch MAE: 0.1762\n",
      "Batch Loss: 0.1634, Batch MAE: 0.1714\n",
      "Batch Loss: 0.1668, Batch MAE: 0.1799\n",
      "Epoch 4/10\n",
      "Batch Loss: 0.1853, Batch MAE: 0.1952\n",
      "Batch Loss: 0.1702, Batch MAE: 0.1916\n",
      "Batch Loss: 0.1734, Batch MAE: 0.1966\n",
      "Batch Loss: 0.1709, Batch MAE: 0.1987\n",
      "Batch Loss: 0.1762, Batch MAE: 0.2064\n",
      "Batch Loss: 0.1724, Batch MAE: 0.2038\n",
      "Batch Loss: 0.1685, Batch MAE: 0.2038\n",
      "Batch Loss: 0.1627, Batch MAE: 0.1866\n",
      "Batch Loss: 0.1644, Batch MAE: 0.1750\n",
      "Epoch 5/10\n",
      "Batch Loss: 0.1796, Batch MAE: 0.1763\n",
      "Batch Loss: 0.1662, Batch MAE: 0.1542\n",
      "Batch Loss: 0.1703, Batch MAE: 0.1492\n",
      "Batch Loss: 0.1667, Batch MAE: 0.1423\n",
      "Batch Loss: 0.1739, Batch MAE: 0.1545\n",
      "Batch Loss: 0.1682, Batch MAE: 0.1530\n",
      "Batch Loss: 0.1603, Batch MAE: 0.1636\n",
      "Batch Loss: 0.1579, Batch MAE: 0.1583\n",
      "Batch Loss: 0.1571, Batch MAE: 0.1596\n",
      "Epoch 6/10\n",
      "Batch Loss: 0.1769, Batch MAE: 0.1772\n",
      "Batch Loss: 0.1631, Batch MAE: 0.1679\n",
      "Batch Loss: 0.1659, Batch MAE: 0.1738\n",
      "Batch Loss: 0.1635, Batch MAE: 0.1742\n",
      "Batch Loss: 0.1684, Batch MAE: 0.1859\n",
      "Batch Loss: 0.1651, Batch MAE: 0.1854\n",
      "Batch Loss: 0.1573, Batch MAE: 0.1860\n",
      "Batch Loss: 0.1551, Batch MAE: 0.1782\n",
      "Batch Loss: 0.1562, Batch MAE: 0.1719\n",
      "Epoch 7/10\n",
      "Batch Loss: 0.1727, Batch MAE: 0.1840\n",
      "Batch Loss: 0.1599, Batch MAE: 0.1658\n",
      "Batch Loss: 0.1624, Batch MAE: 0.1641\n",
      "Batch Loss: 0.1600, Batch MAE: 0.1572\n",
      "Batch Loss: 0.1669, Batch MAE: 0.1668\n",
      "Batch Loss: 0.1615, Batch MAE: 0.1639\n",
      "Batch Loss: 0.1534, Batch MAE: 0.1664\n",
      "Batch Loss: 0.1508, Batch MAE: 0.1618\n",
      "Batch Loss: 0.1511, Batch MAE: 0.1600\n",
      "Epoch 8/10\n",
      "Batch Loss: 0.1691, Batch MAE: 0.1783\n",
      "Batch Loss: 0.1555, Batch MAE: 0.1657\n",
      "Batch Loss: 0.1590, Batch MAE: 0.1695\n",
      "Batch Loss: 0.1551, Batch MAE: 0.1667\n",
      "Batch Loss: 0.1630, Batch MAE: 0.1777\n",
      "Batch Loss: 0.1581, Batch MAE: 0.1762\n",
      "Batch Loss: 0.1514, Batch MAE: 0.1760\n",
      "Batch Loss: 0.1482, Batch MAE: 0.1710\n",
      "Batch Loss: 0.1477, Batch MAE: 0.1661\n",
      "Epoch 9/10\n",
      "Batch Loss: 0.1656, Batch MAE: 0.1823\n",
      "Batch Loss: 0.1531, Batch MAE: 0.1660\n",
      "Batch Loss: 0.1568, Batch MAE: 0.1665\n",
      "Batch Loss: 0.1518, Batch MAE: 0.1604\n",
      "Batch Loss: 0.1593, Batch MAE: 0.1703\n",
      "Batch Loss: 0.1543, Batch MAE: 0.1677\n",
      "Batch Loss: 0.1476, Batch MAE: 0.1682\n",
      "Batch Loss: 0.1445, Batch MAE: 0.1645\n",
      "Batch Loss: 0.1455, Batch MAE: 0.1615\n",
      "Epoch 10/10\n",
      "Batch Loss: 0.1614, Batch MAE: 0.1805\n",
      "Batch Loss: 0.1494, Batch MAE: 0.1671\n",
      "Batch Loss: 0.1528, Batch MAE: 0.1699\n",
      "Batch Loss: 0.1489, Batch MAE: 0.1654\n",
      "Batch Loss: 0.1562, Batch MAE: 0.1758\n",
      "Batch Loss: 0.1515, Batch MAE: 0.1736\n",
      "Batch Loss: 0.1445, Batch MAE: 0.1725\n",
      "Batch Loss: 0.1417, Batch MAE: 0.1682\n",
      "Batch Loss: 0.1416, Batch MAE: 0.1636\n",
      "tf.Tensor(0.17151025, shape=(), dtype=float32)\n",
      "tf.Tensor(0.16944382, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1691811, shape=(), dtype=float32)\n",
      "Final Test MAE: 0.169181\n",
      "Training model with time_steps: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch Loss: 1.0297, Batch MAE: 0.7870\n",
      "Batch Loss: 0.7725, Batch MAE: 0.6767\n",
      "Batch Loss: 0.5680, Batch MAE: 0.5276\n",
      "Batch Loss: 0.3983, Batch MAE: 0.3686\n",
      "Batch Loss: 0.2537, Batch MAE: 0.2394\n",
      "Batch Loss: 0.2403, Batch MAE: 0.2893\n",
      "Batch Loss: 0.3190, Batch MAE: 0.2947\n",
      "Batch Loss: 0.3697, Batch MAE: 0.2652\n",
      "Epoch 2/10\n",
      "Batch Loss: 0.4013, Batch MAE: 0.2674\n",
      "Batch Loss: 0.2282, Batch MAE: 0.2491\n",
      "Batch Loss: 0.2104, Batch MAE: 0.3008\n",
      "Batch Loss: 0.2152, Batch MAE: 0.3435\n",
      "Batch Loss: 0.2256, Batch MAE: 0.3640\n",
      "Batch Loss: 0.2741, Batch MAE: 0.4108\n",
      "Batch Loss: 0.2705, Batch MAE: 0.3963\n",
      "Batch Loss: 0.2576, Batch MAE: 0.3698\n",
      "Epoch 3/10\n",
      "Batch Loss: 0.2384, Batch MAE: 0.3348\n",
      "Batch Loss: 0.2235, Batch MAE: 0.3083\n",
      "Batch Loss: 0.2142, Batch MAE: 0.2795\n",
      "Batch Loss: 0.1988, Batch MAE: 0.2466\n",
      "Batch Loss: 0.1780, Batch MAE: 0.1985\n",
      "Batch Loss: 0.1864, Batch MAE: 0.2069\n",
      "Batch Loss: 0.1787, Batch MAE: 0.1716\n",
      "Batch Loss: 0.1785, Batch MAE: 0.1648\n",
      "Epoch 4/10\n",
      "Batch Loss: 0.2141, Batch MAE: 0.1784\n",
      "Batch Loss: 0.2018, Batch MAE: 0.1510\n",
      "Batch Loss: 0.2093, Batch MAE: 0.1549\n",
      "Batch Loss: 0.1920, Batch MAE: 0.1615\n",
      "Batch Loss: 0.1689, Batch MAE: 0.1638\n",
      "Batch Loss: 0.1751, Batch MAE: 0.2159\n",
      "Batch Loss: 0.1731, Batch MAE: 0.2246\n",
      "Batch Loss: 0.1663, Batch MAE: 0.2234\n",
      "Epoch 5/10\n",
      "Batch Loss: 0.1759, Batch MAE: 0.2265\n",
      "Batch Loss: 0.1740, Batch MAE: 0.2311\n",
      "Batch Loss: 0.1824, Batch MAE: 0.2387\n",
      "Batch Loss: 0.1767, Batch MAE: 0.2358\n",
      "Batch Loss: 0.1682, Batch MAE: 0.2203\n",
      "Batch Loss: 0.1767, Batch MAE: 0.2347\n",
      "Batch Loss: 0.1718, Batch MAE: 0.2186\n",
      "Batch Loss: 0.1608, Batch MAE: 0.1946\n",
      "Epoch 6/10\n",
      "Batch Loss: 0.1694, Batch MAE: 0.1768\n",
      "Batch Loss: 0.1665, Batch MAE: 0.1599\n",
      "Batch Loss: 0.1764, Batch MAE: 0.1597\n",
      "Batch Loss: 0.1720, Batch MAE: 0.1560\n",
      "Batch Loss: 0.1600, Batch MAE: 0.1394\n",
      "Batch Loss: 0.1674, Batch MAE: 0.1660\n",
      "Batch Loss: 0.1654, Batch MAE: 0.1626\n",
      "Batch Loss: 0.1550, Batch MAE: 0.1559\n",
      "Epoch 7/10\n",
      "Batch Loss: 0.1659, Batch MAE: 0.1576\n",
      "Batch Loss: 0.1635, Batch MAE: 0.1606\n",
      "Batch Loss: 0.1731, Batch MAE: 0.1786\n",
      "Batch Loss: 0.1668, Batch MAE: 0.1871\n",
      "Batch Loss: 0.1554, Batch MAE: 0.1827\n",
      "Batch Loss: 0.1635, Batch MAE: 0.2071\n",
      "Batch Loss: 0.1615, Batch MAE: 0.2048\n",
      "Batch Loss: 0.1529, Batch MAE: 0.1950\n",
      "Epoch 8/10\n",
      "Batch Loss: 0.1616, Batch MAE: 0.1955\n",
      "Batch Loss: 0.1606, Batch MAE: 0.1882\n",
      "Batch Loss: 0.1695, Batch MAE: 0.1911\n",
      "Batch Loss: 0.1635, Batch MAE: 0.1852\n",
      "Batch Loss: 0.1524, Batch MAE: 0.1687\n",
      "Batch Loss: 0.1604, Batch MAE: 0.1840\n",
      "Batch Loss: 0.1575, Batch MAE: 0.1758\n",
      "Batch Loss: 0.1487, Batch MAE: 0.1642\n",
      "Epoch 9/10\n",
      "Batch Loss: 0.1594, Batch MAE: 0.1632\n",
      "Batch Loss: 0.1576, Batch MAE: 0.1579\n",
      "Batch Loss: 0.1674, Batch MAE: 0.1689\n",
      "Batch Loss: 0.1621, Batch MAE: 0.1724\n",
      "Batch Loss: 0.1509, Batch MAE: 0.1649\n",
      "Batch Loss: 0.1574, Batch MAE: 0.1875\n",
      "Batch Loss: 0.1550, Batch MAE: 0.1866\n",
      "Batch Loss: 0.1463, Batch MAE: 0.1802\n",
      "Epoch 10/10\n",
      "Batch Loss: 0.1554, Batch MAE: 0.1853\n",
      "Batch Loss: 0.1558, Batch MAE: 0.1810\n",
      "Batch Loss: 0.1637, Batch MAE: 0.1876\n",
      "Batch Loss: 0.1581, Batch MAE: 0.1847\n",
      "Batch Loss: 0.1477, Batch MAE: 0.1717\n",
      "Batch Loss: 0.1548, Batch MAE: 0.1868\n",
      "Batch Loss: 0.1521, Batch MAE: 0.1808\n",
      "Batch Loss: 0.1420, Batch MAE: 0.1707\n",
      "tf.Tensor(0.17291728, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17828582, shape=(), dtype=float32)\n",
      "Final Test MAE: 0.178286\n",
      "Training model with time_steps: 160\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 24.2 GiB for an array with shape (2508, 1295680) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ALEXAN~1\\AppData\\Local\\Temp/ipykernel_19560/1801501344.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Clean features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mclean_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mresult_df_en_top20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0momit_lexemes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Create time features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\clean_features.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(df, omit_lexemes)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# Join the dummies back to the original DataFrame or use them separately as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_of_day_dummies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m#     # Keep users either in training or testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   9097\u001b[0m         \u001b[1;36m5\u001b[0m  \u001b[0mK5\u001b[0m  \u001b[0mA5\u001b[0m  \u001b[0mNaN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9098\u001b[0m         \"\"\"\n\u001b[1;32m-> 9099\u001b[1;33m         return self._join_compat(\n\u001b[0m\u001b[0;32m   9100\u001b[0m             \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9101\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   9128\u001b[0m                     \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9129\u001b[0m                 )\n\u001b[1;32m-> 9130\u001b[1;33m             return merge(\n\u001b[0m\u001b[0;32m   9131\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9132\u001b[0m                 \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     )\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[0mrindexers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m         result_data = concatenate_managers(\n\u001b[0m\u001b[0;32m    725\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 24.2 GiB for an array with shape (2508, 1295680) and data type float64"
     ]
    }
   ],
   "source": [
    "%run clean_data_en_lstm\n",
    "%run clean_features\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "#import duolingo_replica_alex as dr\n",
    "#import duolingo_replica as d\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_steps_range = range(40, 201, 20)  # Time steps from 20 to 200 in steps of 20\n",
    "mae_results = {}\n",
    "\n",
    "for time_steps in time_steps_range:\n",
    "  \n",
    "    print(f\"Training model with time_steps: {time_steps}\")\n",
    "        \n",
    "    # time_steps = 20\n",
    "\n",
    "    file_path = \"https://www.dropbox.com/scl/fi/pnxa2jv4xf23bfwry1q9x/learning_traces.13m.csv?rlkey=2dt9848lutbgyys5sujq8dgw2&dl=1\"\n",
    "    result_df_en_top20 = process_data(file_path, time_steps)\n",
    "\n",
    "    # Clean features \n",
    "    clean_data, feature_vars = read_data( result_df_en_top20, omit_lexemes = False )\n",
    "\n",
    "    # Create time features \n",
    "    feature_vars_time = feature_vars + ['time_afternoon', 'time_evening', 'time_morning', 'time_night']\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Assuming clean_data is your DataFrame, Y is the target column, and feature_vars are the features\n",
    "    # Extract unique users\n",
    "    unique_users = clean_data['user_id'].unique()  # Replace 'user_id' with your user identifier column\n",
    "\n",
    "    # Split users into train and test groups\n",
    "    train_users, test_users = train_test_split(unique_users, test_size=0.2, random_state=42)  # Adjust test_size as needed\n",
    "\n",
    "    # Create training and testing datasets\n",
    "    train_data = clean_data[clean_data['user_id'].isin(train_users)]\n",
    "    test_data = clean_data[clean_data['user_id'].isin(test_users)]\n",
    "\n",
    "    # Pre-sort the data by 'user_id' and 'datetime'\n",
    "    train_data.sort_values(by=['user_id', 'datetime'], inplace= True)\n",
    "    test_data.sort_values(by=['user_id', 'datetime'], inplace= True)\n",
    "\n",
    "    # Create Batches so we cann run the model\n",
    "    import numpy as np\n",
    "\n",
    "    def create_non_overlapping_sequences(data, feature_vars, time_steps=time_steps, batch_size=1000):\n",
    "        # Convert feature columns to numpy for faster processing\n",
    "        feature_data = data[feature_vars].to_numpy()\n",
    "        output_data = data['p'].to_numpy()  # Assuming 'p' is your target variable\n",
    "\n",
    "        # Get the starting index for each new user\n",
    "        user_change_indices = np.where(data['user_id'].to_numpy()[:-1] != data['user_id'].to_numpy()[1:])[0] + 1\n",
    "        user_start_indices = np.insert(user_change_indices, 0, 0)\n",
    "\n",
    "        for batch_start in range(0, len(user_start_indices), batch_size):\n",
    "            sequences = []\n",
    "            outputs = []\n",
    "\n",
    "            # Iterate through each user in the batch\n",
    "            for i in range(batch_start, min(batch_start + batch_size, len(user_start_indices))):\n",
    "                start_idx = user_start_indices[i]\n",
    "                end_idx = start_idx + time_steps if i + 1 < len(user_start_indices) else len(feature_data)\n",
    "\n",
    "                # Check if the user data is exactly equal to time_steps\n",
    "                if end_idx - start_idx == time_steps:\n",
    "                    sequences.append(feature_data[start_idx:end_idx])\n",
    "                    outputs.append(output_data[end_idx - 1])  # Target for the last period in the sequence\n",
    "\n",
    "            yield np.array(sequences), np.array(outputs)\n",
    "\n",
    "    # Complex model \n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "    # # Enable mixed precision training\n",
    "    # from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    # policy = mixed_precision.Policy('mixed_float16')\n",
    "    # mixed_precision.set_policy(policy)\n",
    "\n",
    "    # # Create and compile the model within a strategy scope if using multiple GPUs\n",
    "    # strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    # with strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(time_steps, len(feature_vars_time)), return_sequences=True, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(1, kernel_regularizer=L1L2(l1=0.01, l2=0.01)))\n",
    "    # Use experimental options for potential speed-up\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "    import numpy as np\n",
    "\n",
    "    # Training model \n",
    "    # Initialize the MAE metric\n",
    "    mae_metric = MeanAbsoluteError()\n",
    "    num_epochs = 20\n",
    "    # Train the model using the generator\n",
    "    for epoch in range(num_epochs):  # You can define the number of epochs\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "        for X_batch, Y_batch in create_non_overlapping_sequences(train_data, feature_vars_time):\n",
    "            # Train on batch and compute loss\n",
    "            loss = model.train_on_batch(X_batch, Y_batch)\n",
    "\n",
    "            # Compute MAE\n",
    "            predictions = model.predict_on_batch(X_batch)\n",
    "            mae_metric.update_state(Y_batch, predictions)\n",
    "            mae = mae_metric.result().numpy()  # Get the MAE value\n",
    "\n",
    "            # Reset MAE metric at the end of each batch\n",
    "            mae_metric.reset_states()\n",
    "\n",
    "            # Optionally print the loss and MAE for each batch\n",
    "            print(\"Batch Loss: {:.4f}, Batch MAE: {:.4f}\".format(loss, mae))\n",
    "\n",
    "    # Initialize metric for testing\n",
    "    test_mae_metric = MeanAbsoluteError()\n",
    "\n",
    "    # Run model on test data\n",
    "    for X_test_batch, Y_test_batch in create_non_overlapping_sequences(test_data, feature_vars_time):\n",
    "        # Make predictions\n",
    "        test_predictions = model.predict_on_batch(X_test_batch)\n",
    "\n",
    "        # Update MAE metric\n",
    "        test_mae_metric.update_state(Y_test_batch, test_predictions)\n",
    "        print(test_mae_metric.result())\n",
    "\n",
    "    # Calculate final MAE on test data\n",
    "    final_test_mae = test_mae_metric.result().numpy()\n",
    "    print(\"Final Test MAE: {:.6f}\".format(final_test_mae))\n",
    "\n",
    "    # Save the result\n",
    "    mae_results[time_steps] = final_test_mae\n",
    "\n",
    "# Save MAE results to a text file\n",
    "with open('mae_results.txt', 'w') as file:\n",
    "    for time_steps, mae in mae_results.items():\n",
    "        file.write(f\"{time_steps}: {mae}\\n\")\n",
    "\n",
    "print(\"MAE results saved to mae_results.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex\n"
     ]
    }
   ],
   "source": [
    "print(\"Alex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
