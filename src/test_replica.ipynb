{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1c7718-f98c-46c5-abc7-78a7019a7a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db1555e-63ed-4944-8c01-19780f2769d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import duolingo_replica as dr\n",
    "import duolingo_original as do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0d02e-899c-4a68-add3-e95fe6fd5ceb",
   "metadata": {},
   "source": [
    "We load the data and select a subset of 5000 observations to test the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af61c42e-90c2-4fc6-8177-6265c5e5e20d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data   = pd.read_csv('https://www.dropbox.com/scl/fi/pnxa2jv4xf23bfwry1q9x/learning_traces.13m.csv?rlkey=2dt9848lutbgyys5sujq8dgw2&dl=1' )\n",
    "#subset = data.sample( n = 5000, random_state = 5 )\n",
    "subset = data[data[\"learning_language\"] == \"en\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "559301a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_recall</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>delta</th>\n",
       "      <th>user_id</th>\n",
       "      <th>learning_language</th>\n",
       "      <th>ui_language</th>\n",
       "      <th>lexeme_id</th>\n",
       "      <th>lexeme_string</th>\n",
       "      <th>history_seen</th>\n",
       "      <th>history_correct</th>\n",
       "      <th>session_seen</th>\n",
       "      <th>session_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1362082504</td>\n",
       "      <td>357</td>\n",
       "      <td>u:dwbJ</td>\n",
       "      <td>en</td>\n",
       "      <td>pt</td>\n",
       "      <td>1052c3ace653dbc8923eaa183bc02b88</td>\n",
       "      <td>definition/definition&lt;n&gt;&lt;sg&gt;</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1362082504</td>\n",
       "      <td>357</td>\n",
       "      <td>u:dwbJ</td>\n",
       "      <td>en</td>\n",
       "      <td>pt</td>\n",
       "      <td>9cba1b30f88bf3c047b22cffcaf88c12</td>\n",
       "      <td>surface/surface&lt;n&gt;&lt;sg&gt;</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1362082504</td>\n",
       "      <td>357</td>\n",
       "      <td>u:dwbJ</td>\n",
       "      <td>en</td>\n",
       "      <td>pt</td>\n",
       "      <td>961cd149f20f2571419b1412d849f19a</td>\n",
       "      <td>scale/scale&lt;n&gt;&lt;sg&gt;</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1362082504</td>\n",
       "      <td>357</td>\n",
       "      <td>u:dwbJ</td>\n",
       "      <td>en</td>\n",
       "      <td>pt</td>\n",
       "      <td>5cbb1249562e95794a4c4ae0e2d8ae26</td>\n",
       "      <td>temperature/temperature&lt;n&gt;&lt;sg&gt;</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1362082504</td>\n",
       "      <td>357</td>\n",
       "      <td>u:dwbJ</td>\n",
       "      <td>en</td>\n",
       "      <td>pt</td>\n",
       "      <td>2df65bdf80d10d2b78d62cb2e0a731d8</td>\n",
       "      <td>distance/distance&lt;n&gt;&lt;sg&gt;</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12854221</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1363104897</td>\n",
       "      <td>368</td>\n",
       "      <td>u:i5D8</td>\n",
       "      <td>en</td>\n",
       "      <td>it</td>\n",
       "      <td>d5efc552aaea3109eb5388aa1ec8673d</td>\n",
       "      <td>the/the&lt;det&gt;&lt;def&gt;&lt;sp&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12854222</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1363104897</td>\n",
       "      <td>368</td>\n",
       "      <td>u:i5D8</td>\n",
       "      <td>en</td>\n",
       "      <td>it</td>\n",
       "      <td>a826c47947d68549fa81e19cafa57ba0</td>\n",
       "      <td>eat/eat&lt;vblex&gt;&lt;pres&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12854223</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1363104897</td>\n",
       "      <td>368</td>\n",
       "      <td>u:i5D8</td>\n",
       "      <td>en</td>\n",
       "      <td>it</td>\n",
       "      <td>5e29d77697d23070a1fb92eb6c90e9b6</td>\n",
       "      <td>bread/bread&lt;n&gt;&lt;sg&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12854224</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>1363104897</td>\n",
       "      <td>368</td>\n",
       "      <td>u:i5D8</td>\n",
       "      <td>en</td>\n",
       "      <td>it</td>\n",
       "      <td>cdfecc9247566d40bb964a218c54c783</td>\n",
       "      <td>drink/drink&lt;vblex&gt;&lt;pres&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12854225</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>1363104897</td>\n",
       "      <td>368</td>\n",
       "      <td>u:i5D8</td>\n",
       "      <td>en</td>\n",
       "      <td>it</td>\n",
       "      <td>c52ab45d4e22ee7580041911159e3c0c</td>\n",
       "      <td>water/water&lt;n&gt;&lt;sg&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5014791 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          p_recall   timestamp  delta user_id learning_language ui_language  \\\n",
       "87        1.000000  1362082504    357  u:dwbJ                en          pt   \n",
       "88        1.000000  1362082504    357  u:dwbJ                en          pt   \n",
       "89        1.000000  1362082504    357  u:dwbJ                en          pt   \n",
       "90        0.800000  1362082504    357  u:dwbJ                en          pt   \n",
       "91        1.000000  1362082504    357  u:dwbJ                en          pt   \n",
       "...            ...         ...    ...     ...               ...         ...   \n",
       "12854221  0.800000  1363104897    368  u:i5D8                en          it   \n",
       "12854222  0.800000  1363104897    368  u:i5D8                en          it   \n",
       "12854223  1.000000  1363104897    368  u:i5D8                en          it   \n",
       "12854224  0.600000  1363104897    368  u:i5D8                en          it   \n",
       "12854225  0.666667  1363104897    368  u:i5D8                en          it   \n",
       "\n",
       "                                 lexeme_id                   lexeme_string  \\\n",
       "87        1052c3ace653dbc8923eaa183bc02b88    definition/definition<n><sg>   \n",
       "88        9cba1b30f88bf3c047b22cffcaf88c12          surface/surface<n><sg>   \n",
       "89        961cd149f20f2571419b1412d849f19a              scale/scale<n><sg>   \n",
       "90        5cbb1249562e95794a4c4ae0e2d8ae26  temperature/temperature<n><sg>   \n",
       "91        2df65bdf80d10d2b78d62cb2e0a731d8        distance/distance<n><sg>   \n",
       "...                                    ...                             ...   \n",
       "12854221  d5efc552aaea3109eb5388aa1ec8673d           the/the<det><def><sp>   \n",
       "12854222  a826c47947d68549fa81e19cafa57ba0            eat/eat<vblex><pres>   \n",
       "12854223  5e29d77697d23070a1fb92eb6c90e9b6              bread/bread<n><sg>   \n",
       "12854224  cdfecc9247566d40bb964a218c54c783        drink/drink<vblex><pres>   \n",
       "12854225  c52ab45d4e22ee7580041911159e3c0c              water/water<n><sg>   \n",
       "\n",
       "          history_seen  history_correct  session_seen  session_correct  \n",
       "87                  17               17             2                2  \n",
       "88                  19               19             3                3  \n",
       "89                  21               20             3                3  \n",
       "90                  44               36             5                4  \n",
       "91                  21               20             3                3  \n",
       "...                ...              ...           ...              ...  \n",
       "12854221             6                4             5                4  \n",
       "12854222             4                4             5                4  \n",
       "12854223             4                4             4                4  \n",
       "12854224             3                2             5                3  \n",
       "12854225             5                3             9                6  \n",
       "\n",
       "[5014791 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = data[data[\"learning_language\"] == \"en\"]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566fc0e-c83b-4883-9d35-6d75c9b25e2c",
   "metadata": {},
   "source": [
    "# 1. LOGIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efdb2c-3b58-4e09-b06a-15d8ec8da9d6",
   "metadata": {},
   "source": [
    "## 1.1. Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f231e91-dac0-4d03-ba32-a854228303eb",
   "metadata": {},
   "source": [
    "We perform the train-test split and, in addition, obtain the list of predictor variables. The predictor variables include: 'right,' 'wrong,' 'bias,', 'time' and dummy variables for lexemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92dad457-7e7d-4f94-94c7-366694435a68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'p' ]          = df[ 'p_recall' ].apply( lambda x: np.clip( float( x ),  0.0001, .9999 ) )\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:297: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 't' ]          = df[ 'delta' ].apply( lambda x: float( x ) / ( 60 * 60 * 24 ) )\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:298: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'h' ]          = df.apply( lambda row: np.clip( -row[ 't' ] / np.log2( row [ 'p' ] ), min_hl, max_hl ), axis = 1 )\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:299: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'lang' ]       = df.apply( lambda row: f\"{ row[ 'ui_language' ] } -> { row[ 'learning_language' ] }\", axis = 1 )\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:300: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'lexeme' ]     = df.apply( lambda row: f\"{ row[ 'learning_language' ] }:{ row[ 'lexeme_string' ] }\", axis = 1 )\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:301: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'right' ]      = df[ 'history_correct' ].astype( int )\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:302: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'wrong' ]      = df[ 'history_seen' ].astype( int ) - df[ 'right' ]\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'right_this' ] = df[ 'session_correct' ].astype( int )\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:304: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'wrong_this' ] = df[ 'session_seen' ].astype( int ) - df[ 'right_this' ]\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:305: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'right' ]      = df[ 'right' ].apply( lambda x: np.sqrt( 1 + x ) )\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:306: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'wrong' ]      = df[ 'wrong' ].apply( lambda x: np.sqrt( 1 + x ) )\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:307: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'bias' ]       = 1\n",
      "c:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py:308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ 'time' ]       = df[ 't' ] if method == 'lr' else None\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 111. GiB for an array with shape (2983, 5014791) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ALEXAN~1\\AppData\\Local\\Temp/ipykernel_3996/3530686205.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0momit_lexemes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\duolingo_replica.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(df, method, omit_lexemes)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mlexeme_dummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;34m'lexeme'\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mlexeme_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexeme_dummies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexeme_dummies\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mfeature_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m'right'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wrong'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bias'\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m'time'\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lr'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlexeme_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             new_data = concatenate_managers(\n\u001b[0m\u001b[0;32m    533\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 111. GiB for an array with shape (2983, 5014791) and data type float64"
     ]
    }
   ],
   "source": [
    "trainset, testset, feature_vars = dr.read_data( subset, method = 'lr', omit_lexemes = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153c9ad-29e5-4c3d-bb32-e4ecc1bcf663",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define and fit a Logistic Regression Model for the replication code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc966a4-157f-48ff-8aab-de89b81944f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 34.7 s\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_1 = dr.logit_model( feature_columns = feature_vars )\n",
    "model_1.train( trainset )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41f8d8-6635-453e-b7e3-d8caa827d930",
   "metadata": {},
   "source": [
    "We evaluate the model with the test set and obtain results. The h_seed allows replicating the values of h_hat, which are random for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e694b6fb-0fad-48a5-8498-c7aa6c2fbe90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "            Results          \n",
      "-----------------------------\n",
      "Total Loss : 188763.186\n",
      "p          : 65.696\n",
      "h          : 188697.488\n",
      "l2         : 0.002\n",
      "mae (p)    : 0.332\n",
      "cor (p)    : -0.022\n",
      "mae (h)    : 149.518\n",
      "cor (h)    : -0.024\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "model_1.test_model( testset, h_seed = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00488f9b-1f63-4ae9-9411-6488f488175e",
   "metadata": {},
   "source": [
    "We export theta values for the Logit replication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131996ca-2143-42b4-b53a-0b4ea4f5b66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1.dump_theta( 'logit_replication_thetas.txt' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce5cc9-3555-4e21-a0bd-bfc9ee453078",
   "metadata": {},
   "source": [
    "## 1.2. Original code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49736a75-4cff-44e1-bdce-368336232d60",
   "metadata": {},
   "source": [
    "We perform the train-test split and, in addition, obtain the list of predictor variables. The predictor variables include: 'right,' 'wrong,' 'bias,' and dummy variables for lexemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625bac9c-66b2-4782-a529-7c61a198f453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "trainset2, testset2 = do.read_data( subset, method = 'lr' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da8e00-0547-48f2-821c-0da5a80bf443",
   "metadata": {},
   "source": [
    "We define and fit a Logistic Regression Model for the original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f7c2b36-375e-406b-9432-a6a5d0713e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 44.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_2 = do.SpacedRepetitionModel( method = 'lr' )\n",
    "model_2.train( trainset2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ddf7e-9dbb-4777-9d5b-0b3419c1c78b",
   "metadata": {},
   "source": [
    "We evaluate the model with the test set and obtain results. The first value on the right corresponds to the metric 'Total Loss.' The other metrics are named accordingly. The metrics are almost exactly the same as in the replication code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "219ccc38-c421-47ec-9b82-983efb029307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188763.117 (p=65.626, h=188697.488, l2=0.002)\tmae(p)=0.332\tcor(p)=-0.022\tmae(h)=149.518\tcor(h)=-0.024\n"
     ]
    }
   ],
   "source": [
    "model_2.eval( testset2, h_seed = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051a7d4-49a6-4657-8061-ba0e12ca8dc7",
   "metadata": {},
   "source": [
    "We export theta values for the Logit original model. Thetas are almost exactly the same as in the replication code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64995564-049a-41a1-aac8-877b4ec652f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_2.dump_weights( 'logit_original_thetas.txt' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4d608-8bdf-4360-b316-6724ea81e986",
   "metadata": {},
   "source": [
    "# 2. HLR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6085c7e-6ee6-460b-a8f1-5c329134ff70",
   "metadata": {},
   "source": [
    "## 2.1. Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf86e57-8db8-4b8b-aaec-7ee2b5609667",
   "metadata": {},
   "source": [
    "We perform the train-test split and, in addition, obtain the list of predictor variables. The predictor variables include: 'right,' 'wrong,' 'bias,' and dummy variables for lexemes. Unlike Model 1, in this case, the variable 'time' is not included as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f53dd9b3-34ca-43b7-8b1b-37d94c90e48b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainset3, testset3, feature_vars3 = dr.read_data( subset, method = 'hlr', omit_lexemes = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e2f2a-18ff-4953-8df3-9727184ec084",
   "metadata": {},
   "source": [
    "We train the HLR replication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fc41b2d-3c4f-419a-a42e-2c491aa55cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 42 s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_3 = dr.HLR_model( feature_columns = feature_vars3, omit_h_term = True )\n",
    "model_3.train( trainset3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c613e3-eeb3-43eb-a40b-51229e28cd5d",
   "metadata": {},
   "source": [
    "We evaluate the model with the test set and obtain results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c820da51-1ccd-4075-baab-aa4f65a09603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "            Results          \n",
      "-----------------------------\n",
      "Total Loss : 188049.037\n",
      "p          : 180.259\n",
      "h          : 187868.778\n",
      "l2         : 0.000\n",
      "mae (p)    : 0.431\n",
      "cor (p)    : 0.011\n",
      "mae (h)    : 149.080\n",
      "cor (h)    : -0.092\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "model_3.test_model( testset3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb47bf-11e2-48d9-a604-cea21a40478a",
   "metadata": {},
   "source": [
    "We export theta values for the Logit replication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87205adb-86bb-438f-b637-e7c82fab4fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_3.dump_theta( 'hlr_replication_thetas.txt' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943a960-0978-4944-9ed7-a9249263282f",
   "metadata": {},
   "source": [
    "## 2.2. Original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897f74f-594a-4d6b-a5fe-6c6aae84cc36",
   "metadata": {},
   "source": [
    "We perform the train-test split and, in addition, obtain the list of predictor variables. The predictor variables include: 'right,' 'wrong,' 'bias,' and dummy variables for lexemes. Unlike Model 3, in this case, the variable 'time' is not included as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e81c26d9-b135-4acd-bdac-210b92797878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "trainset2, testset2 = do.read_data( subset, method = 'hlr' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc0e82-34db-40c3-9ee8-9e58a3232288",
   "metadata": {},
   "source": [
    "We fit the HLR original model. We omit h_term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27a1367a-1388-497e-8aa1-f26cb66a03d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 53.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_4 = do.SpacedRepetitionModel( method = 'hlr', omit_h_term = True )\n",
    "model_4.train( trainset2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450dfeda-1fd6-4d9e-bb95-831a58bd6097",
   "metadata": {},
   "source": [
    "We evaluate the model with the test set and obtain results. The first value on the right corresponds to the metric 'Total Loss.' The other metrics are named accordingly. The metrics are almost exactly the same as in the replication code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "614472e3-811b-4dee-a9d1-829472b29588",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188048.806 (p=180.252, h=187868.554, l2=0.000)\tmae(p)=0.431\tcor(p)=0.011\tmae(h)=149.080\tcor(h)=-0.092\n"
     ]
    }
   ],
   "source": [
    "model_4.eval( testset2, h_seed = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c2c9c-c452-4df6-891a-d69b2311decc",
   "metadata": {},
   "source": [
    "We export theta values for the HLR original model. Thetas are almost exactly the same as in the replication code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a779d84-4f26-4ac1-b018-01e6a3710668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_4.dump_weights( 'hlr_original_thetas.txt' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
