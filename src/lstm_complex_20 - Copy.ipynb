{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with time_steps: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Batch Loss: 1.0560, Batch MAE: 0.8058\n",
      "Batch Loss: 0.8382, Batch MAE: 0.7220\n",
      "Batch Loss: 0.6453, Batch MAE: 0.6002\n",
      "Batch Loss: 0.4879, Batch MAE: 0.4627\n",
      "Batch Loss: 0.3365, Batch MAE: 0.3129\n",
      "Batch Loss: 0.2254, Batch MAE: 0.2433\n",
      "Batch Loss: 0.2650, Batch MAE: 0.2950\n",
      "Batch Loss: 0.3029, Batch MAE: 0.2541\n",
      "Batch Loss: 0.2464, Batch MAE: 0.2079\n",
      "Batch Loss: 0.2141, Batch MAE: 0.2094\n",
      "Batch Loss: 0.1886, Batch MAE: 0.2422\n",
      "Batch Loss: 0.2072, Batch MAE: 0.3044\n",
      "Batch Loss: 0.2134, Batch MAE: 0.3114\n",
      "Batch Loss: 0.2080, Batch MAE: 0.2909\n",
      "Batch Loss: 0.1991, Batch MAE: 0.2576\n",
      "Batch Loss: 0.1831, Batch MAE: 0.2287\n",
      "Batch Loss: 0.1867, Batch MAE: 0.2243\n",
      "Epoch 2/20\n",
      "Batch Loss: 0.1937, Batch MAE: 0.1987\n",
      "Batch Loss: 0.1745, Batch MAE: 0.1654\n",
      "Batch Loss: 0.1979, Batch MAE: 0.1776\n",
      "Batch Loss: 0.1832, Batch MAE: 0.1531\n",
      "Batch Loss: 0.1812, Batch MAE: 0.1551\n",
      "Batch Loss: 0.1739, Batch MAE: 0.1553\n",
      "Batch Loss: 0.1717, Batch MAE: 0.1697\n",
      "Batch Loss: 0.1615, Batch MAE: 0.1765\n",
      "Batch Loss: 0.1675, Batch MAE: 0.1960\n",
      "Batch Loss: 0.1633, Batch MAE: 0.1954\n",
      "Batch Loss: 0.1570, Batch MAE: 0.1947\n",
      "Batch Loss: 0.1710, Batch MAE: 0.2170\n",
      "Batch Loss: 0.1615, Batch MAE: 0.2032\n",
      "Batch Loss: 0.1543, Batch MAE: 0.1834\n",
      "Batch Loss: 0.1608, Batch MAE: 0.1723\n",
      "Batch Loss: 0.1534, Batch MAE: 0.1512\n",
      "Batch Loss: 0.1641, Batch MAE: 0.1571\n",
      "Epoch 3/20\n",
      "Batch Loss: 0.1677, Batch MAE: 0.1454\n",
      "Batch Loss: 0.1606, Batch MAE: 0.1446\n",
      "Batch Loss: 0.1810, Batch MAE: 0.1759\n",
      "Batch Loss: 0.1646, Batch MAE: 0.1706\n",
      "Batch Loss: 0.1638, Batch MAE: 0.1848\n",
      "Batch Loss: 0.1618, Batch MAE: 0.1915\n",
      "Batch Loss: 0.1624, Batch MAE: 0.2000\n",
      "Batch Loss: 0.1529, Batch MAE: 0.1916\n",
      "Batch Loss: 0.1597, Batch MAE: 0.1928\n",
      "Batch Loss: 0.1545, Batch MAE: 0.1846\n",
      "Batch Loss: 0.1448, Batch MAE: 0.1662\n",
      "Batch Loss: 0.1606, Batch MAE: 0.1764\n",
      "Batch Loss: 0.1501, Batch MAE: 0.1596\n",
      "Batch Loss: 0.1439, Batch MAE: 0.1500\n",
      "Batch Loss: 0.1544, Batch MAE: 0.1558\n",
      "Batch Loss: 0.1467, Batch MAE: 0.1498\n",
      "Batch Loss: 0.1552, Batch MAE: 0.1681\n",
      "Epoch 4/20\n",
      "Batch Loss: 0.1572, Batch MAE: 0.1746\n",
      "Batch Loss: 0.1517, Batch MAE: 0.1738\n",
      "Batch Loss: 0.1707, Batch MAE: 0.1973\n",
      "Batch Loss: 0.1581, Batch MAE: 0.1877\n",
      "Batch Loss: 0.1561, Batch MAE: 0.1896\n",
      "Batch Loss: 0.1549, Batch MAE: 0.1840\n",
      "Batch Loss: 0.1557, Batch MAE: 0.1812\n",
      "Batch Loss: 0.1456, Batch MAE: 0.1679\n",
      "Batch Loss: 0.1512, Batch MAE: 0.1666\n",
      "Batch Loss: 0.1468, Batch MAE: 0.1612\n",
      "Batch Loss: 0.1369, Batch MAE: 0.1485\n",
      "Batch Loss: 0.1535, Batch MAE: 0.1688\n",
      "Batch Loss: 0.1441, Batch MAE: 0.1606\n",
      "Batch Loss: 0.1363, Batch MAE: 0.1581\n",
      "Batch Loss: 0.1471, Batch MAE: 0.1684\n",
      "Batch Loss: 0.1392, Batch MAE: 0.1636\n",
      "Batch Loss: 0.1495, Batch MAE: 0.1804\n",
      "Epoch 5/20\n",
      "Batch Loss: 0.1504, Batch MAE: 0.1813\n",
      "Batch Loss: 0.1456, Batch MAE: 0.1760\n",
      "Batch Loss: 0.1638, Batch MAE: 0.1945\n",
      "Batch Loss: 0.1514, Batch MAE: 0.1801\n",
      "Batch Loss: 0.1506, Batch MAE: 0.1794\n",
      "Batch Loss: 0.1485, Batch MAE: 0.1731\n",
      "Batch Loss: 0.1477, Batch MAE: 0.1710\n",
      "Batch Loss: 0.1391, Batch MAE: 0.1616\n",
      "Batch Loss: 0.1441, Batch MAE: 0.1640\n",
      "Batch Loss: 0.1415, Batch MAE: 0.1621\n",
      "Batch Loss: 0.1310, Batch MAE: 0.1529\n",
      "Batch Loss: 0.1473, Batch MAE: 0.1749\n",
      "Batch Loss: 0.1382, Batch MAE: 0.1674\n",
      "Batch Loss: 0.1314, Batch MAE: 0.1632\n",
      "Batch Loss: 0.1420, Batch MAE: 0.1711\n",
      "Batch Loss: 0.1332, Batch MAE: 0.1641\n",
      "Batch Loss: 0.1426, Batch MAE: 0.1789\n",
      "Epoch 6/20\n",
      "Batch Loss: 0.1435, Batch MAE: 0.1770\n",
      "Batch Loss: 0.1396, Batch MAE: 0.1716\n",
      "Batch Loss: 0.1573, Batch MAE: 0.1908\n",
      "Batch Loss: 0.1449, Batch MAE: 0.1773\n",
      "Batch Loss: 0.1441, Batch MAE: 0.1785\n",
      "Batch Loss: 0.1423, Batch MAE: 0.1743\n",
      "Batch Loss: 0.1411, Batch MAE: 0.1733\n",
      "Batch Loss: 0.1319, Batch MAE: 0.1640\n",
      "Batch Loss: 0.1381, Batch MAE: 0.1656\n",
      "Batch Loss: 0.1336, Batch MAE: 0.1635\n",
      "Batch Loss: 0.1237, Batch MAE: 0.1529\n",
      "Batch Loss: 0.1396, Batch MAE: 0.1737\n",
      "Batch Loss: 0.1305, Batch MAE: 0.1653\n",
      "Batch Loss: 0.1238, Batch MAE: 0.1609\n",
      "Batch Loss: 0.1342, Batch MAE: 0.1694\n",
      "Batch Loss: 0.1267, Batch MAE: 0.1624\n",
      "Batch Loss: 0.1376, Batch MAE: 0.1771\n",
      "Epoch 7/20\n",
      "Batch Loss: 0.1377, Batch MAE: 0.1764\n",
      "Batch Loss: 0.1324, Batch MAE: 0.1694\n",
      "Batch Loss: 0.1521, Batch MAE: 0.1888\n",
      "Batch Loss: 0.1385, Batch MAE: 0.1755\n",
      "Batch Loss: 0.1373, Batch MAE: 0.1767\n",
      "Batch Loss: 0.1357, Batch MAE: 0.1722\n",
      "Batch Loss: 0.1352, Batch MAE: 0.1708\n",
      "Batch Loss: 0.1266, Batch MAE: 0.1616\n",
      "Batch Loss: 0.1319, Batch MAE: 0.1628\n",
      "Batch Loss: 0.1289, Batch MAE: 0.1622\n",
      "Batch Loss: 0.1195, Batch MAE: 0.1517\n",
      "Batch Loss: 0.1333, Batch MAE: 0.1726\n",
      "Batch Loss: 0.1251, Batch MAE: 0.1648\n",
      "Batch Loss: 0.1177, Batch MAE: 0.1606\n",
      "Batch Loss: 0.1282, Batch MAE: 0.1683\n",
      "Batch Loss: 0.1203, Batch MAE: 0.1598\n",
      "Batch Loss: 0.1304, Batch MAE: 0.1736\n",
      "Epoch 8/20\n",
      "Batch Loss: 0.1314, Batch MAE: 0.1716\n",
      "Batch Loss: 0.1270, Batch MAE: 0.1624\n",
      "Batch Loss: 0.1465, Batch MAE: 0.1823\n",
      "Batch Loss: 0.1322, Batch MAE: 0.1698\n",
      "Batch Loss: 0.1307, Batch MAE: 0.1725\n",
      "Batch Loss: 0.1293, Batch MAE: 0.1691\n",
      "Batch Loss: 0.1285, Batch MAE: 0.1682\n",
      "Batch Loss: 0.1209, Batch MAE: 0.1595\n",
      "Batch Loss: 0.1254, Batch MAE: 0.1602\n",
      "Batch Loss: 0.1222, Batch MAE: 0.1603\n",
      "Batch Loss: 0.1130, Batch MAE: 0.1500\n",
      "Batch Loss: 0.1284, Batch MAE: 0.1712\n",
      "Batch Loss: 0.1193, Batch MAE: 0.1626\n",
      "Batch Loss: 0.1125, Batch MAE: 0.1578\n",
      "Batch Loss: 0.1232, Batch MAE: 0.1643\n",
      "Batch Loss: 0.1150, Batch MAE: 0.1558\n",
      "Batch Loss: 0.1253, Batch MAE: 0.1703\n",
      "Epoch 9/20\n",
      "Batch Loss: 0.1247, Batch MAE: 0.1684\n",
      "Batch Loss: 0.1216, Batch MAE: 0.1600\n",
      "Batch Loss: 0.1392, Batch MAE: 0.1822\n",
      "Batch Loss: 0.1263, Batch MAE: 0.1709\n",
      "Batch Loss: 0.1254, Batch MAE: 0.1737\n",
      "Batch Loss: 0.1243, Batch MAE: 0.1690\n",
      "Batch Loss: 0.1240, Batch MAE: 0.1655\n",
      "Batch Loss: 0.1154, Batch MAE: 0.1544\n",
      "Batch Loss: 0.1202, Batch MAE: 0.1543\n",
      "Batch Loss: 0.1171, Batch MAE: 0.1557\n",
      "Batch Loss: 0.1077, Batch MAE: 0.1473\n",
      "Batch Loss: 0.1234, Batch MAE: 0.1721\n",
      "Batch Loss: 0.1142, Batch MAE: 0.1663\n",
      "Batch Loss: 0.1075, Batch MAE: 0.1620\n",
      "Batch Loss: 0.1158, Batch MAE: 0.1671\n",
      "Batch Loss: 0.1095, Batch MAE: 0.1550\n",
      "Batch Loss: 0.1194, Batch MAE: 0.1664\n",
      "Epoch 10/20\n",
      "Batch Loss: 0.1193, Batch MAE: 0.1622\n",
      "Batch Loss: 0.1160, Batch MAE: 0.1525\n",
      "Batch Loss: 0.1341, Batch MAE: 0.1782\n",
      "Batch Loss: 0.1212, Batch MAE: 0.1701\n",
      "Batch Loss: 0.1203, Batch MAE: 0.1752\n",
      "Batch Loss: 0.1184, Batch MAE: 0.1715\n",
      "Batch Loss: 0.1180, Batch MAE: 0.1668\n",
      "Batch Loss: 0.1090, Batch MAE: 0.1537\n",
      "Batch Loss: 0.1144, Batch MAE: 0.1523\n",
      "Batch Loss: 0.1125, Batch MAE: 0.1544\n",
      "Batch Loss: 0.1016, Batch MAE: 0.1466\n",
      "Batch Loss: 0.1180, Batch MAE: 0.1720\n",
      "Batch Loss: 0.1088, Batch MAE: 0.1669\n",
      "Batch Loss: 0.1012, Batch MAE: 0.1620\n",
      "Batch Loss: 0.1117, Batch MAE: 0.1657\n",
      "Batch Loss: 0.1047, Batch MAE: 0.1521\n",
      "Batch Loss: 0.1147, Batch MAE: 0.1632\n",
      "Epoch 11/20\n",
      "Batch Loss: 0.1138, Batch MAE: 0.1598\n",
      "Batch Loss: 0.1103, Batch MAE: 0.1524\n",
      "Batch Loss: 0.1300, Batch MAE: 0.1818\n",
      "Batch Loss: 0.1159, Batch MAE: 0.1756\n",
      "Batch Loss: 0.1159, Batch MAE: 0.1785\n",
      "Batch Loss: 0.1143, Batch MAE: 0.1700\n",
      "Batch Loss: 0.1133, Batch MAE: 0.1599\n",
      "Batch Loss: 0.1043, Batch MAE: 0.1446\n",
      "Batch Loss: 0.1105, Batch MAE: 0.1465\n",
      "Batch Loss: 0.1066, Batch MAE: 0.1550\n",
      "Batch Loss: 0.0979, Batch MAE: 0.1510\n",
      "Batch Loss: 0.1121, Batch MAE: 0.1766\n",
      "Batch Loss: 0.1033, Batch MAE: 0.1685\n",
      "Batch Loss: 0.0977, Batch MAE: 0.1589\n",
      "Batch Loss: 0.1057, Batch MAE: 0.1582\n",
      "Batch Loss: 0.0996, Batch MAE: 0.1434\n",
      "Batch Loss: 0.1095, Batch MAE: 0.1595\n",
      "Epoch 12/20\n",
      "Batch Loss: 0.1084, Batch MAE: 0.1603\n",
      "Batch Loss: 0.1052, Batch MAE: 0.1550\n",
      "Batch Loss: 0.1245, Batch MAE: 0.1856\n",
      "Batch Loss: 0.1112, Batch MAE: 0.1768\n",
      "Batch Loss: 0.1105, Batch MAE: 0.1733\n",
      "Batch Loss: 0.1093, Batch MAE: 0.1595\n",
      "Batch Loss: 0.1071, Batch MAE: 0.1473\n",
      "Batch Loss: 0.0997, Batch MAE: 0.1390\n",
      "Batch Loss: 0.1050, Batch MAE: 0.1516\n",
      "Batch Loss: 0.1021, Batch MAE: 0.1662\n",
      "Batch Loss: 0.0939, Batch MAE: 0.1591\n",
      "Batch Loss: 0.1069, Batch MAE: 0.1755\n",
      "Batch Loss: 0.0992, Batch MAE: 0.1590\n",
      "Batch Loss: 0.0926, Batch MAE: 0.1469\n",
      "Batch Loss: 0.1029, Batch MAE: 0.1511\n",
      "Batch Loss: 0.0958, Batch MAE: 0.1480\n",
      "Batch Loss: 0.1056, Batch MAE: 0.1727\n",
      "Epoch 13/20\n",
      "Batch Loss: 0.1043, Batch MAE: 0.1719\n",
      "Batch Loss: 0.1010, Batch MAE: 0.1558\n",
      "Batch Loss: 0.1200, Batch MAE: 0.1772\n",
      "Batch Loss: 0.1064, Batch MAE: 0.1631\n",
      "Batch Loss: 0.1052, Batch MAE: 0.1637\n",
      "Batch Loss: 0.1041, Batch MAE: 0.1574\n",
      "Batch Loss: 0.1041, Batch MAE: 0.1516\n",
      "Batch Loss: 0.0964, Batch MAE: 0.1467\n",
      "Batch Loss: 0.1008, Batch MAE: 0.1570\n",
      "Batch Loss: 0.0975, Batch MAE: 0.1663\n",
      "Batch Loss: 0.0890, Batch MAE: 0.1494\n",
      "Batch Loss: 0.1036, Batch MAE: 0.1666\n",
      "Batch Loss: 0.0951, Batch MAE: 0.1567\n",
      "Batch Loss: 0.0883, Batch MAE: 0.1506\n",
      "Batch Loss: 0.0974, Batch MAE: 0.1584\n",
      "Batch Loss: 0.0917, Batch MAE: 0.1513\n",
      "Batch Loss: 0.1009, Batch MAE: 0.1695\n",
      "Epoch 14/20\n",
      "Batch Loss: 0.0992, Batch MAE: 0.1567\n",
      "Batch Loss: 0.0958, Batch MAE: 0.1373\n",
      "Batch Loss: 0.1155, Batch MAE: 0.1915\n",
      "Batch Loss: 0.1024, Batch MAE: 0.1866\n",
      "Batch Loss: 0.1028, Batch MAE: 0.1635\n",
      "Batch Loss: 0.0999, Batch MAE: 0.1314\n",
      "Batch Loss: 0.0999, Batch MAE: 0.1404\n",
      "Batch Loss: 0.0922, Batch MAE: 0.1661\n",
      "Batch Loss: 0.0971, Batch MAE: 0.1771\n",
      "Batch Loss: 0.0949, Batch MAE: 0.1593\n",
      "Batch Loss: 0.0863, Batch MAE: 0.1214\n",
      "Batch Loss: 0.1015, Batch MAE: 0.1654\n",
      "Batch Loss: 0.0919, Batch MAE: 0.1763\n",
      "Batch Loss: 0.0863, Batch MAE: 0.1646\n",
      "Batch Loss: 0.0948, Batch MAE: 0.1505\n",
      "Batch Loss: 0.0888, Batch MAE: 0.1345\n",
      "Batch Loss: 0.0973, Batch MAE: 0.1667\n",
      "Epoch 15/20\n",
      "Batch Loss: 0.0950, Batch MAE: 0.1676\n",
      "Batch Loss: 0.0922, Batch MAE: 0.1527\n",
      "Batch Loss: 0.1110, Batch MAE: 0.1830\n",
      "Batch Loss: 0.0986, Batch MAE: 0.1646\n",
      "Batch Loss: 0.0971, Batch MAE: 0.1529\n",
      "Batch Loss: 0.0962, Batch MAE: 0.1469\n",
      "Batch Loss: 0.0958, Batch MAE: 0.1504\n",
      "Batch Loss: 0.0890, Batch MAE: 0.1559\n",
      "Batch Loss: 0.0937, Batch MAE: 0.1605\n",
      "Batch Loss: 0.0911, Batch MAE: 0.1538\n",
      "Batch Loss: 0.0820, Batch MAE: 0.1280\n",
      "Batch Loss: 0.0985, Batch MAE: 0.1802\n",
      "Batch Loss: 0.0888, Batch MAE: 0.1744\n",
      "Batch Loss: 0.0838, Batch MAE: 0.1395\n",
      "Batch Loss: 0.0915, Batch MAE: 0.1476\n",
      "Batch Loss: 0.0851, Batch MAE: 0.1628\n",
      "Batch Loss: 0.0946, Batch MAE: 0.1781\n",
      "Epoch 16/20\n",
      "Batch Loss: 0.0908, Batch MAE: 0.1415\n",
      "Batch Loss: 0.0894, Batch MAE: 0.1336\n",
      "Batch Loss: 0.1092, Batch MAE: 0.2124\n",
      "Batch Loss: 0.0990, Batch MAE: 0.1964\n",
      "Batch Loss: 0.0967, Batch MAE: 0.1445\n",
      "Batch Loss: 0.0938, Batch MAE: 0.1211\n",
      "Batch Loss: 0.0946, Batch MAE: 0.1574\n",
      "Batch Loss: 0.0855, Batch MAE: 0.1817\n",
      "Batch Loss: 0.0920, Batch MAE: 0.1730\n",
      "Batch Loss: 0.0889, Batch MAE: 0.1425\n",
      "Batch Loss: 0.0789, Batch MAE: 0.1172\n",
      "Batch Loss: 0.0975, Batch MAE: 0.1717\n",
      "Batch Loss: 0.0852, Batch MAE: 0.1844\n",
      "Batch Loss: 0.0819, Batch MAE: 0.1681\n",
      "Batch Loss: 0.0897, Batch MAE: 0.1461\n",
      "Batch Loss: 0.0822, Batch MAE: 0.1259\n",
      "Batch Loss: 0.0920, Batch MAE: 0.1613\n",
      "Epoch 17/20\n",
      "Batch Loss: 0.0879, Batch MAE: 0.1698\n",
      "Batch Loss: 0.0865, Batch MAE: 0.1630\n",
      "Batch Loss: 0.1043, Batch MAE: 0.1806\n",
      "Batch Loss: 0.0920, Batch MAE: 0.1544\n",
      "Batch Loss: 0.0908, Batch MAE: 0.1478\n",
      "Batch Loss: 0.0909, Batch MAE: 0.1499\n",
      "Batch Loss: 0.0893, Batch MAE: 0.1513\n",
      "Batch Loss: 0.0821, Batch MAE: 0.1518\n",
      "Batch Loss: 0.0867, Batch MAE: 0.1557\n",
      "Batch Loss: 0.0854, Batch MAE: 0.1568\n",
      "Batch Loss: 0.0767, Batch MAE: 0.1359\n",
      "Batch Loss: 0.0913, Batch MAE: 0.1672\n",
      "Batch Loss: 0.0828, Batch MAE: 0.1685\n",
      "Batch Loss: 0.0775, Batch MAE: 0.1523\n",
      "Batch Loss: 0.0854, Batch MAE: 0.1466\n",
      "Batch Loss: 0.0800, Batch MAE: 0.1424\n",
      "Batch Loss: 0.0881, Batch MAE: 0.1719\n",
      "Epoch 18/20\n",
      "Batch Loss: 0.0849, Batch MAE: 0.1585\n",
      "Batch Loss: 0.0826, Batch MAE: 0.1342\n",
      "Batch Loss: 0.1031, Batch MAE: 0.1867\n",
      "Batch Loss: 0.0890, Batch MAE: 0.1758\n",
      "Batch Loss: 0.0879, Batch MAE: 0.1484\n",
      "Batch Loss: 0.0878, Batch MAE: 0.1354\n",
      "Batch Loss: 0.0861, Batch MAE: 0.1509\n",
      "Batch Loss: 0.0803, Batch MAE: 0.1626\n",
      "Batch Loss: 0.0850, Batch MAE: 0.1563\n",
      "Batch Loss: 0.0838, Batch MAE: 0.1488\n",
      "Batch Loss: 0.0739, Batch MAE: 0.1306\n",
      "Batch Loss: 0.0894, Batch MAE: 0.1826\n",
      "Batch Loss: 0.0821, Batch MAE: 0.1709\n",
      "Batch Loss: 0.0753, Batch MAE: 0.1304\n",
      "Batch Loss: 0.0850, Batch MAE: 0.1555\n",
      "Batch Loss: 0.0769, Batch MAE: 0.1691\n",
      "Batch Loss: 0.0856, Batch MAE: 0.1649\n",
      "Epoch 19/20\n",
      "Batch Loss: 0.0825, Batch MAE: 0.1329\n",
      "Batch Loss: 0.0822, Batch MAE: 0.1500\n",
      "Batch Loss: 0.0977, Batch MAE: 0.2141\n",
      "Batch Loss: 0.0904, Batch MAE: 0.1718\n",
      "Batch Loss: 0.0853, Batch MAE: 0.1261\n",
      "Batch Loss: 0.0884, Batch MAE: 0.1467\n",
      "Batch Loss: 0.0833, Batch MAE: 0.1758\n",
      "Batch Loss: 0.0788, Batch MAE: 0.1620\n",
      "Batch Loss: 0.0823, Batch MAE: 0.1387\n",
      "Batch Loss: 0.0816, Batch MAE: 0.1443\n",
      "Batch Loss: 0.0720, Batch MAE: 0.1443\n",
      "Batch Loss: 0.0862, Batch MAE: 0.1811\n",
      "Batch Loss: 0.0800, Batch MAE: 0.1663\n",
      "Batch Loss: 0.0731, Batch MAE: 0.1355\n",
      "Batch Loss: 0.0817, Batch MAE: 0.1442\n",
      "Batch Loss: 0.0750, Batch MAE: 0.1580\n",
      "Batch Loss: 0.0821, Batch MAE: 0.1791\n",
      "Epoch 20/20\n",
      "Batch Loss: 0.0807, Batch MAE: 0.1481\n",
      "Batch Loss: 0.0768, Batch MAE: 0.1203\n",
      "Batch Loss: 0.0996, Batch MAE: 0.1895\n",
      "Batch Loss: 0.0840, Batch MAE: 0.1932\n",
      "Batch Loss: 0.0854, Batch MAE: 0.1597\n",
      "Batch Loss: 0.0827, Batch MAE: 0.1228\n",
      "Batch Loss: 0.0832, Batch MAE: 0.1367\n",
      "Batch Loss: 0.0760, Batch MAE: 0.1726\n",
      "Batch Loss: 0.0818, Batch MAE: 0.1798\n",
      "Batch Loss: 0.0801, Batch MAE: 0.1516\n",
      "Batch Loss: 0.0700, Batch MAE: 0.1134\n",
      "Batch Loss: 0.0890, Batch MAE: 0.1677\n",
      "Batch Loss: 0.0760, Batch MAE: 0.1857\n",
      "Batch Loss: 0.0733, Batch MAE: 0.1642\n",
      "Batch Loss: 0.0806, Batch MAE: 0.1375\n",
      "Batch Loss: 0.0743, Batch MAE: 0.1256\n",
      "Batch Loss: 0.0816, Batch MAE: 0.1694\n",
      "tf.Tensor(0.176306, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17816629, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17249209, shape=(), dtype=float32)\n",
      "tf.Tensor(0.17214447, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1743842, shape=(), dtype=float32)\n",
      "Final Test MAE: 0.174384\n",
      "Training model with time_steps: 60\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 18.8 GiB for an array with shape (2457, 1027320) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ALEXAN~1\\AppData\\Local\\Temp/ipykernel_46868/1109084926.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Clean features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mclean_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mresult_df_en_top20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0momit_lexemes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Create time features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\Documents\\GitHub\\NNLangRecall\\src\\clean_features.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(df, omit_lexemes)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mlexeme_dummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;34m'lexeme'\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mlexeme_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexeme_dummies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexeme_dummies\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mfeature_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;34m'right'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wrong'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bias'\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlexeme_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             new_data = concatenate_managers(\n\u001b[0m\u001b[0;32m    533\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\Alexander\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 18.8 GiB for an array with shape (2457, 1027320) and data type float64"
     ]
    }
   ],
   "source": [
    "%run clean_data_en_lstm\n",
    "%run clean_features\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "#import duolingo_replica_alex as dr\n",
    "#import duolingo_replica as d\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_steps_range = range(20, 201, 20)  # Time steps from 20 to 200 in steps of 20\n",
    "mae_results = {}\n",
    "\n",
    "for time_steps in time_steps_range:\n",
    "  \n",
    "    print(f\"Training model with time_steps: {time_steps}\")\n",
    "        \n",
    "    # time_steps = 20\n",
    "\n",
    "    file_path = \"https://www.dropbox.com/scl/fi/pnxa2jv4xf23bfwry1q9x/learning_traces.13m.csv?rlkey=2dt9848lutbgyys5sujq8dgw2&dl=1\"\n",
    "    result_df_en_top20 = process_data(file_path, time_steps)\n",
    "\n",
    "    # Clean features \n",
    "    clean_data, feature_vars = read_data( result_df_en_top20, omit_lexemes = False )\n",
    "\n",
    "    # Create time features \n",
    "    feature_vars_time = feature_vars + ['time_afternoon', 'time_evening', 'time_morning', 'time_night']\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Assuming clean_data is your DataFrame, Y is the target column, and feature_vars are the features\n",
    "    # Extract unique users\n",
    "    unique_users = clean_data['user_id'].unique()  # Replace 'user_id' with your user identifier column\n",
    "\n",
    "    # Split users into train and test groups\n",
    "    train_users, test_users = train_test_split(unique_users, test_size=0.2, random_state=42)  # Adjust test_size as needed\n",
    "\n",
    "    # Create training and testing datasets\n",
    "    train_data = clean_data[clean_data['user_id'].isin(train_users)]\n",
    "    test_data = clean_data[clean_data['user_id'].isin(test_users)]\n",
    "\n",
    "    # Pre-sort the data by 'user_id' and 'datetime'\n",
    "    train_data.sort_values(by=['user_id', 'datetime'], inplace= True)\n",
    "    test_data.sort_values(by=['user_id', 'datetime'], inplace= True)\n",
    "\n",
    "    # Create Batches so we cann run the model\n",
    "    import numpy as np\n",
    "\n",
    "    def create_non_overlapping_sequences(data, feature_vars, time_steps=time_steps, batch_size=1000):\n",
    "        # Convert feature columns to numpy for faster processing\n",
    "        feature_data = data[feature_vars].to_numpy()\n",
    "        output_data = data['p'].to_numpy()  # Assuming 'p' is your target variable\n",
    "\n",
    "        # Get the starting index for each new user\n",
    "        user_change_indices = np.where(data['user_id'].to_numpy()[:-1] != data['user_id'].to_numpy()[1:])[0] + 1\n",
    "        user_start_indices = np.insert(user_change_indices, 0, 0)\n",
    "\n",
    "        for batch_start in range(0, len(user_start_indices), batch_size):\n",
    "            sequences = []\n",
    "            outputs = []\n",
    "\n",
    "            # Iterate through each user in the batch\n",
    "            for i in range(batch_start, min(batch_start + batch_size, len(user_start_indices))):\n",
    "                start_idx = user_start_indices[i]\n",
    "                end_idx = start_idx + time_steps if i + 1 < len(user_start_indices) else len(feature_data)\n",
    "\n",
    "                # Check if the user data is exactly equal to time_steps\n",
    "                if end_idx - start_idx == time_steps:\n",
    "                    sequences.append(feature_data[start_idx:end_idx])\n",
    "                    outputs.append(output_data[end_idx - 1])  # Target for the last period in the sequence\n",
    "\n",
    "            yield np.array(sequences), np.array(outputs)\n",
    "\n",
    "    # Complex model \n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "    # # Enable mixed precision training\n",
    "    # from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    # policy = mixed_precision.Policy('mixed_float16')\n",
    "    # mixed_precision.set_policy(policy)\n",
    "\n",
    "    # # Create and compile the model within a strategy scope if using multiple GPUs\n",
    "    # strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    # with strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(time_steps, len(feature_vars_time)), return_sequences=True, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(1, kernel_regularizer=L1L2(l1=0.01, l2=0.01)))\n",
    "    # Use experimental options for potential speed-up\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "    import numpy as np\n",
    "\n",
    "    # Training model \n",
    "    # Initialize the MAE metric\n",
    "    mae_metric = MeanAbsoluteError()\n",
    "    num_epochs = 20\n",
    "    # Train the model using the generator\n",
    "    for epoch in range(num_epochs):  # You can define the number of epochs\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "        for X_batch, Y_batch in create_non_overlapping_sequences(train_data, feature_vars_time):\n",
    "            # Train on batch and compute loss\n",
    "            loss = model.train_on_batch(X_batch, Y_batch)\n",
    "\n",
    "            # Compute MAE\n",
    "            predictions = model.predict_on_batch(X_batch)\n",
    "            mae_metric.update_state(Y_batch, predictions)\n",
    "            mae = mae_metric.result().numpy()  # Get the MAE value\n",
    "\n",
    "            # Reset MAE metric at the end of each batch\n",
    "            mae_metric.reset_states()\n",
    "\n",
    "            # Optionally print the loss and MAE for each batch\n",
    "            print(\"Batch Loss: {:.4f}, Batch MAE: {:.4f}\".format(loss, mae))\n",
    "\n",
    "    # Initialize metric for testing\n",
    "    test_mae_metric = MeanAbsoluteError()\n",
    "\n",
    "    # Run model on test data\n",
    "    for X_test_batch, Y_test_batch in create_non_overlapping_sequences(test_data, feature_vars_time):\n",
    "        # Make predictions\n",
    "        test_predictions = model.predict_on_batch(X_test_batch)\n",
    "\n",
    "        # Update MAE metric\n",
    "        test_mae_metric.update_state(Y_test_batch, test_predictions)\n",
    "        print(test_mae_metric.result())\n",
    "\n",
    "    # Calculate final MAE on test data\n",
    "    final_test_mae = test_mae_metric.result().numpy()\n",
    "    print(\"Final Test MAE: {:.6f}\".format(final_test_mae))\n",
    "\n",
    "    # Save the result\n",
    "    mae_results[time_steps] = final_test_mae\n",
    "\n",
    "# Save MAE results to a text file\n",
    "with open('mae_results.txt', 'w') as file:\n",
    "    for time_steps, mae in mae_results.items():\n",
    "        file.write(f\"{time_steps}: {mae}\\n\")\n",
    "\n",
    "print(\"MAE results saved to mae_results.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex\n"
     ]
    }
   ],
   "source": [
    "print(\"Alex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
