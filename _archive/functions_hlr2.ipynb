{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7f22f0c-bf90-4707-9af2-a86324fa1297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af4afa3a-f26d-46ad-a55c-eb15f1735bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_h_hat( theta, x ):\n",
    "    \n",
    "    '''\n",
    "    Calculate the estimated half-life h_hat.\n",
    "\n",
    "    Parameters:\n",
    "    - theta (np.array): Model parameters.\n",
    "    - x (np.array): Feature vector for a data instance.\n",
    "\n",
    "    Returns:\n",
    "    - float: Estimated half-life h_hat.\n",
    "    '''\n",
    "    \n",
    "    theta_x = np.clip( theta.dot( x ), -10, 10 )\n",
    "    h_hat   = 2 ** theta_x\n",
    "    \n",
    "    return h_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01b963f8-f53b-48a8-b5c2-0c5280815215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_p_hat( h_hat, Delta ):\n",
    "    \n",
    "    '''\n",
    "    Calculate the predicted probability of recall p_hat.\n",
    "\n",
    "    Parameters:\n",
    "    - h_hat (float): Estimated half-life.\n",
    "    - Delta (float): Lag time since the item was last practiced.\n",
    "\n",
    "    Returns:\n",
    "    - float: Predicted probability of recall p_hat.\n",
    "    '''\n",
    "    \n",
    "    p_hat = 2 ** np.clip( -Delta / h_hat, -10, 10 )  # Prevent underflow/overflow\n",
    "    \n",
    "    return p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11c5e24b-ce67-4c46-91ff-f60ca3fbff7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost_function( theta, X, p, Delta, D, alpha, lambda_, epsilon ):\n",
    "    \n",
    "    '''\n",
    "    Calculate the cost for the given theta values.\n",
    "\n",
    "    Parameters:\n",
    "    - theta (np.array): Model parameters.\n",
    "    - X (np.array): Feature vectors for each data instance.\n",
    "    - p (np.array): Observed recall rates.\n",
    "    - Delta (np.array): Lag times.\n",
    "    - alpha (float): Weight for the half-life term in the loss function.\n",
    "    - lambda_ (float): Regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "    - float: Total cost.\n",
    "    \n",
    "    '''\n",
    "    total_cost = 0\n",
    "\n",
    "    for t in range( D ):\n",
    "        h_hat = find_h_hat( theta, X[ t ] )\n",
    "        p_hat = find_p_hat( h_hat, Delta[ t ] )\n",
    "\n",
    "        # if p[ t ] > 0:\n",
    "        #     h = -Delta[ t ] / np.log2( p[ t ] )\n",
    "        # else:\n",
    "        #     h = 0\n",
    "        \n",
    "        h = h = -Delta[ t ] / np.log2( p[ t ] + epsilon )\n",
    "\n",
    "        total_cost += ( p[ t ] - p_hat ) ** 2 + alpha * ( h - h_hat ) ** 2\n",
    "\n",
    "    total_cost += lambda_ * np.sum( theta ** 2 )\n",
    "    \n",
    "    return total_cost / D  # Average cost per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97b4a120-f851-4244-a793-213539d8e9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adagrad_optimization(X, p, Delta, alpha, lambda_, eta, D, n_iter, tolerance=1e-5, max_iter=200000, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Optimize theta using the AdaGrad optimization algorithm, stopping when convergence is reached.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Feature vectors for each data instance.\n",
    "    - p (np.array): Observed recall rates.\n",
    "    - Delta (np.array): Lag times since each item was last practiced.\n",
    "    - alpha (float): Weight for the half-life term in the loss function.\n",
    "    - lambda_ (float): Regularization parameter.\n",
    "    - eta (float): Learning rate.\n",
    "    - theta_initial (np.array): Initial theta values.\n",
    "    - tolerance (float): Tolerance level for convergence.\n",
    "    - max_iter (int): Maximum number of iterations.\n",
    "    - epsilon (float): Small constant to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Optimized theta values.\n",
    "    - list: Cost per iteration.\n",
    "    \"\"\"\n",
    "    theta = np.random.randn( X.shape[ 1 ] )\n",
    "    grad_accumulation = np.zeros_like(theta)\n",
    "    cost_history = []\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        grad_theta = np.zeros_like(theta)\n",
    "        cost = cost_function(theta, X, p, Delta, D, alpha, lambda_, epsilon )\n",
    "\n",
    "        for t in range(len(X)):\n",
    "            h_hat = find_h_hat(theta, X[t])\n",
    "            p_hat = find_p_hat(h_hat, Delta[t])\n",
    "\n",
    "            # Compute the gradients for each theta\n",
    "            for k in range(len(theta)):\n",
    "                term1 = 2 * (p[t] - p_hat) * np.log(2) * p_hat * (2 ** (-Delta[t] / h_hat)) * X[t][k]\n",
    "                term2 = 2 * alpha * (h_hat + Delta[t] / np.log2(p[t])) * np.log(2) * h_hat * X[t][k]\n",
    "                term3 = 2 * lambda_ * theta[k]\n",
    "                grad_theta[k] += term1 + term2 + term3\n",
    "\n",
    "        # Update the accumulated gradient\n",
    "        grad_accumulation += grad_theta ** 2\n",
    "\n",
    "        # Update theta using AdaGrad adjustment\n",
    "        adjusted_eta = eta / (np.sqrt(grad_accumulation) + epsilon)\n",
    "        theta -= adjusted_eta * grad_theta / len(X)\n",
    "\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if iteration % n_iter == 0:\n",
    "            print( f'At iteration { iteration } cost is { cost }' )\n",
    "\n",
    "        # Check for convergence\n",
    "        if iteration > 0 and np.abs(cost_history[-1] - cost_history[-2]) < tolerance:\n",
    "            break\n",
    "\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c66ed-a655-4477-9805-54573ac44407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbbe4b7-5fd0-4b96-bf01-04b484544a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba69dc1e-bf77-4aab-8575-8ced2a7da457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class HalfLifeRegressionModel(object):\n",
    "    def __init__(self, lrate=.001, hlwt=.01, l2wt=.1, sigma=1., feature_columns=[]):\n",
    "        self.weights = defaultdict(float)\n",
    "        self.fcounts = defaultdict(int)\n",
    "        self.lrate = lrate\n",
    "        self.hlwt = hlwt\n",
    "        self.l2wt = l2wt\n",
    "        self.sigma = sigma\n",
    "        self.feature_columns = feature_columns\n",
    "\n",
    "    def halflife(self, features):\n",
    "        try:\n",
    "            dp = sum([self.weights[k]*features[k] for k in self.feature_columns])\n",
    "            return max(min(2 ** dp, MAX_HALF_LIFE), MIN_HALF_LIFE)\n",
    "        except:\n",
    "            return MAX_HALF_LIFE\n",
    "\n",
    "    def predict(self, row):\n",
    "        h = self.halflife(row)\n",
    "        p = 2 ** (-row['t']/h)\n",
    "        return max(min(p, 0.9999), 0.0001), h\n",
    "\n",
    "    def train_update(self, row):\n",
    "        p, h = self.predict(row)\n",
    "        dlp_dw = 2.*(p-row['p'])*(LN2**2)*p*(row['t']/h)\n",
    "        dlh_dw = 2.*(h-row['h'])*LN2*h\n",
    "        for feature in self.feature_columns:\n",
    "            rate = self.lrate / math.sqrt(1 + self.fcounts[feature])\n",
    "            x_k = row[feature]\n",
    "            self.weights[feature] -= rate * dlp_dw * x_k\n",
    "            self.weights[feature] -= rate * self.hlwt * dlh_dw * x_k\n",
    "            self.weights[feature] -= rate * self.l2wt * self.weights[feature] / self.sigma**2\n",
    "            self.fcounts[feature] += 1\n",
    "\n",
    "    def train(self, dataframe):\n",
    "        for _, row in dataframe.iterrows():\n",
    "            self.train_update(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8094f56b-b6a8-4b66-bc6c-4a9f82a5bfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>t</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p  t  feature1  feature2   h\n",
       "0  0.8  1       0.5       0.2  50\n",
       "1  0.6  2       0.3       0.4  60"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrenamiento\n",
    "data = {\n",
    "    'p': [0.8, 0.6],\n",
    "    't': [1, 2],\n",
    "    'feature1': [0.5, 0.3],\n",
    "    'feature2': [0.2, 0.4],\n",
    "    'h': [50, 60]\n",
    "}\n",
    "\n",
    "train_df = pd.DataFrame(data)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5923256d-198f-4df2-aada-e09ceec548ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15573727682682853,\n",
       " 0.156422227129307,\n",
       " 0.15691298093920147,\n",
       " 0.15731725736031754,\n",
       " 0.15766986359083204,\n",
       " 0.15798716000770952,\n",
       " 0.15827836774438186,\n",
       " 0.1585492857867951,\n",
       " 0.15880384041974052,\n",
       " 0.1590448355278803,\n",
       " 0.15927435592395062,\n",
       " 0.15949400161206267,\n",
       " 0.15970503224861365,\n",
       " 0.15990846058434396,\n",
       " 0.16010511531260102,\n",
       " 0.16029568473879108,\n",
       " 0.16048074797136075,\n",
       " 0.16066079773296177,\n",
       " 0.16083625738886043,\n",
       " 0.16100749388904,\n",
       " 0.16117482776198844,\n",
       " 0.16133854094162853,\n",
       " 0.16149888297528864,\n",
       " 0.16165607600406318,\n",
       " 0.1618103187998084,\n",
       " 0.1619617900683712,\n",
       " 0.16211065117574652,\n",
       " 0.16225704841578584,\n",
       " 0.16240111491029918,\n",
       " 0.16254297221185698,\n",
       " 0.16268273166424002,\n",
       " 0.1628204955638732,\n",
       " 0.16295635815670795,\n",
       " 0.16309040649818046,\n",
       " 0.16322272119855216,\n",
       " 0.1633533770717674,\n",
       " 0.16348244370266862,\n",
       " 0.1636099859447826,\n",
       " 0.16373606435878985,\n",
       " 0.16386073560009562,\n",
       " 0.16398405276254008,\n",
       " 0.16410606568416786,\n",
       " 0.16422682122005267,\n",
       " 0.16434636348641396,\n",
       " 0.16446473407963352,\n",
       " 0.16458197227325694,\n",
       " 0.16469811519562422,\n",
       " 0.1648131979904086,\n",
       " 0.1649272539620318,\n",
       " 0.16504031470766095,\n",
       " 0.16515241023726998,\n",
       " 0.16526356908305967,\n",
       " 0.16537381839936538,\n",
       " 0.1654831840540456,\n",
       " 0.16559169071222124,\n",
       " 0.16569936191313522,\n",
       " 0.16580622014080976,\n",
       " 0.16591228688910303,\n",
       " 0.16601758272169814,\n",
       " 0.16612212732749837,\n",
       " 0.1662259395718524,\n",
       " 0.16632903754398537,\n",
       " 0.16643143860097576,\n",
       " 0.1665331594085782,\n",
       " 0.16663421597916664,\n",
       " 0.16673462370704065,\n",
       " 0.16683439740131664,\n",
       " 0.16693355131660176,\n",
       " 0.16703209918163173,\n",
       " 0.16713005422603394,\n",
       " 0.16722742920536518,\n",
       " 0.16732423642455707,\n",
       " 0.16742048775989082,\n",
       " 0.16751619467961448,\n",
       " 0.16761136826330106,\n",
       " 0.16770601922004283,\n",
       " 0.16780015790556424,\n",
       " 0.16789379433833201,\n",
       " 0.16798693821473348,\n",
       " 0.16807959892338703,\n",
       " 0.16817178555864598,\n",
       " 0.16826350693334985,\n",
       " 0.1683547715908736,\n",
       " 0.16844558781652266,\n",
       " 0.16853596364831475,\n",
       " 0.16862590688719006,\n",
       " 0.16871542510668586,\n",
       " 0.1688045256621088,\n",
       " 0.16889321569923726,\n",
       " 0.16898150216258284,\n",
       " 0.1690693918032371,\n",
       " 0.16915689118633015,\n",
       " 0.1692440066981223,\n",
       " 0.1693307445527531,\n",
       " 0.16941711079866556,\n",
       " 0.1695031113247255,\n",
       " 0.16958875186605374,\n",
       " 0.16967403800958647,\n",
       " 0.16975897519938055,\n",
       " 0.16984356874167605,\n",
       " 0.1699278238097314,\n",
       " 0.1700117454484422,\n",
       " 0.17009533857875558,\n",
       " 0.17017860800189194,\n",
       " 0.1702615584033828,\n",
       " 0.17034419435693593,\n",
       " 0.1704265203281355,\n",
       " 0.17050854067798676,\n",
       " 0.1705902596663123,\n",
       " 0.17067168145500805,\n",
       " 0.17075281011116572,\n",
       " 0.17083364961006836,\n",
       " 0.1709142038380655,\n",
       " 0.17099447659533318,\n",
       " 0.17107447159852535,\n",
       " 0.1711541924833207,\n",
       " 0.17123364280687114,\n",
       " 0.1713128260501553,\n",
       " 0.17139174562024256,\n",
       " 0.17147040485247117,\n",
       " 0.17154880701254438,\n",
       " 0.17162695529854866,\n",
       " 0.17170485284289727,\n",
       " 0.1717825027142025,\n",
       " 0.17185990791908023,\n",
       " 0.17193707140388864,\n",
       " 0.1720139960564055,\n",
       " 0.17209068470744554,\n",
       " 0.17216714013242082,\n",
       " 0.17224336505284682,\n",
       " 0.17231936213779597,\n",
       " 0.1723951340053015,\n",
       " 0.17247068322371306,\n",
       " 0.17254601231300654,\n",
       " 0.17262112374605,\n",
       " 0.1726960199498268,\n",
       " 0.17277070330661878,\n",
       " 0.1728451761551503,\n",
       " 0.17291944079169463,\n",
       " 0.1729934994711449,\n",
       " 0.17306735440805032,\n",
       " 0.17314100777761932,\n",
       " 0.1732144617166908,\n",
       " 0.17328771832467504,\n",
       " 0.17336077966446467,\n",
       " 0.17343364776331782,\n",
       " 0.17350632461371351,\n",
       " 0.17357881217418142,\n",
       " 0.17365111237010578,\n",
       " 0.17372322709450555,\n",
       " 0.17379515820879038,\n",
       " 0.17386690754349482,\n",
       " 0.17393847689899028,\n",
       " 0.17400986804617602,\n",
       " 0.17408108272715034,\n",
       " 0.1741521226558615,\n",
       " 0.1742229895187407,\n",
       " 0.17429368497531633,\n",
       " 0.1743642106588107,\n",
       " 0.17443456817672015,\n",
       " 0.17450475911137875,\n",
       " 0.1745747850205058,\n",
       " 0.174644647437739,\n",
       " 0.17471434787315165,\n",
       " 0.17478388781375692,\n",
       " 0.17485326872399723,\n",
       " 0.1749224920462214,\n",
       " 0.1749915592011477,\n",
       " 0.17506047158831586,\n",
       " 0.17512923058652574,\n",
       " 0.17519783755426532,\n",
       " 0.1752662938301269,\n",
       " 0.17533460073321258,\n",
       " 0.17540275956352913,\n",
       " 0.17547077160237265,\n",
       " 0.17553863811270332,\n",
       " 0.17560636033951055,\n",
       " 0.17567393951016883,\n",
       " 0.17574137683478458,\n",
       " 0.17580867350653442,\n",
       " 0.17587583070199458,\n",
       " 0.1759428495814624,\n",
       " 0.17600973128927005,\n",
       " 0.17607647695409,\n",
       " 0.1761430876892336,\n",
       " 0.17620956459294176,\n",
       " 0.17627590874866927,\n",
       " 0.17634212122536158,\n",
       " 0.17640820307772564,\n",
       " 0.17647415534649372,\n",
       " 0.17653997905868118,\n",
       " 0.17660567522783824,\n",
       " 0.17667124485429583,\n",
       " 0.17673668892540537,\n",
       " 0.17680200841577365,\n",
       " 0.17686720428749153,\n",
       " 0.17693227749035795,\n",
       " 0.17699722896209846,\n",
       " 0.17706205962857904,\n",
       " 0.17712677040401503,\n",
       " 0.17719136219117482,\n",
       " 0.17725583588158011,\n",
       " 0.17732019235570057,\n",
       " 0.17738443248314484,\n",
       " 0.1774485571228469,\n",
       " 0.17751256712324884,\n",
       " 0.17757646332247912,\n",
       " 0.17764024654852723,\n",
       " 0.17770391761941476,\n",
       " 0.17776747734336218,\n",
       " 0.17783092651895246,\n",
       " 0.1778942659352914,\n",
       " 0.17795749637216401,\n",
       " 0.17802061860018809,\n",
       " 0.17808363338096403,\n",
       " 0.1781465414672222,\n",
       " 0.17820934360296653,\n",
       " 0.17827204052361553,\n",
       " 0.17833463295614038,\n",
       " 0.17839712161920007,\n",
       " 0.17845950722327364,\n",
       " 0.17852179047079011,\n",
       " 0.17858397205625554,\n",
       " 0.17864605266637726,\n",
       " 0.17870803298018625,\n",
       " 0.1787699136691564,\n",
       " 0.17883169539732185,\n",
       " 0.17889337882139175,\n",
       " 0.17895496459086302,\n",
       " 0.17901645334813038,\n",
       " 0.17907784572859486,\n",
       " 0.17913914236076975,\n",
       " 0.17920034386638473,\n",
       " 0.17926145086048778,\n",
       " 0.1793224639515453,\n",
       " 0.17938338374154023,\n",
       " 0.17944421082606843,\n",
       " 0.1795049457944328,\n",
       " 0.17956558922973637,\n",
       " 0.17962614170897284,\n",
       " 0.17968660380311605,\n",
       " 0.17974697607720722,\n",
       " 0.17980725909044112,\n",
       " 0.1798674533962502,\n",
       " 0.17992755954238748,\n",
       " 0.17998757807100768,\n",
       " 0.18004750951874698,\n",
       " 0.18010735441680134,\n",
       " 0.18016711329100327,\n",
       " 0.18022678666189756,\n",
       " 0.18028637504481504,\n",
       " 0.18034587894994558,\n",
       " 0.18040529888240972,\n",
       " 0.18046463534232848,\n",
       " 0.1805238888248926,\n",
       " 0.18058305982043033,\n",
       " 0.1806421488144739,\n",
       " 0.1807011562878248,\n",
       " 0.18076008271661814,\n",
       " 0.18081892857238596,\n",
       " 0.18087769432211875,\n",
       " 0.1809363804283271,\n",
       " 0.18099498734910108,\n",
       " 0.1810535155381694,\n",
       " 0.1811119654449571,\n",
       " 0.1811703375146427,\n",
       " 0.18122863218821395,\n",
       " 0.18128684990252286,\n",
       " 0.1813449910903399,\n",
       " 0.18140305618040697,\n",
       " 0.18146104559748985,\n",
       " 0.18151895976242943,\n",
       " 0.1815767990921925,\n",
       " 0.18163456399992128,\n",
       " 0.18169225489498236,\n",
       " 0.181749872183015,\n",
       " 0.18180741626597807,\n",
       " 0.181864887542197,\n",
       " 0.1819222864064094,\n",
       " 0.18197961324980996,\n",
       " 0.18203686846009517,\n",
       " 0.18209405242150656,\n",
       " 0.18215116551487381,\n",
       " 0.18220820811765698,\n",
       " 0.1822651806039882,\n",
       " 0.18232208334471212,\n",
       " 0.18237891670742695,\n",
       " 0.18243568105652333,\n",
       " 0.18249237675322383,\n",
       " 0.1825490041556212,\n",
       " 0.18260556361871622,\n",
       " 0.18266205549445486,\n",
       " 0.18271848013176498,\n",
       " 0.18277483787659252,\n",
       " 0.18283112907193688,\n",
       " 0.18288735405788592,\n",
       " 0.18294351317165064,\n",
       " 0.18299960674759885,\n",
       " 0.18305563511728873,\n",
       " 0.18311159860950182,\n",
       " 0.18316749755027528,\n",
       " 0.18322333226293394,\n",
       " 0.18327910306812176,\n",
       " 0.18333481028383286,\n",
       " 0.183390454225442,\n",
       " 0.18344603520573463,\n",
       " 0.1835015535349367,\n",
       " 0.18355700952074372,\n",
       " 0.18361240346834967,\n",
       " 0.18366773568047523,\n",
       " 0.1837230064573957,\n",
       " 0.18377821609696887,\n",
       " 0.1838333648946618,\n",
       " 0.1838884531435777,\n",
       " 0.18394348113448256,\n",
       " 0.18399844915583063,\n",
       " 0.18405335749379048,\n",
       " 0.18410820643227016,\n",
       " 0.18416299625294202,\n",
       " 0.18421772723526733,\n",
       " 0.1842723996565206,\n",
       " 0.18432701379181304,\n",
       " 0.1843815699141166,\n",
       " 0.18443606829428705,\n",
       " 0.1844905092010865,\n",
       " 0.18454489290120646,\n",
       " 0.18459921965928977,\n",
       " 0.18465348973795281,\n",
       " 0.18470770339780676,\n",
       " 0.18476186089747948,\n",
       " 0.18481596249363597,\n",
       " 0.1848700084409997,\n",
       " 0.18492399899237277,\n",
       " 0.18497793439865623,\n",
       " 0.18503181490886997,\n",
       " 0.1850856407701724,\n",
       " 0.18513941222788002,\n",
       " 0.18519312952548633,\n",
       " 0.18524679290468082,\n",
       " 0.18530040260536773,\n",
       " 0.18535395886568434,\n",
       " 0.18540746192201896,\n",
       " 0.18546091200902928,\n",
       " 0.18551430935965962,\n",
       " 0.18556765420515853,\n",
       " 0.18562094677509622,\n",
       " 0.18567418729738105,\n",
       " 0.185727375998277,\n",
       " 0.18578051310241958,\n",
       " 0.18583359883283254,\n",
       " 0.18588663341094394,\n",
       " 0.185939617056602,\n",
       " 0.1859925499880909,\n",
       " 0.18604543242214633,\n",
       " 0.18609826457397072,\n",
       " 0.18615104665724835,\n",
       " 0.1862037788841605,\n",
       " 0.18625646146539995,\n",
       " 0.18630909461018574,\n",
       " 0.18636167852627744,\n",
       " 0.18641421341998937,\n",
       " 0.1864666994962046,\n",
       " 0.18651913695838898,\n",
       " 0.18657152600860455,\n",
       " 0.18662386684752336,\n",
       " 0.18667615967444035,\n",
       " 0.1867284046872871,\n",
       " 0.18678060208264446,\n",
       " 0.18683275205575545,\n",
       " 0.18688485480053818,\n",
       " 0.1869369105095981,\n",
       " 0.18698891937424061,\n",
       " 0.18704088158448326,\n",
       " 0.18709279732906778,\n",
       " 0.18714466679547212,\n",
       " 0.18719649016992213,\n",
       " 0.18724826763740338,\n",
       " 0.1872999993816726,\n",
       " 0.18735168558526916,\n",
       " 0.1874033264295261,\n",
       " 0.18745492209458148,\n",
       " 0.1875064727593893,\n",
       " 0.1875579786017304,\n",
       " 0.18760943979822303,\n",
       " 0.18766085652433362,\n",
       " 0.18771222895438727,\n",
       " 0.187763557261578,\n",
       " 0.18781484161797907,\n",
       " 0.18786608219455292,\n",
       " 0.1879172791611616,\n",
       " 0.18796843268657615,\n",
       " 0.18801954293848683,\n",
       " 0.18807061008351236,\n",
       " 0.18812163428720968,\n",
       " 0.18817261571408356,\n",
       " 0.18822355452759554,\n",
       " 0.1882744508901736,\n",
       " 0.18832530496322086,\n",
       " 0.18837611690712486,\n",
       " 0.18842688688126658,\n",
       " 0.18847761504402882,\n",
       " 0.18852830155280537,\n",
       " 0.18857894656400942,\n",
       " 0.18862955023308206,\n",
       " 0.18868011271450097,\n",
       " 0.1887306341617883,\n",
       " 0.18878111472751935,\n",
       " 0.1888315545633304,\n",
       " 0.18888195381992712,\n",
       " 0.18893231264709204,\n",
       " 0.18898263119369313,\n",
       " 0.18903290960769076,\n",
       " 0.18908314803614623,\n",
       " 0.18913334662522888,\n",
       " 0.18918350552022373,\n",
       " 0.18923362486553907,\n",
       " 0.18928370480471385,\n",
       " 0.1893337454804247,\n",
       " 0.18938374703449362,\n",
       " 0.18943370960789468,\n",
       " 0.18948363334076138,\n",
       " 0.18953351837239363,\n",
       " 0.18958336484126434,\n",
       " 0.1896331728850268,\n",
       " 0.18968294264052105,\n",
       " 0.18973267424378065,\n",
       " 0.1897823678300396,\n",
       " 0.18983202353373835,\n",
       " 0.18988164148853098,\n",
       " 0.18993122182729089,\n",
       " 0.1899807646821179,\n",
       " 0.19003027018434399,\n",
       " 0.19007973846453952,\n",
       " 0.19012916965252002,\n",
       " 0.19017856387735155,\n",
       " 0.19022792126735694,\n",
       " 0.1902772419501222,\n",
       " 0.19032652605250183,\n",
       " 0.19037577370062492,\n",
       " 0.19042498501990116,\n",
       " 0.19047416013502605,\n",
       " 0.19052329916998711,\n",
       " 0.1905724022480692,\n",
       " 0.19062146949186015,\n",
       " 0.1906705010232561,\n",
       " 0.1907194969634673,\n",
       " 0.19076845743302312,\n",
       " 0.19081738255177771,\n",
       " 0.19086627243891505,\n",
       " 0.19091512721295423,\n",
       " 0.19096394699175465,\n",
       " 0.19101273189252127,\n",
       " 0.19106148203180945,\n",
       " 0.19111019752553005,\n",
       " 0.19115887848895463,\n",
       " 0.1912075250367199,\n",
       " 0.1912561372828332,\n",
       " 0.19130471534067678,\n",
       " 0.19135325932301284,\n",
       " 0.19140176934198833,\n",
       " 0.19145024550913936,\n",
       " 0.19149868793539626,\n",
       " 0.19154709673108777,\n",
       " 0.1915954720059458,\n",
       " 0.19164381386910972,\n",
       " 0.19169212242913145,\n",
       " 0.19174039779397897,\n",
       " 0.19178864007104135,\n",
       " 0.1918368493671328,\n",
       " 0.19188502578849728,\n",
       " 0.1919331694408122,\n",
       " 0.19198128042919316,\n",
       " 0.192029358858198,\n",
       " 0.19207740483183056,\n",
       " 0.19212541845354536,\n",
       " 0.19217339982625117,\n",
       " 0.19222134905231533,\n",
       " 0.19226926623356755,\n",
       " 0.19231715147130396,\n",
       " 0.19236500486629093,\n",
       " 0.1924128265187689,\n",
       " 0.1924606165284564,\n",
       " 0.19250837499455375,\n",
       " 0.19255610201574666,\n",
       " 0.1926037976902103,\n",
       " 0.19265146211561252,\n",
       " 0.19269909538911792,\n",
       " 0.19274669760739135,\n",
       " 0.1927942688666014,\n",
       " 0.19284180926242395,\n",
       " 0.19288931889004599,\n",
       " 0.19293679784416867,\n",
       " 0.1929842462190111,\n",
       " 0.19303166410831357,\n",
       " 0.193079051605341,\n",
       " 0.19312640880288645,\n",
       " 0.19317373579327432,\n",
       " 0.19322103266836355,\n",
       " 0.1932682995195511,\n",
       " 0.1933155364377751,\n",
       " 0.193362743513518,\n",
       " 0.19340992083681002,\n",
       " 0.19345706849723188,\n",
       " 0.19350418658391827,\n",
       " 0.1935512751855609,\n",
       " 0.19359833439041146,\n",
       " 0.1936453642862847,\n",
       " 0.19369236496056152,\n",
       " 0.19373933650019193,\n",
       " 0.19378627899169798,\n",
       " 0.1938331925211768,\n",
       " 0.19388007717430322,\n",
       " 0.19392693303633313,\n",
       " 0.19397376019210597,\n",
       " 0.19402055872604773,\n",
       " 0.1940673287221737,\n",
       " 0.1941140702640912,\n",
       " 0.1941607834350026,\n",
       " 0.19420746831770774,\n",
       " 0.19425412499460687,\n",
       " 0.19430075354770326,\n",
       " 0.19434735405860581,\n",
       " 0.1943939266085318,\n",
       " 0.19444047127830946,\n",
       " 0.19448698814838067,\n",
       " 0.19453347729880333,\n",
       " 0.19457993880925412,\n",
       " 0.19462637275903097,\n",
       " 0.1946727792270555,\n",
       " 0.19471915829187558,\n",
       " 0.1947655100316678,\n",
       " 0.1948118345242401,\n",
       " 0.19485813184703354,\n",
       " 0.19490440207712578,\n",
       " 0.1949506452912324,\n",
       " 0.19499686156571,\n",
       " 0.1950430509765581,\n",
       " 0.1950892135994218,\n",
       " 0.19513534950959394,\n",
       " 0.19518145878201734,\n",
       " 0.19522754149128713,\n",
       " 0.19527359771165292,\n",
       " 0.1953196275170213,\n",
       " 0.19536563098095772,\n",
       " 0.1954116081766889,\n",
       " 0.19545755917710483,\n",
       " 0.1955034840547612,\n",
       " 0.19554938288188126,\n",
       " 0.19559525573035813,\n",
       " 0.19564110267175683,\n",
       " 0.19568692377731625,\n",
       " 0.19573271911795168,\n",
       " 0.19577848876425624,\n",
       " 0.19582423278650324,\n",
       " 0.19586995125464846,\n",
       " 0.19591564423833152,\n",
       " 0.1959613118068786,\n",
       " 0.19600695402930363,\n",
       " 0.196052570974311,\n",
       " 0.1960981627102971,\n",
       " 0.19614372930535218,\n",
       " 0.19618927082726254,\n",
       " 0.1962347873435121,\n",
       " 0.19628027892128472,\n",
       " 0.19632574562746563,\n",
       " 0.19637118752864352,\n",
       " 0.19641660469111225,\n",
       " 0.19646199718087276,\n",
       " 0.1965073650636349,\n",
       " 0.1965527084048191,\n",
       " 0.19659802726955836,\n",
       " 0.19664332172269966,\n",
       " 0.19668859182880616,\n",
       " 0.19673383765215857,\n",
       " 0.19677905925675704,\n",
       " 0.1968242567063228,\n",
       " 0.19686943006430002,\n",
       " 0.19691457939385726,\n",
       " 0.1969597047578892,\n",
       " 0.19700480621901853,\n",
       " 0.1970498838395972,\n",
       " 0.19709493768170844,\n",
       " 0.19713996780716808,\n",
       " 0.19718497427752643,\n",
       " 0.1972299571540695,\n",
       " 0.19727491649782097,\n",
       " 0.19731985236954352,\n",
       " 0.19736476482974052,\n",
       " 0.1974096539386575,\n",
       " 0.1974545197562836,\n",
       " 0.19749936234235327,\n",
       " 0.19754418175634775,\n",
       " 0.1975889780574965,\n",
       " 0.19763375130477867,\n",
       " 0.19767850155692457,\n",
       " 0.19772322887241733,\n",
       " 0.19776793330949413,\n",
       " 0.19781261492614757,\n",
       " 0.19785727378012746,\n",
       " 0.19790190992894202,\n",
       " 0.19794652342985913,\n",
       " 0.1979911143399081,\n",
       " 0.19803568271588068,\n",
       " 0.19808022861433283,\n",
       " 0.19812475209158567,\n",
       " 0.1981692532037272,\n",
       " 0.19821373200661344,\n",
       " 0.19825818855586985,\n",
       " 0.19830262290689255,\n",
       " 0.19834703511484986,\n",
       " 0.19839142523468334,\n",
       " 0.19843579332110933,\n",
       " 0.19848013942862014,\n",
       " 0.19852446361148524,\n",
       " 0.19856876592375267,\n",
       " 0.1986130464192502,\n",
       " 0.19865730515158675,\n",
       " 0.19870154217415337,\n",
       " 0.19874575754012466,\n",
       " 0.19878995130246002,\n",
       " 0.19883412351390478,\n",
       " 0.1988782742269914,\n",
       " 0.1989224034940406,\n",
       " 0.19896651136716287,\n",
       " 0.19901059789825937,\n",
       " 0.19905466313902295,\n",
       " 0.19909870714093975,\n",
       " 0.19914272995529012,\n",
       " 0.19918673163314965,\n",
       " 0.19923071222539057,\n",
       " 0.1992746717826827,\n",
       " 0.19931861035549467,\n",
       " 0.1993625279940949,\n",
       " 0.19940642474855297,\n",
       " 0.19945030066874037,\n",
       " 0.19949415580433183,\n",
       " 0.1995379902048064,\n",
       " 0.1995818039194484,\n",
       " 0.19962559699734875,\n",
       " 0.19966936948740557,\n",
       " 0.1997131214383257,\n",
       " 0.1997568528986255,\n",
       " 0.19980056391663195,\n",
       " 0.1998442545404836,\n",
       " 0.19988792481813183,\n",
       " 0.19993157479734164,\n",
       " 0.19997520452569262,\n",
       " 0.20001881405058025,\n",
       " 0.20006240341921666,\n",
       " 0.20010597267863156,\n",
       " 0.2001495218756734,\n",
       " 0.20019305105701032,\n",
       " 0.20023656026913106,\n",
       " 0.20028004955834594,\n",
       " 0.2003235189707878,\n",
       " 0.20036696855241287,\n",
       " 0.20041039834900215,\n",
       " 0.20045380840616156,\n",
       " 0.20049719876932365,\n",
       " 0.200540569483748,\n",
       " 0.20058392059452237,\n",
       " 0.20062725214656368,\n",
       " 0.2006705641846187,\n",
       " 0.20071385675326503,\n",
       " 0.20075712989691208,\n",
       " 0.20080038365980193,\n",
       " 0.20084361808600992,\n",
       " 0.20088683321944623,\n",
       " 0.2009300291038558,\n",
       " 0.20097320578282002,\n",
       " 0.20101636329975717,\n",
       " 0.20105950169792333,\n",
       " 0.2011026210204132,\n",
       " 0.20114572131016117,\n",
       " 0.20118880260994176,\n",
       " 0.20123186496237086,\n",
       " 0.20127490840990617,\n",
       " 0.20131793299484843,\n",
       " 0.20136093875934183,\n",
       " 0.20140392574537497,\n",
       " 0.20144689399478188,\n",
       " 0.20148984354924251,\n",
       " 0.20153277445028356,\n",
       " 0.2015756867392795,\n",
       " 0.20161858045745307,\n",
       " 0.2016614556458763,\n",
       " 0.20170431234547095,\n",
       " 0.2017471505970097,\n",
       " 0.20178997044111663,\n",
       " 0.20183277191826796,\n",
       " 0.20187555506879296,\n",
       " 0.2019183199328746,\n",
       " 0.2019610665505503,\n",
       " 0.2020037949617127,\n",
       " 0.20204650520611028,\n",
       " 0.20208919732334818,\n",
       " 0.20213187135288915,\n",
       " 0.20217452733405367,\n",
       " 0.20221716530602118,\n",
       " 0.20225978530783084,\n",
       " 0.20230238737838155,\n",
       " 0.20234497155643355,\n",
       " 0.2023875378806086,\n",
       " 0.20243008638939047,\n",
       " 0.20247261712112635,\n",
       " 0.2025151301140267,\n",
       " 0.20255762540616656,\n",
       " 0.20260010303548584,\n",
       " 0.2026425630397903,\n",
       " 0.2026850054567519,\n",
       " 0.20272743032390966,\n",
       " 0.20276983767867016,\n",
       " 0.20281222755830852,\n",
       " 0.20285459999996852,\n",
       " 0.2028969550406638,\n",
       " 0.202939292717278,\n",
       " 0.20298161306656592,\n",
       " 0.20302391612515364,\n",
       " 0.2030662019295394,\n",
       " 0.20310847051609432,\n",
       " 0.20315072192106282,\n",
       " 0.2031929561805632,\n",
       " 0.20323517333058855,\n",
       " 0.20327737340700708,\n",
       " 0.2033195564455628,\n",
       " 0.20336172248187614,\n",
       " 0.20340387155144465,\n",
       " 0.20344600368964327,\n",
       " 0.20348811893172536,\n",
       " 0.2035302173128229,\n",
       " 0.2035722988679473,\n",
       " 0.20361436363198987,\n",
       " 0.20365641163972253,\n",
       " 0.20369844292579814,\n",
       " 0.20374045752475128,\n",
       " 0.20378245547099882,\n",
       " 0.20382443679884027,\n",
       " 0.2038664015424586,\n",
       " 0.20390834973592054,\n",
       " 0.2039502814131775,\n",
       " 0.20399219660806556,\n",
       " 0.20403409535430644,\n",
       " 0.20407597768550811,\n",
       " 0.20411784363516497,\n",
       " 0.20415969323665867,\n",
       " 0.2042015265232583,\n",
       " 0.20424334352812146,\n",
       " 0.2042851442842943,\n",
       " 0.2043269288247123,\n",
       " 0.2043686971822006,\n",
       " 0.20441044938947472,\n",
       " 0.20445218547914093,\n",
       " 0.20449390548369684,\n",
       " 0.20453560943553178,\n",
       " 0.20457729736692748,\n",
       " 0.20461896931005846,\n",
       " 0.20466062529699247,\n",
       " 0.20470226535969116,\n",
       " 0.20474388953001055,\n",
       " 0.20478549783970112,\n",
       " 0.204827090320409,\n",
       " 0.20486866700367581,\n",
       " 0.20491022792093955,\n",
       " 0.20495177310353468,\n",
       " 0.20499330258269322,\n",
       " 0.20503481638954457,\n",
       " 0.20507631455511616,\n",
       " 0.2051177971103344,\n",
       " 0.20515926408602428,\n",
       " 0.20520071551291064,\n",
       " 0.20524215142161809,\n",
       " 0.20528357184267176,\n",
       " 0.20532497680649764,\n",
       " 0.20536636634342292,\n",
       " 0.20540774048367672,\n",
       " 0.20544909925739022,\n",
       " 0.20549044269459743,\n",
       " 0.20553177082523533,\n",
       " 0.20557308367914434,\n",
       " 0.20561438128606907,\n",
       " 0.20565566367565835,\n",
       " 0.2056969308774659,\n",
       " 0.20573818292095064,\n",
       " 0.20577941983547726,\n",
       " 0.20582064165031638,\n",
       " 0.20586184839464539,\n",
       " 0.20590304009754828,\n",
       " 0.20594421678801664,\n",
       " 0.20598537849494972,\n",
       " 0.20602652524715498,\n",
       " 0.2060676570733484,\n",
       " 0.20610877400215497,\n",
       " 0.20614987606210908,\n",
       " 0.20619096328165482,\n",
       " 0.2062320356891466,\n",
       " 0.2062730933128492,\n",
       " 0.2063141361809387,\n",
       " 0.20635516432150217,\n",
       " 0.20639617776253857,\n",
       " 0.2064371765319592,\n",
       " 0.20647816065758756,\n",
       " 0.20651913016716028,\n",
       " 0.20656008508832724,\n",
       " 0.2066010254486518,\n",
       " 0.20664195127561166,\n",
       " 0.20668286259659874,\n",
       " 0.20672375943891977,\n",
       " 0.20676464182979667,\n",
       " 0.20680550979636686,\n",
       " 0.20684636336568374,\n",
       " 0.2068872025647168,\n",
       " 0.20692802742035232,\n",
       " 0.20696883795939336,\n",
       " 0.20700963420856058,\n",
       " 0.2070504161944921,\n",
       " 0.2070911839437442,\n",
       " 0.2071319374827915,\n",
       " 0.20717267683802743,\n",
       " 0.20721340203576444,\n",
       " 0.20725411310223446,\n",
       " 0.20729481006358927,\n",
       " 0.20733549294590065,\n",
       " 0.20737616177516097,\n",
       " 0.20741681657728328,\n",
       " 0.20745745737810195,\n",
       " 0.20749808420337262,\n",
       " 0.20753869707877287,\n",
       " 0.20757929602990244,\n",
       " 0.20761988108228352,\n",
       " 0.20766045226136104,\n",
       " 0.2077010095925031,\n",
       " 0.20774155310100131,\n",
       " 0.207782082812071,\n",
       " 0.20782259875085146,\n",
       " 0.20786310094240676,\n",
       " 0.20790358941172524,\n",
       " 0.20794406418372066,\n",
       " 0.2079845252832319,\n",
       " 0.20802497273502374,\n",
       " 0.2080654065637867,\n",
       " 0.2081058267941377,\n",
       " 0.20814623345062033,\n",
       " 0.20818662655770498,\n",
       " 0.20822700613978923,\n",
       " 0.20826737222119834,\n",
       " 0.20830772482618523,\n",
       " 0.20834806397893088,\n",
       " 0.20838838970354492,\n",
       " 0.20842870202406538,\n",
       " 0.2084690009644595,\n",
       " 0.2085092865486238,\n",
       " 0.20854955880038423,\n",
       " 0.20858981774349677,\n",
       " 0.20863006340164755,\n",
       " 0.20867029579845298,\n",
       " 0.2087105149574604,\n",
       " 0.20875072090214808,\n",
       " 0.20879091365592561,\n",
       " 0.20883109324213406,\n",
       " 0.20887125968404655,\n",
       " 0.20891141300486815,\n",
       " 0.20895155322773645,\n",
       " 0.20899168037572166,\n",
       " 0.20903179447182707,\n",
       " 0.209071895538989,\n",
       " 0.2091119836000774,\n",
       " 0.20915205867789607,\n",
       " 0.20919212079518262,\n",
       " 0.2092321699746092,\n",
       " 0.20927220623878243,\n",
       " 0.2093122296102437,\n",
       " 0.20935224011146963,\n",
       " 0.2093922377648721,\n",
       " 0.2094322225927986,\n",
       " 0.20947219461753258,\n",
       " 0.2095121538612935,\n",
       " 0.20955210034623736,\n",
       " 0.2095920340944567,\n",
       " 0.20963195512798094,\n",
       " 0.2096718634687767,\n",
       " 0.2097117591387479,\n",
       " 0.20975164215973638,\n",
       " 0.20979151255352146,\n",
       " 0.20983137034182092,\n",
       " 0.20987121554629082,\n",
       " 0.2099110481885258,\n",
       " 0.20995086829005943,\n",
       " 0.20999067587236445,\n",
       " 0.21003047095685276,\n",
       " 0.2100702535648761,\n",
       " 0.21011002371772586,\n",
       " 0.21014978143663357,\n",
       " 0.21018952674277103,\n",
       " 0.21022925965725062,\n",
       " 0.21026898020112542,\n",
       " 0.21030868839538963,\n",
       " 0.21034838426097857,\n",
       " 0.2103880678187691,\n",
       " 0.21042773908957968,\n",
       " 0.21046739809417075,\n",
       " 0.21050704485324492,\n",
       " 0.2105466793874472,\n",
       " 0.21058630171736498,\n",
       " 0.2106259118635287,\n",
       " 0.21066550984641186,\n",
       " 0.21070509568643092,\n",
       " 0.21074466940394615,\n",
       " 0.2107842310192613,\n",
       " 0.21082378055262424,\n",
       " 0.21086331802422673,\n",
       " 0.210902843454205,\n",
       " 0.2109423568626399,\n",
       " 0.21098185826955695,\n",
       " 0.21102134769492664,\n",
       " 0.21106082515866476,\n",
       " 0.21110029068063244,\n",
       " 0.21113974428063642,\n",
       " 0.21117918597842938,\n",
       " 0.21121861579370976,\n",
       " 0.21125803374612256,\n",
       " 0.21129743985525895,\n",
       " 0.21133683414065688,\n",
       " 0.21137621662180117,\n",
       " 0.2114155873181236,\n",
       " 0.2114549462490033,\n",
       " 0.21149429343376674,\n",
       " 0.21153362889168809,\n",
       " 0.2115729526419894,\n",
       " 0.2116122647038407,\n",
       " 0.21165156509636035,\n",
       " 0.21169085383861497,\n",
       " 0.21173013094962,\n",
       " 0.21176939644833967,\n",
       " 0.21180865035368712,\n",
       " 0.21184789268452475,\n",
       " 0.21188712345966432,\n",
       " 0.2119263426978673,\n",
       " 0.21196555041784487,\n",
       " 0.21200474663825805,\n",
       " 0.2120439313777182,\n",
       " 0.21208310465478686,\n",
       " 0.21212226648797616,\n",
       " 0.21216141689574902,\n",
       " 0.21220055589651904,\n",
       " 0.21223968350865105,\n",
       " 0.21227879975046116,\n",
       " 0.21231790464021666,\n",
       " 0.21235699819613676,\n",
       " 0.2123960804363923,\n",
       " 0.2124351513791061,\n",
       " 0.21247421104235317,\n",
       " 0.2125132594441608,\n",
       " 0.21255229660250874,\n",
       " 0.21259132253532956,\n",
       " 0.2126303372605085,\n",
       " 0.2126693407958839,\n",
       " 0.2127083331592473,\n",
       " 0.21274731436834365,\n",
       " 0.21278628444087128,\n",
       " 0.2128252433944824,\n",
       " 0.21286419124678285,\n",
       " 0.21290312801533284,\n",
       " 0.21294205371764663,\n",
       " 0.21298096837119268,\n",
       " 0.21301987199339428,\n",
       " 0.2130587646016293,\n",
       " 0.21309764621323043,\n",
       " 0.21313651684548543,\n",
       " 0.21317537651563728,\n",
       " 0.21321422524088435,\n",
       " 0.21325306303838043,\n",
       " 0.21329188992523507,\n",
       " 0.21333070591851366,\n",
       " 0.21336951103523752,\n",
       " 0.21340830529238425,\n",
       " 0.21344708870688764,\n",
       " 0.21348586129563796,\n",
       " 0.2135246230754822,\n",
       " 0.2135633740632241,\n",
       " 0.2136021142756243,\n",
       " 0.21364084372940056,\n",
       " 0.21367956244122788,\n",
       " 0.21371827042773872,\n",
       " 0.2137569677055229,\n",
       " 0.21379565429112815,\n",
       " 0.21383433020106,\n",
       " 0.21387299545178196,\n",
       " 0.21391165005971563,\n",
       " 0.213950294041241,\n",
       " 0.21398892741269643,\n",
       " 0.214027550190379,\n",
       " 0.21406616239054443,\n",
       " 0.21410476402940737,\n",
       " 0.21414335512314145,\n",
       " 0.2141819356878795,\n",
       " 0.21422050573971363,\n",
       " 0.21425906529469552,\n",
       " 0.21429761436883646,\n",
       " 0.21433615297810735,\n",
       " 0.21437468113843905,\n",
       " ...]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = ['feature1', 'feature2']\n",
    "model = HalfLifeRegressionModel(feature_columns=feature_columns)\n",
    "model.train(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c27c8daf-2f0f-4709-babd-8f097da1e77f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t  feature1  feature2\n",
       "0  1.5       0.4       0.3\n",
       "1  2.0       0.6       0.1\n",
       "2  0.5       0.2       0.5\n",
       "3  1.0       0.3       0.4\n",
       "4  2.5       0.5       0.2"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de prueba\n",
    "test_data = {\n",
    "    't': [1.5, 2.0, 0.5, 1.0, 2.5],\n",
    "    'feature1': [0.4, 0.6, 0.2, 0.3, 0.5],\n",
    "    'feature2': [0.3, 0.1, 0.5, 0.4, 0.2]\n",
    "}\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "09aacb8f-c7f5-4585-a9e1-7a2d212b272e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>predicted_p</th>\n",
       "      <th>predicted_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.378945</td>\n",
       "      <td>1.071474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.11918</td>\n",
       "      <td>0.651723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.821403</td>\n",
       "      <td>1.76157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.603791</td>\n",
       "      <td>1.373855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.125721</td>\n",
       "      <td>0.835646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t  feature1  feature2 predicted_p predicted_h\n",
       "0  1.5       0.4       0.3    0.378945    1.071474\n",
       "1  2.0       0.6       0.1     0.11918    0.651723\n",
       "2  0.5       0.2       0.5    0.821403     1.76157\n",
       "3  1.0       0.3       0.4    0.603791    1.373855\n",
       "4  2.5       0.5       0.2    0.125721    0.835646"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = test_df.copy()\n",
    "results['predicted_p'] = None\n",
    "results['predicted_h'] = None\n",
    "\n",
    "# Hacer la prediccin para cada fila\n",
    "for index, row in test_df.iterrows():\n",
    "    predicted_p, predicted_h = model.predict(row)\n",
    "    results.at[index, 'predicted_p'] = predicted_p\n",
    "    results.at[index, 'predicted_h'] = predicted_h\n",
    "\n",
    "# Mostrar los resultados\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9cf90fca-2129-4517-98f2-dbf04f121480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t  feature1  feature2\n",
       "0  1.5       0.4       0.3\n",
       "1  2.0       0.6       0.1\n",
       "2  0.5       0.2       0.5\n",
       "3  1.0       0.3       0.4\n",
       "4  2.5       0.5       0.2"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f20dd7-e77b-40a0-bd72-44b9f87e6050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188ada9-58a3-4d11-aece-6c8d365e0397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a351ab8-3c9e-45cc-9fbc-11f0d6e412e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "efdf8b67-84f4-486a-99f7-520ac78ad861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ejemplp pequeo\n",
    "\n",
    "# Creating a sample DataFrame to use with the optimize_theta function\n",
    "data = {\n",
    "    \"feature1\": [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    \"feature2\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"feature3\": [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    \"recall_rate\": [0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "    \"lag_time\": [1, 2, 3, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Converting DataFrame columns to numpy arrays\n",
    "X = df[[\"feature1\", \"feature2\", \"feature3\"]].values\n",
    "p = df[\"recall_rate\"].values\n",
    "Delta = df[\"lag_time\"].values\n",
    "\n",
    "# Set the parameters for optimization\n",
    "D = len(df)  # Number of data instances\n",
    "alpha = 0.5\n",
    "lambda_ = 0.1\n",
    "eta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da7fcbe-c8a7-474c-aa4d-18804c63ee66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669e3b4-a319-4bc4-bb71-d495908b1c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf19f2-8759-4a68-968e-054b52f3b497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d04ca-10c0-40da-a56c-07eec3d1b0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6974b45e-94cd-40aa-b424-cb0ac83174df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class HalfLifeRegressionModel:\n",
    "    def __init__(self, feature_columns, alpha=0.01, lambda_=0.1, eta=0.01, epsilon=1e-8):\n",
    "        self.theta = np.random.randn(len(feature_columns))\n",
    "        self.feature_columns = feature_columns\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _find_h_hat(self, x):\n",
    "        theta_x = np.clip(np.dot(self.theta, x), -10, 10)\n",
    "        h_hat = 2 ** theta_x\n",
    "        return h_hat\n",
    "\n",
    "    def _find_p_hat(self, h_hat, Delta):\n",
    "        p_hat = 2 ** np.clip(-Delta / h_hat, -10, 10)  # Prevent underflow/overflow\n",
    "        return p_hat\n",
    "\n",
    "    def _cost_function(self, X, p, Delta):\n",
    "        D = len(X)\n",
    "        total_cost = 0\n",
    "\n",
    "        for t in range(D):\n",
    "            h_hat = self._find_h_hat(X[t])\n",
    "            p_hat = self._find_p_hat(h_hat, Delta[t])\n",
    "            h = -Delta[t] / np.log2(p[t] + self.epsilon)\n",
    "            total_cost += (p[t] - p_hat) ** 2 + self.alpha * (h - h_hat) ** 2\n",
    "\n",
    "        total_cost += self.lambda_ * np.sum(self.theta ** 2)\n",
    "        return total_cost / D  # Average cost per instance\n",
    "\n",
    "    def train(self, dataframe, n_iter=10000, tolerance=1e-5):\n",
    "        X = dataframe[self.feature_columns].values\n",
    "        p = dataframe['p'].values\n",
    "        Delta = dataframe['t'].values\n",
    "        grad_accumulation = np.zeros_like(self.theta)\n",
    "        cost_history = []\n",
    "\n",
    "        for iteration in range(n_iter):\n",
    "            grad_theta = np.zeros_like(self.theta)\n",
    "            cost = self._cost_function(X, p, Delta)\n",
    "\n",
    "            for t in range(len(X)):\n",
    "                h_hat = self._find_h_hat(X[t])\n",
    "                p_hat = self._find_p_hat(h_hat, Delta[t])\n",
    "\n",
    "                # Compute the gradients for each theta\n",
    "                for k in range(len(self.theta)):\n",
    "                    term1 = 2 * (p[t] - p_hat) * np.log(2) * p_hat * (2 ** (-Delta[t] / h_hat)) * X[t][k]\n",
    "                    term2 = 2 * self.alpha * (h_hat + Delta[t] / np.log2(p[t])) * np.log(2) * h_hat * X[t][k]\n",
    "                    term3 = 2 * self.lambda_ * self.theta[k]\n",
    "                    grad_theta[k] += term1 + term2 - term3\n",
    "\n",
    "            # Update the accumulated gradient\n",
    "            grad_accumulation += grad_theta ** 2\n",
    "\n",
    "            # Update theta using AdaGrad adjustment\n",
    "            adjusted_eta = self.eta / (np.sqrt(grad_accumulation) + self.epsilon)\n",
    "            self.theta -= adjusted_eta * grad_theta / len(X)\n",
    "\n",
    "            cost_history.append(cost)\n",
    "\n",
    "            # Check for convergence\n",
    "            if iteration > 0 and np.abs(cost_history[-1] - cost_history[-2]) < tolerance:\n",
    "                break\n",
    "\n",
    "        return cost_history\n",
    "\n",
    "    def predict(self, row):\n",
    "        x = np.array([row[feature] for feature in self.feature_columns])\n",
    "        h_hat = self._find_h_hat(x)\n",
    "        p_hat = self._find_p_hat(h_hat, row['t'])\n",
    "        return p_hat, h_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ac848cd-cb4d-4b86-af14-86143ce9aef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548814</td>\n",
       "      <td>0.677817</td>\n",
       "      <td>3.117959</td>\n",
       "      <td>0.906555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.270008</td>\n",
       "      <td>6.963435</td>\n",
       "      <td>0.774047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.735194</td>\n",
       "      <td>3.777518</td>\n",
       "      <td>0.333145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.544883</td>\n",
       "      <td>0.962189</td>\n",
       "      <td>1.796037</td>\n",
       "      <td>0.081101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.423655</td>\n",
       "      <td>0.248753</td>\n",
       "      <td>0.246787</td>\n",
       "      <td>0.407241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.183191</td>\n",
       "      <td>0.490459</td>\n",
       "      <td>2.243170</td>\n",
       "      <td>0.958983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.586513</td>\n",
       "      <td>0.227415</td>\n",
       "      <td>0.978445</td>\n",
       "      <td>0.355369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.020108</td>\n",
       "      <td>0.254356</td>\n",
       "      <td>8.621915</td>\n",
       "      <td>0.356707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.828940</td>\n",
       "      <td>0.058029</td>\n",
       "      <td>9.729195</td>\n",
       "      <td>0.016329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.434417</td>\n",
       "      <td>9.608347</td>\n",
       "      <td>0.185232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature1  feature2         t         p\n",
       "0   0.548814  0.677817  3.117959  0.906555\n",
       "1   0.715189  0.270008  6.963435  0.774047\n",
       "2   0.602763  0.735194  3.777518  0.333145\n",
       "3   0.544883  0.962189  1.796037  0.081101\n",
       "4   0.423655  0.248753  0.246787  0.407241\n",
       "..       ...       ...       ...       ...\n",
       "95  0.183191  0.490459  2.243170  0.958983\n",
       "96  0.586513  0.227415  0.978445  0.355369\n",
       "97  0.020108  0.254356  8.621915  0.356707\n",
       "98  0.828940  0.058029  9.729195  0.016329\n",
       "99  0.004695  0.434417  9.608347  0.185232\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Simulacin de datos de ejemplo\n",
    "np.random.seed(0)  # Para reproducibilidad\n",
    "data_size = 100  # Nmero de filas en el conjunto de datos\n",
    "\n",
    "data = {\n",
    "    'feature1': np.random.rand(data_size),\n",
    "    'feature2': np.random.rand(data_size),\n",
    "    't': np.random.rand(data_size) * 10,  # Supongamos que 't' est en un rango de 0 a 10\n",
    "    'p': np.random.rand(data_size)  # Probabilidad de recuerdo simulada\n",
    "}\n",
    "\n",
    "# Convertir en DataFrame\n",
    "dataframe = pd.DataFrame(data)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a8690-9f58-4a68-a33d-4f656c551d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5492c0c1-6ff1-4db6-8eb1-95a655a49637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = ['feature1', 'feature2']\n",
    "model = HalfLifeRegressionModel(feature_columns)\n",
    "\n",
    "# Entrenar el modelo\n",
    "cost_history = model.train(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3a17616e-00a0-4593-a3b1-458986154d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.47378907355776,\n",
       " 4.47376968813008,\n",
       " 4.473755977744204,\n",
       " 4.4737447817841485,\n",
       " 4.473735084798954]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "86d2e91c-bf55-4aca-a25c-8ee6a9361ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "      <th>predicted_p</th>\n",
       "      <th>predicted_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548814</td>\n",
       "      <td>0.677817</td>\n",
       "      <td>3.117959</td>\n",
       "      <td>0.906555</td>\n",
       "      <td>0.167190</td>\n",
       "      <td>1.208307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.270008</td>\n",
       "      <td>6.963435</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>0.029452</td>\n",
       "      <td>1.369274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.735194</td>\n",
       "      <td>3.777518</td>\n",
       "      <td>0.333145</td>\n",
       "      <td>0.119448</td>\n",
       "      <td>1.232251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.544883</td>\n",
       "      <td>0.962189</td>\n",
       "      <td>1.796037</td>\n",
       "      <td>0.081101</td>\n",
       "      <td>0.344671</td>\n",
       "      <td>1.168755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.423655</td>\n",
       "      <td>0.248753</td>\n",
       "      <td>0.246787</td>\n",
       "      <td>0.407241</td>\n",
       "      <td>0.866406</td>\n",
       "      <td>1.192875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.645894</td>\n",
       "      <td>0.576157</td>\n",
       "      <td>0.672496</td>\n",
       "      <td>0.232234</td>\n",
       "      <td>0.694845</td>\n",
       "      <td>1.280369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.437587</td>\n",
       "      <td>0.592042</td>\n",
       "      <td>6.793928</td>\n",
       "      <td>0.132488</td>\n",
       "      <td>0.017029</td>\n",
       "      <td>1.156243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.891773</td>\n",
       "      <td>0.572252</td>\n",
       "      <td>4.536968</td>\n",
       "      <td>0.053427</td>\n",
       "      <td>0.112907</td>\n",
       "      <td>1.441774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.963663</td>\n",
       "      <td>0.223082</td>\n",
       "      <td>5.365792</td>\n",
       "      <td>0.725594</td>\n",
       "      <td>0.090923</td>\n",
       "      <td>1.551162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.383442</td>\n",
       "      <td>0.952749</td>\n",
       "      <td>8.966713</td>\n",
       "      <td>0.011427</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>1.082540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2         t         p  predicted_p  predicted_h\n",
       "0  0.548814  0.677817  3.117959  0.906555     0.167190     1.208307\n",
       "1  0.715189  0.270008  6.963435  0.774047     0.029452     1.369274\n",
       "2  0.602763  0.735194  3.777518  0.333145     0.119448     1.232251\n",
       "3  0.544883  0.962189  1.796037  0.081101     0.344671     1.168755\n",
       "4  0.423655  0.248753  0.246787  0.407241     0.866406     1.192875\n",
       "5  0.645894  0.576157  0.672496  0.232234     0.694845     1.280369\n",
       "6  0.437587  0.592042  6.793928  0.132488     0.017029     1.156243\n",
       "7  0.891773  0.572252  4.536968  0.053427     0.112907     1.441774\n",
       "8  0.963663  0.223082  5.365792  0.725594     0.090923     1.551162\n",
       "9  0.383442  0.952749  8.966713  0.011427     0.003210     1.082540"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacer predicciones en el mismo conjunto de datos\n",
    "predictions = []\n",
    "for _, row in dataframe.iterrows():\n",
    "    predicted_p, predicted_h = model.predict(row)\n",
    "    predictions.append((predicted_p, predicted_h))\n",
    "\n",
    "# Agregar las predicciones al dataframe para comparar\n",
    "dataframe['predicted_p'] = [pred[0] for pred in predictions]\n",
    "dataframe['predicted_h'] = [pred[1] for pred in predictions]\n",
    "\n",
    "# Mostrar los primeros 10 resultados\n",
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb0736-9622-4dba-af72-b4811e937512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d790ec-cc7c-4c9c-89a1-d8680d26c34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
