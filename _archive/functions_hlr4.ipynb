{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "aa02eded-8744-4334-9af6-1891caca8113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "52be27ce-1d63-493e-b5f6-c21d7879387a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class hlr_adagrad:\n",
    "    def __init__(self, feature_columns, alpha=0.01, lambda_=0.1, eta=0.1, epsilon=0.001):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the Half Life Regression Model with adaptive learning rate (AdaGrad).\n",
    "\n",
    "        :param feature_columns: List of feature names.\n",
    "        :param alpha: Weight for the half-life term in the loss function.\n",
    "        :param lambda_: Regularization parameter.\n",
    "        :param eta: Initial learning rate for AdaGrad.\n",
    "        :param epsilon: Small value to avoid division by zero in AdaGrad.\n",
    "        \"\"\"        \n",
    "\n",
    "        self.theta = np.random.randn( len( feature_columns ) ) * 0.01\n",
    "        self.feature_columns = feature_columns\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _find_h_hat(self, X):\n",
    "        min_h = 15.0 / ( 24 * 60 ) # 15 Mins\n",
    "        max_h = 274 # 9 months\n",
    "        theta_x = X.dot(self.theta)\n",
    "        h_hat = 2 ** theta_x\n",
    "        h_hat = np.clip( h_hat, min_h, max_h )\n",
    "        return h_hat\n",
    "\n",
    "    def _find_p_hat(self, h_hat, Delta):\n",
    "        p_hat = 2 ** -Delta / h_hat\n",
    "        p_hat = np.clip( p_hat, 0.0001, 0.9999 )\n",
    "        return p_hat\n",
    "\n",
    "    def _cost_function(self, X, p, Delta):\n",
    "        h_hat = self._find_h_hat(X)\n",
    "        p_hat = self._find_p_hat(h_hat, Delta)\n",
    "        h = -Delta / np.log2(p + self.epsilon)\n",
    "        total_cost = np.mean((p - p_hat) ** 2 + self.alpha * (h - h_hat) ** 2)\n",
    "        total_cost += self.lambda_ * np.sum(self.theta ** 2)\n",
    "        return total_cost\n",
    "\n",
    "    def train(self, dataframe_x, dataframe_y, max_iter=1000000, tolerance=1e-7, print_iter=1000):\n",
    "        X = dataframe_x[self.feature_columns].values\n",
    "        p = dataframe_y.values.flatten()\n",
    "        Delta = dataframe_x['t'].values\n",
    "        grad_accumulation = np.zeros_like(self.theta)\n",
    "        cost_history = []\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            h_hat = self._find_h_hat(X)\n",
    "            p_hat = self._find_p_hat(h_hat, Delta)\n",
    "            grad_theta = -2 * X.T.dot((p - p_hat) * np.log(2) * p_hat * (2 ** (-Delta / h_hat)) + \n",
    "                                      self.alpha * (h_hat + Delta / np.log2(p)) * np.log(2) * h_hat) + 2 * self.lambda_ * self.theta\n",
    "            grad_accumulation += grad_theta ** 2\n",
    "\n",
    "            adjusted_eta = self.eta / (np.sqrt(grad_accumulation) + self.epsilon)\n",
    "            self.theta -= adjusted_eta * grad_theta / len(X)\n",
    "            cost = self._cost_function(X, p, Delta)\n",
    "            cost_history.append(cost)\n",
    "\n",
    "            if iteration % print_iter == 0:\n",
    "                print(f\"Iteration {iteration}, Loss: {cost}\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if iteration > 0 and np.abs(cost_history[-1] - cost_history[-2]) < tolerance:\n",
    "                print(f\"Convergence reached at iteration {iteration}, Loss {cost}\")\n",
    "                break\n",
    "\n",
    "        return cost_history\n",
    "\n",
    "    def predict(self, row):\n",
    "        x = np.array([row[feature] for feature in self.feature_columns])\n",
    "        h_hat = self._find_h_hat(x[np.newaxis, :])[0]\n",
    "        p_hat = self._find_p_hat(h_hat, row['t'])\n",
    "        \n",
    "        return p_hat, h_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a162a7-2437-4ecc-9378-f081f98b0994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9d48c02b-e651-49ed-b9c9-6d54ec25dd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f59fc8-3b3f-446e-b4a1-bf8670aa03b9",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "da103cc4-717b-4950-9767-92df5601b7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data        = pd.read_csv( 'subset_1000.csv' )\n",
    "dummies     = pd.get_dummies( data[ 'lexeme' ], prefix = 'cat', dtype=float )\n",
    "dummies_col = dummies.columns.to_list()\n",
    "df          = pd.concat( [ data, dummies ], axis = 1 )\n",
    "\n",
    "pred_vars = [ 'right', 'wrong', 'bias', 't' ]\n",
    "dummies_  = dummies_col + pred_vars\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( df[ dummies_ ], \n",
    "                                                     df[ 'p' ], \n",
    "                                                     test_size    = 0.30,\n",
    "                                                     random_state = 7 )\n",
    "\n",
    "dummies_.remove('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8fe09adf-d1c3-451f-8b33-4b0e2b46b538",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_en:'/'&lt;apos&gt;</th>\n",
       "      <th>cat_en:'s/'s&lt;gen&gt;</th>\n",
       "      <th>cat_en:&lt;*sf&gt;/actor&lt;n&gt;&lt;*numb&gt;</th>\n",
       "      <th>cat_en:&lt;*sf&gt;/authority&lt;n&gt;&lt;*numb&gt;</th>\n",
       "      <th>cat_en:&lt;*sf&gt;/bicycle&lt;n&gt;&lt;*numb&gt;</th>\n",
       "      <th>cat_en:&lt;*sf&gt;/car&lt;n&gt;&lt;*numb&gt;</th>\n",
       "      <th>cat_en:&lt;*sf&gt;/coat&lt;n&gt;&lt;*numb&gt;</th>\n",
       "      <th>cat_en:&lt;*sf&gt;/date&lt;n&gt;&lt;*numb&gt;</th>\n",
       "      <th>cat_en:&lt;*sf&gt;/difference&lt;n&gt;&lt;*numb&gt;</th>\n",
       "      <th>cat_en:&lt;*sf&gt;/house&lt;n&gt;&lt;*numb&gt;</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_en:yesterday/yesterday&lt;adv&gt;</th>\n",
       "      <th>cat_en:you/prpers&lt;@ij:thank_you&gt;</th>\n",
       "      <th>cat_en:you/prpers&lt;prn&gt;&lt;obj&gt;&lt;p2&gt;&lt;mf&gt;&lt;sp&gt;</th>\n",
       "      <th>cat_en:you/prpers&lt;prn&gt;&lt;subj&gt;&lt;p2&gt;&lt;mf&gt;&lt;sp&gt;</th>\n",
       "      <th>cat_en:your/your&lt;det&gt;&lt;pos&gt;&lt;sp&gt;</th>\n",
       "      <th>cat_en:yours/yours&lt;prn&gt;&lt;pos&gt;&lt;mf&gt;&lt;sp&gt;</th>\n",
       "      <th>right</th>\n",
       "      <th>wrong</th>\n",
       "      <th>bias</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.814433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.065637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.009306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.254132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.939039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.795832</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows Ã— 374 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cat_en:'/'<apos>  cat_en:'s/'s<gen>  cat_en:<*sf>/actor<n><*numb>  \\\n",
       "822               0.0                0.0                           0.0   \n",
       "188               0.0                0.0                           0.0   \n",
       "251               0.0                0.0                           0.0   \n",
       "71                0.0                0.0                           0.0   \n",
       "664               0.0                0.0                           0.0   \n",
       "..                ...                ...                           ...   \n",
       "579               0.0                0.0                           0.0   \n",
       "502               0.0                0.0                           0.0   \n",
       "537               0.0                0.0                           0.0   \n",
       "196               0.0                0.0                           0.0   \n",
       "175               0.0                0.0                           0.0   \n",
       "\n",
       "     cat_en:<*sf>/authority<n><*numb>  cat_en:<*sf>/bicycle<n><*numb>  \\\n",
       "822                               0.0                             0.0   \n",
       "188                               0.0                             0.0   \n",
       "251                               0.0                             0.0   \n",
       "71                                0.0                             0.0   \n",
       "664                               0.0                             0.0   \n",
       "..                                ...                             ...   \n",
       "579                               0.0                             0.0   \n",
       "502                               0.0                             0.0   \n",
       "537                               0.0                             0.0   \n",
       "196                               0.0                             0.0   \n",
       "175                               0.0                             0.0   \n",
       "\n",
       "     cat_en:<*sf>/car<n><*numb>  cat_en:<*sf>/coat<n><*numb>  \\\n",
       "822                         0.0                          0.0   \n",
       "188                         0.0                          0.0   \n",
       "251                         0.0                          0.0   \n",
       "71                          0.0                          0.0   \n",
       "664                         0.0                          0.0   \n",
       "..                          ...                          ...   \n",
       "579                         0.0                          0.0   \n",
       "502                         0.0                          0.0   \n",
       "537                         0.0                          0.0   \n",
       "196                         0.0                          0.0   \n",
       "175                         0.0                          0.0   \n",
       "\n",
       "     cat_en:<*sf>/date<n><*numb>  cat_en:<*sf>/difference<n><*numb>  \\\n",
       "822                          0.0                                0.0   \n",
       "188                          0.0                                0.0   \n",
       "251                          0.0                                0.0   \n",
       "71                           0.0                                0.0   \n",
       "664                          0.0                                0.0   \n",
       "..                           ...                                ...   \n",
       "579                          0.0                                0.0   \n",
       "502                          0.0                                0.0   \n",
       "537                          0.0                                0.0   \n",
       "196                          0.0                                0.0   \n",
       "175                          0.0                                0.0   \n",
       "\n",
       "     cat_en:<*sf>/house<n><*numb>  ...  cat_en:yesterday/yesterday<adv>  \\\n",
       "822                           0.0  ...                              0.0   \n",
       "188                           0.0  ...                              0.0   \n",
       "251                           0.0  ...                              0.0   \n",
       "71                            0.0  ...                              0.0   \n",
       "664                           0.0  ...                              0.0   \n",
       "..                            ...  ...                              ...   \n",
       "579                           0.0  ...                              0.0   \n",
       "502                           0.0  ...                              0.0   \n",
       "537                           0.0  ...                              0.0   \n",
       "196                           0.0  ...                              0.0   \n",
       "175                           0.0  ...                              0.0   \n",
       "\n",
       "     cat_en:you/prpers<@ij:thank_you>  \\\n",
       "822                               0.0   \n",
       "188                               0.0   \n",
       "251                               0.0   \n",
       "71                                0.0   \n",
       "664                               0.0   \n",
       "..                                ...   \n",
       "579                               0.0   \n",
       "502                               0.0   \n",
       "537                               0.0   \n",
       "196                               0.0   \n",
       "175                               0.0   \n",
       "\n",
       "     cat_en:you/prpers<prn><obj><p2><mf><sp>  \\\n",
       "822                                      0.0   \n",
       "188                                      0.0   \n",
       "251                                      0.0   \n",
       "71                                       0.0   \n",
       "664                                      0.0   \n",
       "..                                       ...   \n",
       "579                                      0.0   \n",
       "502                                      0.0   \n",
       "537                                      0.0   \n",
       "196                                      0.0   \n",
       "175                                      0.0   \n",
       "\n",
       "     cat_en:you/prpers<prn><subj><p2><mf><sp>  cat_en:your/your<det><pos><sp>  \\\n",
       "822                                       0.0                             0.0   \n",
       "188                                       0.0                             0.0   \n",
       "251                                       0.0                             0.0   \n",
       "71                                        0.0                             0.0   \n",
       "664                                       0.0                             0.0   \n",
       "..                                        ...                             ...   \n",
       "579                                       0.0                             0.0   \n",
       "502                                       0.0                             0.0   \n",
       "537                                       0.0                             0.0   \n",
       "196                                       0.0                             0.0   \n",
       "175                                       0.0                             0.0   \n",
       "\n",
       "     cat_en:yours/yours<prn><pos><mf><sp>     right     wrong  bias          t  \n",
       "822                                   0.0  4.242641  1.732051   1.0   1.814433  \n",
       "188                                   0.0  1.414214  1.414214   1.0   3.065637  \n",
       "251                                   0.0  1.732051  1.414214   1.0  12.009306  \n",
       "71                                    0.0  4.000000  1.414214   1.0   0.016609  \n",
       "664                                   0.0  3.000000  1.000000   1.0   0.003981  \n",
       "..                                    ...       ...       ...   ...        ...  \n",
       "579                                   0.0  4.242641  1.000000   1.0   0.006111  \n",
       "502                                   0.0  2.828427  1.414214   1.0  42.254132  \n",
       "537                                   0.0  3.464102  1.414214   1.0   0.003727  \n",
       "196                                   0.0  2.236068  1.000000   1.0   2.939039  \n",
       "175                                   0.0  4.795832  2.000000   1.0   0.004190  \n",
       "\n",
       "[700 rows x 374 columns]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "55dbb999-14d4-4ab0-a88e-a73653851622",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 3199454.5893432465\n",
      "Iteration 1000, Loss: 3199423.9022667613\n",
      "Iteration 2000, Loss: 3199414.7615177846\n",
      "Iteration 3000, Loss: 3199408.8884899365\n",
      "Iteration 4000, Loss: 3199404.5707250065\n",
      "Iteration 5000, Loss: 3199401.178550703\n",
      "Iteration 6000, Loss: 3199398.403597668\n",
      "Iteration 7000, Loss: 3199396.0702441493\n",
      "Iteration 8000, Loss: 3199394.0682614013\n",
      "Iteration 9000, Loss: 3199392.3239797414\n",
      "Iteration 10000, Loss: 3199390.785648278\n",
      "Iteration 11000, Loss: 3199389.4154871223\n",
      "Iteration 12000, Loss: 3199388.185088869\n",
      "Iteration 13000, Loss: 3199387.0725116553\n",
      "Iteration 14000, Loss: 3199386.060360404\n",
      "Iteration 15000, Loss: 3199385.1347517953\n",
      "Iteration 16000, Loss: 3199384.2844770085\n",
      "Iteration 17000, Loss: 3199383.5002990807\n",
      "Iteration 18000, Loss: 3199382.7745021475\n",
      "Iteration 19000, Loss: 3199382.1005863477\n",
      "Iteration 20000, Loss: 3199381.4730182905\n",
      "Iteration 21000, Loss: 3199380.887086462\n",
      "Iteration 22000, Loss: 3199380.338727083\n",
      "Iteration 23000, Loss: 3199379.824445852\n",
      "Iteration 24000, Loss: 3199379.3411431136\n",
      "Iteration 25000, Loss: 3199378.8861060715\n",
      "Iteration 26000, Loss: 3199378.4569079094\n",
      "Iteration 27000, Loss: 3199378.051461289\n",
      "Iteration 28000, Loss: 3199377.6678990987\n",
      "Iteration 29000, Loss: 3199377.304537232\n",
      "Iteration 30000, Loss: 3199376.959829496\n",
      "Iteration 31000, Loss: 3199376.6325301924\n",
      "Iteration 32000, Loss: 3199376.3213540665\n",
      "Iteration 33000, Loss: 3199376.0251889983\n",
      "Iteration 34000, Loss: 3199375.7430330347\n",
      "Iteration 35000, Loss: 3199375.473971823\n",
      "Iteration 36000, Loss: 3199375.217173444\n",
      "Iteration 37000, Loss: 3199374.9718772382\n",
      "Iteration 38000, Loss: 3199374.7373854043\n",
      "Iteration 39000, Loss: 3199374.513057329\n",
      "Iteration 40000, Loss: 3199374.2983031347\n",
      "Iteration 41000, Loss: 3199374.092578753\n",
      "Iteration 42000, Loss: 3199373.895348598\n",
      "Iteration 43000, Loss: 3199373.7061819937\n",
      "Iteration 44000, Loss: 3199373.5246475134\n",
      "Iteration 45000, Loss: 3199373.3503445927\n",
      "Iteration 46000, Loss: 3199373.1829004544\n",
      "Iteration 47000, Loss: 3199373.0219675503\n",
      "Iteration 48000, Loss: 3199372.8672210756\n",
      "Iteration 49000, Loss: 3199372.7183589707\n",
      "Iteration 50000, Loss: 3199372.575098449\n",
      "Iteration 51000, Loss: 3199372.437174653\n",
      "Iteration 52000, Loss: 3199372.3043393507\n",
      "Iteration 53000, Loss: 3199372.1763601922\n",
      "Iteration 54000, Loss: 3199372.053018756\n",
      "Iteration 55000, Loss: 3199371.934109738\n",
      "Iteration 56000, Loss: 3199371.819439971\n",
      "Iteration 57000, Loss: 3199371.708827365\n",
      "Iteration 58000, Loss: 3199371.602063096\n",
      "Iteration 59000, Loss: 3199371.4990188666\n",
      "Iteration 60000, Loss: 3199371.399543902\n",
      "Iteration 61000, Loss: 3199371.303492643\n",
      "Iteration 62000, Loss: 3199371.210728018\n",
      "Iteration 63000, Loss: 3199371.121120225\n",
      "Iteration 64000, Loss: 3199371.0345459725\n",
      "Iteration 65000, Loss: 3199370.9508884475\n",
      "Iteration 66000, Loss: 3199370.870036857\n",
      "Iteration 67000, Loss: 3199370.7918883427\n",
      "Iteration 68000, Loss: 3199370.716343574\n",
      "Iteration 69000, Loss: 3199370.643305949\n",
      "Iteration 70000, Loss: 3199370.572684401\n",
      "Iteration 71000, Loss: 3199370.5043700654\n",
      "Iteration 72000, Loss: 3199370.4382944284\n",
      "Iteration 73000, Loss: 3199370.3743859828\n",
      "Iteration 74000, Loss: 3199370.3125698157\n",
      "Iteration 75000, Loss: 3199370.2527744877\n",
      "Iteration 76000, Loss: 3199370.194931689\n",
      "Iteration 77000, Loss: 3199370.138976324\n",
      "Iteration 78000, Loss: 3199370.084845829\n",
      "Iteration 79000, Loss: 3199370.0324804904\n",
      "Iteration 80000, Loss: 3199369.9818231496\n",
      "Iteration 81000, Loss: 3199369.9328189343\n",
      "Iteration 82000, Loss: 3199369.8854181617\n",
      "Iteration 83000, Loss: 3199369.8395728855\n",
      "Iteration 84000, Loss: 3199369.795229803\n",
      "Iteration 85000, Loss: 3199369.752342644\n",
      "Iteration 86000, Loss: 3199369.7109117764\n",
      "Iteration 87000, Loss: 3199369.670848419\n",
      "Iteration 88000, Loss: 3199369.6321114646\n",
      "Iteration 89000, Loss: 3199369.5946615054\n",
      "Iteration 90000, Loss: 3199369.5584606966\n",
      "Iteration 91000, Loss: 3199369.523458799\n",
      "Iteration 92000, Loss: 3199369.489630248\n",
      "Iteration 93000, Loss: 3199369.4569457467\n",
      "Iteration 94000, Loss: 3199369.425372847\n",
      "Iteration 95000, Loss: 3199369.3948802855\n",
      "Iteration 96000, Loss: 3199369.365437819\n",
      "Iteration 97000, Loss: 3199369.33701635\n",
      "Iteration 98000, Loss: 3199369.3095929357\n",
      "Iteration 99000, Loss: 3199369.2831359403\n",
      "Iteration 100000, Loss: 3199369.2576178145\n",
      "Iteration 101000, Loss: 3199369.2330133626\n",
      "Iteration 102000, Loss: 3199369.209298459\n",
      "Iteration 103000, Loss: 3199369.1864495417\n",
      "Iteration 104000, Loss: 3199369.1644436414\n",
      "Iteration 105000, Loss: 3199369.14325839\n",
      "Iteration 106000, Loss: 3199369.1228724336\n",
      "Iteration 107000, Loss: 3199369.1032652627\n",
      "Iteration 108000, Loss: 3199369.0844168924\n",
      "Iteration 109000, Loss: 3199369.066307969\n",
      "Iteration 110000, Loss: 3199369.048919923\n",
      "Iteration 111000, Loss: 3199369.0322347707\n",
      "Iteration 112000, Loss: 3199369.0162350973\n",
      "Iteration 113000, Loss: 3199369.0009249914\n",
      "Iteration 114000, Loss: 3199368.986296394\n",
      "Iteration 115000, Loss: 3199368.9723031446\n",
      "Iteration 116000, Loss: 3199368.9589320105\n",
      "Iteration 117000, Loss: 3199368.9462036532\n",
      "Iteration 118000, Loss: 3199368.9341017934\n",
      "Iteration 119000, Loss: 3199368.9226178136\n",
      "Iteration 120000, Loss: 3199368.9116944787\n",
      "Iteration 121000, Loss: 3199368.9013186395\n",
      "Iteration 122000, Loss: 3199368.891476293\n",
      "Iteration 123000, Loss: 3199368.882155232\n",
      "Iteration 124000, Loss: 3199368.8733434463\n",
      "Iteration 125000, Loss: 3199368.865029332\n",
      "Iteration 126000, Loss: 3199368.857201622\n",
      "Iteration 127000, Loss: 3199368.849849357\n",
      "Iteration 128000, Loss: 3199368.842961622\n",
      "Iteration 129000, Loss: 3199368.836527953\n",
      "Iteration 130000, Loss: 3199368.8305382854\n",
      "Iteration 131000, Loss: 3199368.82498283\n",
      "Iteration 132000, Loss: 3199368.819852063\n",
      "Iteration 133000, Loss: 3199368.8152127364\n",
      "Iteration 134000, Loss: 3199368.8112398568\n",
      "Iteration 135000, Loss: 3199368.807656061\n",
      "Iteration 136000, Loss: 3199368.804453053\n",
      "Iteration 137000, Loss: 3199368.8016228913\n",
      "Iteration 138000, Loss: 3199368.7991578006\n",
      "Iteration 139000, Loss: 3199368.797049941\n",
      "Iteration 140000, Loss: 3199368.795291797\n",
      "Iteration 141000, Loss: 3199368.793876025\n",
      "Iteration 142000, Loss: 3199368.792795504\n",
      "Iteration 143000, Loss: 3199368.792043301\n",
      "Iteration 144000, Loss: 3199368.791699849\n",
      "Convergence reached at iteration 144197, Loss 3199368.791674111\n",
      "CPU times: total: 5min 18s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = hlr_adagrad(feature_columns=dummies_, eta = 1)\n",
    "cost_history = model.train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "6f693eff-0b4f-4eec-b92b-82113721812f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions over the same dataset\n",
    "\n",
    "predictions = []\n",
    "for _, row in X_test.iterrows():\n",
    "    predicted_p, predicted_h = model.predict(row)\n",
    "    predictions.append((predicted_p, predicted_h))\n",
    "\n",
    "X_test['predicted_p'] = [pred[0] for pred in predictions]\n",
    "X_test['predicted_h'] = [pred[1] for pred in predictions]\n",
    "\n",
    "Y_pred = X_test[ 'predicted_p' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ed9094cc-ed06-462f-a6ec-56db875a44f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10601866568704317"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mae( Y_test, Y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b83d1-e947-44c6-9b5b-7a499135d4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575804a-6b9c-469c-9d85-891b2cb5b092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11bccd-408e-462c-aa58-986a89d56487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ee3a3-16b6-4947-a4eb-038b482878c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16c3e8-bc9f-4dac-a811-645f3729e30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f02fc215-50e8-4392-a2c0-bb14e98ffc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 3199453.8892288534\n",
      "Iteration 1000, Loss: 3199453.800251489\n",
      "Iteration 2000, Loss: 3199453.7947600908\n",
      "Iteration 3000, Loss: 3199453.7950373953\n",
      "Iteration 4000, Loss: 3199453.7954465696\n",
      "Iteration 5000, Loss: 3199453.7959860917\n",
      "Iteration 6000, Loss: 3199453.7966551343\n",
      "Iteration 7000, Loss: 3199453.797452915\n",
      "Iteration 8000, Loss: 3199453.7983787046\n",
      "Iteration 9000, Loss: 3199453.799431828\n",
      "Iteration 10000, Loss: 3199453.800611668\n",
      "Iteration 11000, Loss: 3199453.8019176624\n",
      "Iteration 12000, Loss: 3199453.8033493026\n",
      "Iteration 13000, Loss: 3199453.8049061312\n",
      "Iteration 14000, Loss: 3199453.806587741\n",
      "Iteration 15000, Loss: 3199453.80839377\n",
      "Iteration 16000, Loss: 3199453.810323906\n",
      "Iteration 17000, Loss: 3199453.8123778813\n",
      "Iteration 18000, Loss: 3199453.8145554727\n",
      "Iteration 19000, Loss: 3199453.8168564974\n",
      "Iteration 20000, Loss: 3199453.819280811\n",
      "Iteration 21000, Loss: 3199453.821828303\n",
      "Iteration 22000, Loss: 3199453.824498887\n",
      "Iteration 23000, Loss: 3199453.8272925015\n",
      "Iteration 24000, Loss: 3199453.830209099\n",
      "Iteration 25000, Loss: 3199453.8332486465\n",
      "Iteration 26000, Loss: 3199453.8364111166\n",
      "Iteration 27000, Loss: 3199453.83969649\n",
      "Iteration 28000, Loss: 3199453.84310475\n",
      "Iteration 29000, Loss: 3199453.8466358804\n",
      "Iteration 30000, Loss: 3199453.8502898673\n",
      "Iteration 31000, Loss: 3199453.854066697\n",
      "Iteration 32000, Loss: 3199453.8579663537\n",
      "Iteration 33000, Loss: 3199453.8619888225\n",
      "Iteration 34000, Loss: 3199453.866134086\n",
      "Iteration 35000, Loss: 3199453.8704021266\n",
      "Iteration 36000, Loss: 3199453.874792923\n",
      "Iteration 37000, Loss: 3199453.879306454\n",
      "Iteration 38000, Loss: 3199453.883942696\n",
      "Iteration 39000, Loss: 3199453.8887016205\n",
      "Iteration 40000, Loss: 3199453.8935832\n",
      "Iteration 41000, Loss: 3199453.8985874015\n",
      "Iteration 42000, Loss: 3199453.903714189\n",
      "Iteration 43000, Loss: 3199453.908963523\n",
      "Iteration 44000, Loss: 3199453.91433536\n",
      "Iteration 45000, Loss: 3199453.9198296512\n",
      "Iteration 46000, Loss: 3199453.9254463455\n",
      "Iteration 47000, Loss: 3199453.931185385\n",
      "Iteration 48000, Loss: 3199453.937046709\n",
      "Iteration 49000, Loss: 3199453.943030251\n",
      "Iteration 50000, Loss: 3199453.949135945\n",
      "Iteration 51000, Loss: 3199453.955363723\n",
      "Iteration 52000, Loss: 3199453.9617135176\n",
      "Iteration 53000, Loss: 3199453.9681852697\n",
      "Iteration 54000, Loss: 3199453.974778928\n",
      "Iteration 55000, Loss: 3199453.981494455\n",
      "Iteration 56000, Loss: 3199453.988331831\n",
      "Iteration 57000, Loss: 3199453.9952910533\n",
      "Iteration 58000, Loss: 3199454.002372137\n",
      "Iteration 59000, Loss: 3199454.0095751127\n",
      "Iteration 60000, Loss: 3199454.016900021\n",
      "Iteration 61000, Loss: 3199454.0243469067\n",
      "Iteration 62000, Loss: 3199454.031915813\n",
      "Iteration 63000, Loss: 3199454.0396067784\n",
      "Iteration 64000, Loss: 3199454.047419833\n",
      "Iteration 65000, Loss: 3199454.055354997\n",
      "Iteration 66000, Loss: 3199454.0634122803\n",
      "Iteration 67000, Loss: 3199454.0715916813\n",
      "Iteration 68000, Loss: 3199454.07989319\n",
      "Iteration 69000, Loss: 3199454.0883167847\n",
      "Iteration 70000, Loss: 3199454.0968624367\n",
      "Iteration 71000, Loss: 3199454.105530107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "Cell \u001b[1;32mIn[295], line 60\u001b[0m, in \u001b[0;36mHalfLifeRegressionModelRMSprop.train\u001b[1;34m(self, dataframe_x, dataframe_y, max_iter, tolerance, print_iter)\u001b[0m\n\u001b[0;32m     57\u001b[0m adjusted_eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39msqrt(grad_accumulation) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m adjusted_eta \u001b[38;5;241m*\u001b[39m grad_theta \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n\u001b[1;32m---> 60\u001b[0m cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cost_function(X, p, Delta)\n\u001b[0;32m     61\u001b[0m cost_history\u001b[38;5;241m.\u001b[39mappend(cost)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m print_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[295], line 35\u001b[0m, in \u001b[0;36mHalfLifeRegressionModelRMSprop._cost_function\u001b[1;34m(self, X, p, Delta)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cost_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, p, Delta):\n\u001b[0;32m     34\u001b[0m     h_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_h_hat(X)\n\u001b[1;32m---> 35\u001b[0m     p_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_p_hat(h_hat, Delta)\n\u001b[0;32m     36\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mDelta \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(p \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[0;32m     37\u001b[0m     total_cost \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((p \u001b[38;5;241m-\u001b[39m p_hat) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (h \u001b[38;5;241m-\u001b[39m h_hat) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[295], line 29\u001b[0m, in \u001b[0;36mHalfLifeRegressionModelRMSprop._find_p_hat\u001b[1;34m(self, h_hat, Delta)\u001b[0m\n\u001b[0;32m     26\u001b[0m     h_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m theta_x\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h_hat\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_find_p_hat\u001b[39m(\u001b[38;5;28mself\u001b[39m, h_hat, Delta):\n\u001b[0;32m     30\u001b[0m     p_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mclip( \u001b[38;5;241m-\u001b[39mDelta \u001b[38;5;241m/\u001b[39m h_hat, \u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m0.9999\u001b[39m )\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p_hat\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = HalfLifeRegressionModelRMSprop(feature_columns=dummies_)\n",
    "cost_history = model.train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01557d-faa3-4bb8-974f-d02466820cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
