{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba379aa-f78c-4585-8c12-e5da8eb85dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71e91798-5f3c-4148-8609-051c498b5e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HalfLifeRegressionModel:\n",
    "    def __init__(self, feature_columns, alpha=0.01, lambda_=0.1, eta=0.01, epsilon=1e-8):\n",
    "        self.theta = np.random.randn(len(feature_columns))\n",
    "        self.feature_columns = feature_columns\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _find_h_hat(self, x):\n",
    "        theta_x = np.clip(np.dot(self.theta, x ), -10, 10 )\n",
    "        h_hat = 2 ** theta_x\n",
    "        return h_hat\n",
    "\n",
    "    def _find_p_hat( self, h_hat, Delta ):\n",
    "        p_hat = 2 ** np.clip( -Delta / h_hat, -10, 10 )  # Prevent underflow/overflow\n",
    "        return p_hat\n",
    "    \n",
    "    def _p_clip(self, p_hat):\n",
    "        # bound min/max model predictions (helps with loss optimization)\n",
    "        return min(max(p_hat, 0.0001), .9999)\n",
    "\n",
    "    def _cost_function(self, X, p, Delta):\n",
    "        D = len(X)\n",
    "        total_cost = 0\n",
    "\n",
    "        for t in range(D):\n",
    "            h_hat = self._find_h_hat(X[t])\n",
    "            p_hat = self._find_p_hat(h_hat, Delta[t])\n",
    "            h = -Delta[t] / np.log2(p[t] + self.epsilon)\n",
    "            total_cost += (p[t] - p_hat) ** 2 + self.alpha * (h - h_hat) ** 2\n",
    "\n",
    "        total_cost += self.lambda_ * np.sum(self.theta ** 2)\n",
    "        return total_cost / D  # Average cost per instance\n",
    "\n",
    "    def train(self, dataframe_x, dataframe_y, n_iter=10000, tolerance=1e-5, print_iter = 1000 ):\n",
    "        X = dataframe_x[self.feature_columns].values\n",
    "        p = dataframe_y.values\n",
    "        Delta = dataframe_x['t'].values\n",
    "        grad_accumulation = np.zeros_like(self.theta)\n",
    "        cost_history = []\n",
    "\n",
    "        for iteration in range(n_iter):\n",
    "            grad_theta = np.zeros_like(self.theta)\n",
    "            cost = self._cost_function(X, p, Delta)\n",
    "\n",
    "            for t in range(len(X)):\n",
    "                h_hat = self._find_h_hat(X[t])\n",
    "                p_hat = self._find_p_hat(h_hat, Delta[t])\n",
    "\n",
    "                # Compute the gradients for each theta\n",
    "                for k in range(len(self.theta)):\n",
    "                    term1 = 2 * (p[t] - p_hat) * np.log(2) * p_hat * (2 ** (-Delta[t] / h_hat)) * X[t][k]\n",
    "                    term2 = 2 * self.alpha * (h_hat + Delta[t] / np.log2(p[t])) * np.log(2) * h_hat * X[t][k]\n",
    "                    term3 = 2 * self.lambda_ * self.theta[k]\n",
    "                    grad_theta[k] += term1 + term2 - term3\n",
    "\n",
    "            # Update the accumulated gradient\n",
    "            grad_accumulation += grad_theta ** 2\n",
    "\n",
    "            # Update theta using AdaGrad adjustment\n",
    "            adjusted_eta = self.eta / (np.sqrt(grad_accumulation) + self.epsilon)\n",
    "            self.theta -= adjusted_eta * grad_theta / len(X)\n",
    "\n",
    "            cost_history.append(cost)\n",
    "            \n",
    "            if iteration % print_iter == 0:\n",
    "                print(f\"Iteration {iteration}, Loss: {cost}\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if iteration > 0 and np.abs(cost_history[-1] - cost_history[-2]) < tolerance:\n",
    "                break\n",
    "\n",
    "        return cost_history\n",
    "\n",
    "    def predict(self, row):\n",
    "        x = np.array([row[feature] for feature in self.feature_columns])\n",
    "        h_hat = self._find_h_hat(x)\n",
    "        p_hat = self._p_clip( self._find_p_hat(h_hat, row['t']) )\n",
    "        return p_hat, h_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a76ebc23-63dd-4b67-9d1d-46e542a2bc98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40de458d-0d4f-400c-9342-1b4a1709ddcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class HalfLifeRegressionModelOptimized:\n",
    "    def __init__(self, feature_columns, alpha=0.01, lambda_=0.1, eta=0.001, epsilon=1e-7):\n",
    "        self.theta = np.random.randn(len(feature_columns))\n",
    "        self.feature_columns = feature_columns\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _find_h_hat(self, X):        \n",
    "        theta_x = np.clip(X.dot(self.theta), -10, 10)\n",
    "        h_hat = 2 ** theta_x\n",
    "        return h_hat\n",
    "\n",
    "    def _find_p_hat(self, h_hat, Delta):\n",
    "        p_hat = 2 ** np.clip(-Delta / h_hat, -10, 10)  # Prevent underflow/overflow\n",
    "        return p_hat\n",
    "    \n",
    "    def _p_clip(self, p_hat):\n",
    "        # bound min/max model predictions (helps with loss optimization)\n",
    "        return min(max(p_hat, 0.0001), .9999)\n",
    "\n",
    "    def _cost_function(self, X, p, Delta):\n",
    "        h_hat = self._find_h_hat(X)\n",
    "        p_hat = self._find_p_hat(h_hat, Delta)\n",
    "        h = -Delta / np.log2(p + self.epsilon)\n",
    "        total_cost = np.mean((p - p_hat) ** 2 + self.alpha * (h - h_hat) ** 2)\n",
    "        total_cost += self.lambda_ * np.sum(self.theta ** 2)\n",
    "        return total_cost\n",
    "\n",
    "    def train(self, dataframe_x, dataframe_y, n_iter=100000, tolerance=1e-5, print_iter=1000):\n",
    "        X = dataframe_x[self.feature_columns].values\n",
    "        p = dataframe_y.values.flatten()\n",
    "        Delta = dataframe_x['t'].values\n",
    "        grad_accumulation = np.zeros_like(self.theta)\n",
    "        cost_history = []\n",
    "\n",
    "        for iteration in range(n_iter):\n",
    "            h_hat = self._find_h_hat(X)\n",
    "            p_hat = self._find_p_hat(h_hat, Delta)\n",
    "            grad_theta = -2 * X.T.dot((p - p_hat) * np.log(2) * p_hat * (2 ** (-Delta / h_hat)) + \n",
    "                                      self.alpha * (h_hat + Delta / np.log2(p)) * np.log(2) * h_hat) + 2 * self.lambda_ * self.theta\n",
    "            grad_accumulation += grad_theta ** 2\n",
    "\n",
    "            # Update theta using AdaGrad adjustment\n",
    "            adjusted_eta = self.eta / (np.sqrt(grad_accumulation) + self.epsilon)\n",
    "            self.theta -= adjusted_eta * grad_theta / len(X)\n",
    "\n",
    "            cost = self._cost_function(X, p, Delta)\n",
    "            cost_history.append(cost)\n",
    "\n",
    "            if iteration % print_iter == 0:\n",
    "                print(f\"Iteration {iteration}, Loss: {cost}\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if iteration > 0 and np.abs(cost_history[-1] - cost_history[-2]) < tolerance:\n",
    "                break\n",
    "\n",
    "        return cost_history\n",
    "\n",
    "    def predict(self, row):\n",
    "        x = np.array([row[feature] for feature in self.feature_columns])\n",
    "        h_hat = self._find_h_hat(x[np.newaxis, :])[0]\n",
    "        p_hat = self._p_clip( self._find_p_hat(h_hat, row['t']) )\n",
    "        \n",
    "        return p_hat, h_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80a98a8d-8d4d-4d8b-8a00-28b608e5b0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data      = pd.read_csv( 'subset_1000.csv' )\n",
    "pred_vars = [ 'right', 'wrong', 'bias', 't' ]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( data[ pred_vars ], \n",
    "                                                     data[ 'p' ], \n",
    "                                                     test_size    = 0.30,\n",
    "                                                     random_state = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2363aa70-05c4-48c2-9e6c-c9a1a685ca51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 244223960.65429652\n",
      "Iteration 1000, Loss: 244223960.99009675\n",
      "Iteration 2000, Loss: 244223961.1340904\n",
      "Iteration 3000, Loss: 244223961.24438635\n",
      "Iteration 4000, Loss: 244223961.33723107\n",
      "Iteration 5000, Loss: 244223961.41892144\n",
      "Iteration 6000, Loss: 244223961.49268788\n",
      "Iteration 7000, Loss: 244223961.56044954\n",
      "Iteration 8000, Loss: 244223961.62345713\n",
      "Iteration 9000, Loss: 244223961.68257928\n",
      "Iteration 10000, Loss: 244223961.73844877\n",
      "Iteration 11000, Loss: 244223961.79154298\n",
      "Iteration 12000, Loss: 244223961.84223297\n",
      "Iteration 13000, Loss: 244223961.89081392\n",
      "Iteration 14000, Loss: 244223961.93752486\n",
      "Iteration 15000, Loss: 244223961.98256314\n",
      "Iteration 16000, Loss: 244223962.02609357\n",
      "Iteration 17000, Loss: 244223962.0682553\n",
      "Iteration 18000, Loss: 244223962.10916772\n",
      "Iteration 19000, Loss: 244223962.1489336\n",
      "Iteration 20000, Loss: 244223962.18764228\n",
      "Iteration 21000, Loss: 244223962.2253722\n",
      "Iteration 22000, Loss: 244223962.2621925\n",
      "Iteration 23000, Loss: 244223962.29816464\n",
      "Iteration 24000, Loss: 244223962.33334333\n",
      "Iteration 25000, Loss: 244223962.36777765\n",
      "Iteration 26000, Loss: 244223962.40151194\n",
      "Iteration 27000, Loss: 244223962.43458617\n",
      "Iteration 28000, Loss: 244223962.46703675\n",
      "Iteration 29000, Loss: 244223962.49889672\n",
      "Iteration 30000, Loss: 244223962.53019658\n",
      "Iteration 31000, Loss: 244223962.56096396\n",
      "Iteration 32000, Loss: 244223962.5912245\n",
      "Iteration 33000, Loss: 244223962.62100172\n",
      "Iteration 34000, Loss: 244223962.6503176\n",
      "Iteration 35000, Loss: 244223962.67919225\n",
      "Iteration 36000, Loss: 244223962.7076444\n",
      "Iteration 37000, Loss: 244223962.73569164\n",
      "Iteration 38000, Loss: 244223962.7633503\n",
      "Iteration 39000, Loss: 244223962.7906356\n",
      "Iteration 40000, Loss: 244223962.81756175\n",
      "Iteration 41000, Loss: 244223962.84414226\n",
      "Iteration 42000, Loss: 244223962.87038958\n",
      "Iteration 43000, Loss: 244223962.89631575\n",
      "Iteration 44000, Loss: 244223962.92193168\n",
      "Iteration 45000, Loss: 244223962.9472481\n",
      "Iteration 46000, Loss: 244223962.9722748\n",
      "Iteration 47000, Loss: 244223962.9970213\n",
      "Iteration 48000, Loss: 244223963.02149642\n",
      "Iteration 49000, Loss: 244223963.0457086\n",
      "Iteration 50000, Loss: 244223963.06966588\n",
      "Iteration 51000, Loss: 244223963.0933759\n",
      "Iteration 52000, Loss: 244223963.11684582\n",
      "Iteration 53000, Loss: 244223963.14008266\n",
      "Iteration 54000, Loss: 244223963.16309288\n",
      "Iteration 55000, Loss: 244223963.1858828\n",
      "Iteration 56000, Loss: 244223963.20845848\n",
      "Iteration 57000, Loss: 244223963.23082545\n",
      "Iteration 58000, Loss: 244223963.25298938\n",
      "Iteration 59000, Loss: 244223963.27495545\n",
      "Iteration 60000, Loss: 244223963.2967286\n",
      "Iteration 61000, Loss: 244223963.31831366\n",
      "Iteration 62000, Loss: 244223963.33971527\n",
      "Iteration 63000, Loss: 244223963.36093792\n",
      "Iteration 64000, Loss: 244223963.38198572\n",
      "Iteration 65000, Loss: 244223963.4028629\n",
      "Iteration 66000, Loss: 244223963.42357323\n",
      "Iteration 67000, Loss: 244223963.44412065\n",
      "Iteration 68000, Loss: 244223963.46450856\n",
      "Iteration 69000, Loss: 244223963.48474082\n",
      "Iteration 70000, Loss: 244223963.50482053\n",
      "Iteration 71000, Loss: 244223963.52475104\n",
      "Iteration 72000, Loss: 244223963.54453552\n",
      "Iteration 73000, Loss: 244223963.5641769\n",
      "Iteration 74000, Loss: 244223963.58367822\n",
      "Iteration 75000, Loss: 244223963.60304233\n",
      "Iteration 76000, Loss: 244223963.62227193\n",
      "Iteration 77000, Loss: 244223963.64136958\n",
      "Iteration 78000, Loss: 244223963.66033798\n",
      "Iteration 79000, Loss: 244223963.67917952\n",
      "Iteration 80000, Loss: 244223963.69789663\n",
      "Iteration 81000, Loss: 244223963.71649167\n",
      "Iteration 82000, Loss: 244223963.73496693\n",
      "Iteration 83000, Loss: 244223963.75332448\n",
      "Iteration 84000, Loss: 244223963.77156654\n",
      "Iteration 85000, Loss: 244223963.7896951\n",
      "Iteration 86000, Loss: 244223963.8077122\n",
      "Iteration 87000, Loss: 244223963.82561982\n",
      "Iteration 88000, Loss: 244223963.84341973\n",
      "Iteration 89000, Loss: 244223963.86111385\n",
      "Iteration 90000, Loss: 244223963.87870398\n",
      "Iteration 91000, Loss: 244223963.89619175\n",
      "Iteration 92000, Loss: 244223963.913579\n",
      "Iteration 93000, Loss: 244223963.93086717\n",
      "Iteration 94000, Loss: 244223963.94805804\n",
      "Iteration 95000, Loss: 244223963.96515307\n",
      "Iteration 96000, Loss: 244223963.98215377\n",
      "Iteration 97000, Loss: 244223963.99906158\n",
      "Iteration 98000, Loss: 244223964.01587805\n",
      "Iteration 99000, Loss: 244223964.03260446\n",
      "CPU times: total: 25.2 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = HalfLifeRegressionModelOptimized(feature_columns=pred_vars)\n",
    "cost_history = model.train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f052186-9a69-4590-8ee3-2d404e603d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hacer predicciones en el mismo conjunto de datos\n",
    "predictions = []\n",
    "for _, row in X_test.iterrows():\n",
    "    predicted_p, predicted_h = model.predict(row)\n",
    "    predictions.append((predicted_p, predicted_h))\n",
    "\n",
    "# Agregar las predicciones al dataframe para comparar\n",
    "X_test['predicted_p'] = [pred[0] for pred in predictions]\n",
    "X_test['predicted_h'] = [pred[1] for pred in predictions]\n",
    "\n",
    "# X_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e8e79dd-af1f-4b43-b259-4346a443786a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847    0.9999\n",
       "874    0.9999\n",
       "471    0.0001\n",
       "476    0.0001\n",
       "764    0.9999\n",
       "        ...  \n",
       "353    0.9999\n",
       "236    0.9999\n",
       "581    0.9999\n",
       "324    0.9999\n",
       "988    0.9999\n",
       "Name: p, Length: 300, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cd042ab-9b3f-4a68-b4a9-faa2f35962d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_pred = X_test[ 'predicted_p' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c1dde2f-594b-4aa6-9ba6-dd5bb3645775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd3bcf39-048b-4641-a045-6af065fe349c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19123345334377023"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mae( Y_test, Y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f03b2b06-886a-4174-bb95-3869be55959d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_roc_auc( Y_test, Y_pred )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
