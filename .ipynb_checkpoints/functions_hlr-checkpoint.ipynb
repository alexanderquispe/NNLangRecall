{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ff5edb-baef-462e-a4ab-6b85ae765752",
   "metadata": {},
   "source": [
    "## 1. Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ab5075-8b38-4655-a8b7-0ec50ec0f71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2c078-722f-4476-8e37-e58c5f3d2ec7",
   "metadata": {},
   "source": [
    "## 2. Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ce75f-f799-40d1-ab58-6b8622a9d84a",
   "metadata": {},
   "source": [
    "https://github.com/duolingo/halflife-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87feae8a-2d2c-4d55-80b6-405f4ad87c39",
   "metadata": {},
   "source": [
    "Funciones\n",
    "\n",
    "1. **estimate_h_hat**: Permite estimar h_hat dados los valores de theta y x. Ecuación N. 2. \n",
    "2. **estimate_p_hat**: Permite estimar p_hat dados los valores de delta y h_hat. Ecuación N. 4.\n",
    "3. **hh_loss_function**: Permite estimar el costo de la función tomando en cuenta el valor de p, p_hat, h_hat, delta y lambda. Ecuación N. 9. \n",
    "4. **gradient_partial**: Permite estimar el gradiente de la función de costo. Ecuación N. 10.\n",
    "5. **adagrad_update**: Permite actualizar el valor de theta con cada iteración utilizando el algoritmo SGD adagrad. Ecuación N. 11.\n",
    "6. **half_life_regression**: Modelo HLR que integra todas las funciones anteriores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fb957ed1-d066-4a28-8fa4-c0c19ac0384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_h_hat( theta, x ):\n",
    "    \n",
    "    '''\n",
    "    Objetivo:\n",
    "        - Estimar hat{h} a través de la ecuación N. 2:\n",
    "          \\hat{h} = 2^{Theta \\cdot x}\n",
    "          \n",
    "    Input:\n",
    "        - theta : valor de los coeficientes de x\n",
    "        - x     : variables predictoras\n",
    "        \n",
    "    Output:\n",
    "        - estimated_half_life: hat{h}\n",
    "    '''\n",
    "    estimated_h = 2 ** np.dot( theta.T, x )\n",
    "    \n",
    "    return estimated_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b177dc80-36db-4e4d-86b4-cd0c92e830cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_p_hat( delta, estimated_h ):\n",
    "    \n",
    "    '''\n",
    "    Objetivo:\n",
    "        - Estimar  hat{p} a través de la ecuación N. 4:\n",
    "          \\hat{p}_{\\Theta} = 2^{-\\Delta/\\hat{h}_{\\Theta}}\n",
    "   \n",
    "   Input:\n",
    "       - delta       : tiempo transcurrido desde \n",
    "                       la última práctica\n",
    "       - estimated_h : valor estimado de la capacidad\n",
    "                       de memoria o half_life ( hat{h} ).\n",
    "   \n",
    "   Output:\n",
    "       - predicted_p : valor estimado de la probabilidad\n",
    "                       de recordar( hat{p} )\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    predicted_p = 2 ** ( - delta / estimated_h )\n",
    "    \n",
    "    return predicted_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a0f5f10c-c918-42d0-a72b-b9796339ab0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hh_loss_function( p, predicted_p, estimated_h, delta, theta, regularization, lambda_param = 0.1, alpha_param = 0.01 ):\n",
    "    \n",
    "    '''\n",
    "    Objetivo: \n",
    "        - Calcular el valor de pérdida del modelo Half Life Regression\n",
    "          Ecuación N. 9.\n",
    "    \n",
    "    Input:\n",
    "        - p            : probabilidad de recordar real\n",
    "        - predicted_p  : probabilidad de recordar predicha\n",
    "                         mediante la función predict_recall_probability\n",
    "        - estimated_h  : capacidad de memoria estimada\n",
    "        - delta        : tiempo transcurrido desde la última práctica\n",
    "        - theta        : valor de los coeficientes de x\n",
    "        - lambda_param : parámetro lambda de importancia relativa de la\n",
    "                         semivida en la función de pérdida\n",
    "        - alpha_param  : parámetro de regularización L2\n",
    "        \n",
    "    Output:\n",
    "        - Valor de pérdida de la función Half Life Regression\n",
    "    '''\n",
    "    \n",
    "    loss_p              = np.square( p - predicted_p )\n",
    "    # loss_p = np.square(p - predicted_p.reshape(p.shape))\n",
    "    loss_h              = np.square( ( -delta / np.log2( p ) ) - estimated_h )\n",
    "    \n",
    "    if regularization == 'l2': \n",
    "    \n",
    "        regularization_term = lambda_param * np.sum( np.square( theta ) )\n",
    "        \n",
    "    elif regularization == 'l1':\n",
    "        \n",
    "        regularization_term = lambda_param * np.sum( np.abs( theta ) )        \n",
    "    \n",
    "    loss = loss_p + alpha_param * loss_h + regularization_term  \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "123689ed-38e3-4547-a3c3-9d57ac90fa4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_partial( p, predicted_p, estimated_h, delta, x, theta, lambda_param = 0.1, alpha_param = 0.01 ):\n",
    "    \n",
    "    '''\n",
    "    Calcula la derivada parcial de la función de pérdida con respecto a cada peso theta_k.\n",
    "\n",
    "    Input:\n",
    "    - p            : probabilidad de recordar real\n",
    "    - predicted_p  : probabilidad de recordar predicha\n",
    "    - estimated_h  : capacidad de memoria estimada\n",
    "    - delta        : tiempo transcurrido desde la última práctica\n",
    "    - x            : vector de características\n",
    "    - theta        : vector de pesos\n",
    "    - lambda_param : parámetro lambda de importancia relativa de la semivida en la función de pérdida\n",
    "    - alpha_param  : parámetro de regularización L2\n",
    "\n",
    "    Output:\n",
    "    - gradient : vector de derivadas parciales con respecto a cada theta_k\n",
    "    '''\n",
    "    \n",
    "    term1 = 2 * ( predicted_p - p ) * np.log( 2 ) * predicted_p * ( 2**( -delta / estimated_h ) ) * x\n",
    "    term2 = 2 * alpha_param * ( estimated_h + delta / np.log2( p ) ) * np.log( 2 ) * estimated_h * x\n",
    "    term3 = 2 * lambda_param * theta\n",
    "\n",
    "    gradient = term1 + term2 + term3\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ca6489bb-ab4c-40e6-b829-d0c87e44e604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adagrad_update( theta, gradient, learning_rate, csg ):\n",
    "    \n",
    "    '''\n",
    "    Actualiza los pesos utilizando el algoritmo AdaGrad.\n",
    "\n",
    "    Input:\n",
    "    - theta         : vector de pesos\n",
    "    - gradient      : vector de derivadas parciales con respecto a cada theta_k\n",
    "    - learning_rate : tasa de aprendizaje\n",
    "    - csg           : acumulación de los cuadrados de los gradientes anteriores\n",
    "\n",
    "    Output:\n",
    "    - theta_updated : vector de pesos actualizado\n",
    "    '''\n",
    "    \n",
    "    csg_updated = csg + gradient**2\n",
    "    theta_updated = theta - ( learning_rate / np.sqrt( csg_updated + 1e-8 ) ) * gradient\n",
    "\n",
    "    return theta_updated, csg_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c2c3bf2c-cd5c-4e9c-bf11-61c301417dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def half_life_regression( X, y, delta, regularization, learning_rate=0.01, lambda_param=0.1, alpha_param=0.01, num_iterations = 1000 ):\n",
    "    '''\n",
    "    Implementa el modelo de Half-Life Regression.\n",
    "\n",
    "    Input:\n",
    "    - X               : Matriz de características (dimensiones: m x n)\n",
    "    - y               : Vector de etiquetas (dimensiones: m x 1)\n",
    "    - theta_init      : Vector de pesos iniciales (dimensiones: n x 1)\n",
    "    - learning_rate   : Tasa de aprendizaje para el algoritmo de optimización (por defecto: 0.01)\n",
    "    - lambda_param    : Parámetro lambda de importancia relativa de la semivida en la función de pérdida (por defecto: 0.1)\n",
    "    - alpha_param     : Parámetro de regularización L2 (por defecto: 0.01)\n",
    "    - num_iterations  : Número de iteraciones para el algoritmo de optimización (por defecto: 1000)\n",
    "\n",
    "    Output:\n",
    "    - theta_optimized : Vector de pesos optimizados\n",
    "    '''\n",
    "\n",
    "    # Inicialización de variables\n",
    "    \n",
    "    m     = X.shape[ 1 ]   # n_rows\n",
    "    n     = X.shape[ 0 ]   # n_columns\n",
    "    theta = np.zeros( ( n, 1 ) ) # weights: matriz vacía. Coeficientes. \n",
    "    \n",
    "    # theta = np.random.rand(X.shape[1])\n",
    "    csg = np.zeros_like(theta)\n",
    "\n",
    "    cost_list = []\n",
    "    # Iteraciones de optimización\n",
    "    for iteration in range(num_iterations):\n",
    "        # Predicción de la semivida y probabilidad de recordar\n",
    "        estimated_h = estimate_h_hat(theta, X)       \n",
    "        predicted_p = estimate_p_hat(delta, estimated_h)\n",
    "\n",
    "        # Cálculo de la pérdida y el gradiente\n",
    "        loss     = hh_loss_function(y, predicted_p, estimated_h, delta, theta, regularization, lambda_param, alpha_param)\n",
    "        gradient = gradient_partial(y, predicted_p, estimated_h, delta, X, theta, lambda_param, alpha_param)\n",
    "\n",
    "        # Actualización de pesos con AdaGrad\n",
    "        theta, csg = adagrad_update(theta, gradient, learning_rate, csg)\n",
    "\n",
    "        # Mostrar la pérdida en cada 100 iteraciones\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {loss}\")\n",
    "            \n",
    "        cost_list.append( loss )\n",
    "\n",
    "    return theta, cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5abfd70-cad9-43c1-8c4b-8a48fb609fb8",
   "metadata": {},
   "source": [
    "## Primer intento con datos de Duolingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42f8f5-b23b-45d9-a95f-6f618d5e121c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b5b4f-9a3e-44f7-8ad8-98775469f4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "08097665-498a-4ae0-bcf7-0f983e8f181c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data      = pd.read_csv( 'subset_1000.csv' )\n",
    "pred_vars = [ 'right', 'wrong', 'bias', 't' ]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( data[ pred_vars ], \n",
    "                                                     data[ 'p' ], \n",
    "                                                     test_size    = 0.30,\n",
    "                                                     random_state = 2023 )\n",
    "\n",
    "t_train = X_train[ 't' ].values\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values\n",
    "X_test = X_test.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "X_train = X_train.T\n",
    "Y_train = Y_train.reshape( 1, X_train.shape[ 1 ] )\n",
    "t_train = t_train.reshape( 1, X_train.shape[ 1 ] )\n",
    "\n",
    "X_test = X_test.T\n",
    "Y_test = Y_test.reshape( 1, X_test.shape[ 1 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4f826f15-d6b1-4193-8166-c42f8d4d45aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: [[2.69441250e+08 7.37541200e+00 1.70376395e+01 9.46436424e+07\n",
      "  2.66597746e+00 7.46281460e+00 1.07364127e+09 1.70844538e+06\n",
      "  2.53931836e+08 6.87525405e+01 4.03974965e+05 5.20247570e+09\n",
      "  1.48721321e+06 3.60024115e+05 1.69106448e+07 1.43552562e+02\n",
      "  7.44384413e+05 1.22296973e+02 1.06474837e+07 3.09851748e+04\n",
      "  4.29282259e+05 1.21306124e+07 2.85250853e+00 4.57752001e+07\n",
      "  4.38341600e+07 3.67765055e+02 8.13474452e+00 1.89424062e+01\n",
      "  6.57388697e+06 4.10413696e+00 1.08991573e+01 7.09760966e+05\n",
      "  2.65508407e+00 1.37983563e+05 3.27345502e+00 1.34438432e+00\n",
      "  1.00638487e+00 1.16662616e-01 8.29537605e-01 4.00346583e+08\n",
      "  2.11423371e+02 1.01702545e+05 2.59114016e+01 8.12607173e-02\n",
      "  2.44621820e+01 6.94612159e+00 3.63106314e+00 3.38637360e+06\n",
      "  5.82654744e+04 6.50238241e+05 4.93104388e-01 1.45411296e-01\n",
      "  4.42341576e+05 6.78389621e-02 3.16918566e+06 1.90915319e+06\n",
      "  1.01138800e-02 8.43020637e+01 2.26710879e+01 6.69472540e+00\n",
      "  5.40132636e+07 1.20449317e+09 1.56118272e+01 1.94343648e+01\n",
      "  5.77791378e+05 4.41648243e+05 6.89843458e-01 1.17614511e-02\n",
      "  2.80713783e+02 3.36356862e+08 3.25371523e+02 1.00368350e+00\n",
      "  4.87857206e+05 4.78498191e-01 1.79033621e-02 8.01960855e+07\n",
      "  1.25698848e+07 2.05206831e+07 1.92227531e+01 2.10584311e+05\n",
      "  3.20543102e+03 1.56689597e+02 1.62017213e+03 3.46172115e+05\n",
      "  3.63478119e+01 2.95231964e+07 1.42705364e+08 3.06351490e+03\n",
      "  3.15837101e+00 3.84852202e+01 2.57327762e-01 1.31686396e+06\n",
      "  7.59488381e+00 8.80613197e+06 2.31183561e+00 6.37569663e-01\n",
      "  1.96039222e+00 1.07680351e+02 3.15837101e+00 5.12985399e+06\n",
      "  2.51111335e+00 4.11391055e+01 1.81065400e+06 1.00813107e+00\n",
      "  9.25989189e+01 1.75480819e+06 5.94558485e+02 6.37569663e-01\n",
      "  2.64795235e+09 3.61566064e+05 7.81757335e+00 5.53842664e+06\n",
      "  2.48575315e+00 2.56221988e+00 1.43598772e+06 1.32737798e+00\n",
      "  7.49781414e-01 6.49843650e+07 1.58981132e+08 1.57155210e+06\n",
      "  2.28255990e-01 5.07178701e-01 8.13474452e+00 1.10975076e+10\n",
      "  1.18075712e+02 1.16778697e+07 2.75705552e+01 3.30555893e+08\n",
      "  6.35252429e+01 7.81757335e+00 8.04348064e+00 4.78763390e+02\n",
      "  4.87123092e+06 4.72676956e-02 1.36982206e+06 7.12190342e-02\n",
      "  4.97983777e+05 9.71492822e+00 1.88854535e+07 1.45673035e+06\n",
      "  4.76302035e+06 5.22852994e+08 2.87967066e+00 9.71492822e+00\n",
      "  3.42020599e+00 1.48603682e+01 1.18689586e+03 4.64554435e-01\n",
      "  1.18639233e+07 6.31537412e+05 5.84259925e+08 2.70868989e+06\n",
      "  4.22735715e+06 1.00774632e+00 1.42064820e+07 1.27171960e+01\n",
      "  4.99925113e+07 6.25391851e+09 1.05672178e+08 7.44176603e-01\n",
      "  4.95876739e+06 5.51264260e-02 5.30990011e+04 4.22016885e+06\n",
      "  3.41267204e+06 4.89169236e+05 3.13534025e-02 2.10221356e+06\n",
      "  9.92256314e-01 4.60952245e+08 1.16766944e+00 1.00269429e+00\n",
      "  4.14255472e+07 1.37311573e+08 3.40488136e+06 2.46767269e+07\n",
      "  1.71039293e+01 3.13120724e+01 3.23074314e+01 9.73187209e+07\n",
      "  5.16228584e-01 7.28666845e+07 1.86895350e+06 6.39319762e+06\n",
      "  3.89674237e+05 4.60254289e+05 1.79529358e+06 1.72606702e+07\n",
      "  7.04314922e+09 2.98311251e-03 8.18085986e-02 1.04208905e+08\n",
      "  1.75410689e+06 1.11136947e+09 3.03815002e+07 4.73923214e+07\n",
      "  2.63213634e+01 1.84605384e+09 1.25297841e+00 5.02864810e+00\n",
      "  6.62914906e+07 1.61743921e+00 7.03538183e-04 1.96471348e+01\n",
      "  7.58873970e-08 1.35592340e+02 1.51087952e+01 2.19147007e+00\n",
      "  1.73871369e+06 1.55467778e+07 3.18864603e+07 2.48575315e+00\n",
      "  2.74278385e+07 3.12463622e+07 4.85078593e+07 1.29825706e-01\n",
      "  1.02214443e+01 3.12992178e+00 4.33481537e+00 5.34380830e+05\n",
      "  1.63790845e+00 2.17366392e-02 5.04359159e-02 4.00484765e+05\n",
      "  2.97168963e+01 1.11854960e+05 2.55921857e-01 1.17081627e+01\n",
      "  4.12420775e+01 1.97932972e+06 7.44176603e-01 2.98044232e+01\n",
      "  4.83393834e+07 3.71324881e+08 9.52676540e+02 9.92851775e+07\n",
      "  1.70314382e+06 3.95288562e+06 4.74948016e-06 4.86462888e+08\n",
      "  1.65119546e+01 5.89062394e+04 2.67910445e+05 4.76770543e+05\n",
      "  1.00696126e+00 8.21924865e+06 3.48124263e+06 3.29934889e+10\n",
      "  5.04002577e+05 2.51815791e+01 8.84306869e-01 1.63214271e+02\n",
      "  4.58143958e+04 2.73398025e-01 5.23540519e+05 2.61340818e+05\n",
      "  5.91283727e+01 2.81634220e+01 1.65969358e+07 1.84967989e+00\n",
      "  2.12079527e+00 3.21565560e+00 4.38380674e-01 4.08044172e+06\n",
      "  7.07355727e+00 4.53141395e+02 9.81520176e+00 3.73843678e+05\n",
      "  2.93438105e+00 4.27580958e+07 3.76161725e+01 3.47980734e+00\n",
      "  1.18182182e+01 4.19788454e+06 4.73419714e+05 6.97456102e-02\n",
      "  5.85549299e+05 2.59002696e-01 2.37990652e-01 2.21909080e+05\n",
      "  3.42221786e-02 1.69823924e+08 2.41514535e+07 4.81509248e+00\n",
      "  9.88120529e+07 7.90840528e+02 8.78145935e-03 2.98044232e+01\n",
      "  9.59746792e-01 5.04230409e+05 1.60128609e-03 2.40169686e+07\n",
      "  5.85594543e+02 2.39063875e+07 8.08267899e+05 5.25524016e+07\n",
      "  2.51111335e+00 1.41077833e+06 6.01556286e+05 2.88927350e+05\n",
      "  6.00759889e+00 8.10149866e+07 4.32289604e+06 1.00803487e+00\n",
      "  4.69003286e+05 1.11496831e+07 6.24822988e-01 2.49193889e-01\n",
      "  5.94756806e-03 2.57745735e-01 1.67218832e+07 4.18538568e+07\n",
      "  2.28430244e+06 4.54341208e+05 5.82716684e+05 4.80900592e+05\n",
      "  1.75669058e+05 5.04458293e+05 2.35459583e+07 2.35648805e+07\n",
      "  2.41044480e+00 1.34552012e+09 3.25072469e+07 1.06439240e+07\n",
      "  7.79127386e-02 6.61195634e+00 5.18364150e+05 1.58132221e+06\n",
      "  4.07169774e+00 8.16965939e+07 2.43541888e+00 7.90780187e+05\n",
      "  6.74194804e+01 2.88848204e+03 1.74779196e+07 1.39357513e-01\n",
      "  5.96833806e+00 1.17692266e+07 2.34487419e+07 1.47985832e+01\n",
      "  2.19856198e+09 1.12427808e+08 5.03222609e+07 2.54236949e+01\n",
      "  5.92768283e+08 2.21161468e+05 5.00273471e+05 1.91836372e+02\n",
      "  7.14224946e+06 1.00282187e+00 5.17359269e+00 1.18714369e-01\n",
      "  5.14512882e+02 5.25569154e+04 4.60617041e+00 2.57196738e-01\n",
      "  3.04839857e+10 4.18002084e+06 4.64066898e+00 1.55548260e+06\n",
      "  1.04276546e+01 2.09749442e+00 3.33176928e+00 2.56403949e+04\n",
      "  8.73955684e-01 4.36911928e+09 3.76119448e+02 4.00720543e+00\n",
      "  3.70280724e+01 2.30641992e+07 8.21316524e+05 1.18582872e+07\n",
      "  2.03487202e+07 3.84075137e+01 9.42941774e+09 2.64547940e+03\n",
      "  2.32735627e+07 1.88517532e+02 6.86134648e-02 4.36828427e+00\n",
      "  4.58745492e+06 9.65746988e-01 1.74376930e+08 5.05872322e+05\n",
      "  1.55441655e+09 3.01558177e+01 8.97472344e+09 3.50980108e+00\n",
      "  2.09749442e+00 1.20398736e+01 4.30230502e+06 2.23968530e+02\n",
      "  3.53992353e+00 7.04364104e-02 5.14973578e+05 4.36680678e+06\n",
      "  3.63785178e+08 4.36828427e+00 1.00477143e-01 1.14753873e+00\n",
      "  6.89297844e+04 1.77336900e+06 3.27327339e+09 3.08873330e+09\n",
      "  4.34959062e+06 5.56046941e+01 2.05687273e-01 2.93438105e+00\n",
      "  1.69530954e+07 2.93515952e-01 2.63984500e+00 1.65687424e+06\n",
      "  4.18074253e+06 1.43708373e+09 4.40635888e+05 3.96783749e-01\n",
      "  1.30048838e+01 7.21091646e+01 4.41569191e+06 4.89692613e-02\n",
      "  1.63790845e+00 1.91137891e+07 1.05315320e+01 3.95108607e-01\n",
      "  3.45268306e+06 3.92564027e+05 8.04348064e+00 4.11125688e+02\n",
      "  1.04276546e+01 2.33835255e+07 7.38987774e-01 7.46010824e-02\n",
      "  9.98067109e-01 2.47008227e+01 2.59572172e+05 3.72336114e+00\n",
      "  4.45313046e+05 2.38472749e+07 1.11342185e+00 1.50979945e+06\n",
      "  9.73377827e-01 1.75715618e+01 3.60055456e+00 5.23052318e+01\n",
      "  1.95706850e+06 2.26741937e-01 8.26245323e+01 5.81258187e+00\n",
      "  2.09218528e+08 3.04636234e+07 1.68395423e+01 4.70828774e-01\n",
      "  4.22686235e+06 1.04332116e+06 1.17345727e+05 1.60505662e+06\n",
      "  5.33804998e+08 1.97542661e+06 2.48012970e+07 2.71862850e+00\n",
      "  5.54780226e+05 1.00755754e+09 1.86641186e+01 4.71005225e+00\n",
      "  1.93665078e+07 3.20344253e+01 5.52568713e-01 3.93357556e+07\n",
      "  9.28106342e+05 9.53988155e+01 1.82958669e+05 1.07981981e+00\n",
      "  1.89815110e+08 9.71492822e+00 5.17359269e+00 3.26732414e+01\n",
      "  2.51537120e-01 5.24683725e+00 2.27475458e+01 4.13670489e+00\n",
      "  8.34409695e+06 7.14360014e+07 1.19649371e+07 6.78152753e+01\n",
      "  3.46275302e+01 1.26953668e+02 2.53660226e+00 3.24449095e+00\n",
      "  1.21731701e+00 1.57688686e+00 2.49712188e+07 3.10196924e+07\n",
      "  2.71509402e+01 2.58427049e-01 8.41957531e+04 1.18734390e+01\n",
      "  1.34442627e+09 1.00020967e+00 4.63786825e+02 7.30400253e-01\n",
      "  3.90440780e+07 7.33272721e+02 4.63368397e+08 3.23616157e+09\n",
      "  2.74514709e+00 6.09934392e+01 3.85070106e+01 3.24902444e+09\n",
      "  7.28468330e+05 5.24115414e+08 5.17372536e-03 6.32631900e+00\n",
      "  1.63903159e+07 4.72878941e+05 1.05430199e+08 4.14385299e+02\n",
      "  1.09655647e+00 3.25815958e+01 5.54567999e+02 2.10824415e+07\n",
      "  6.34520617e+07 2.33629484e+00 1.27746389e+07 3.40245067e+04\n",
      "  5.15894839e-01 1.20454327e+07 3.57017469e+00 4.59623189e+05\n",
      "  3.96764772e-01 1.22809754e+09 2.21394868e+01 3.04534636e+00\n",
      "  2.36088279e+00 9.17262648e+00 5.80171980e+05 1.88726412e+01\n",
      "  8.59878648e+00 1.00358772e+00 4.60617041e+00 4.47564276e+05\n",
      "  1.44915890e+01 4.57721633e+05 6.90200783e+07 2.17424318e+02\n",
      "  6.20583364e+00 9.27006841e+00 7.81757335e+00 9.16784921e+08\n",
      "  2.00898238e+00 4.19372773e+03 1.03344286e-01 1.66598192e-01\n",
      "  3.21565560e+00 9.12409857e+00 4.32082449e+04 1.91391646e-01\n",
      "  4.64366991e+05 1.00476956e+00 3.22163007e+01 1.99729283e+06\n",
      "  5.77396460e+00 1.87156494e+00 1.05315320e+01 1.71006544e+01\n",
      "  7.07531661e+01 4.95694805e+00 1.28915465e+00 7.39033231e-03\n",
      "  1.07298331e+08 1.21234708e+02 1.69458295e+07 2.94550879e+01\n",
      "  9.65289523e+05 1.58576391e+10 2.85250853e+00 1.30228502e+02\n",
      "  1.23580410e+07 4.71005225e+00 4.79454131e+07 2.29502423e-03\n",
      "  1.90235346e+06 2.56248299e-01 3.96297254e+02 5.35812674e+05\n",
      "  1.35218945e+02 7.67707686e+06 9.66498451e+00 1.80629593e+00\n",
      "  4.93471606e+06 2.32856570e+08 1.84925419e+03 5.02864810e+00\n",
      "  6.41341883e+05 1.00809900e+00 2.46281009e-01 7.03233680e-01\n",
      "  7.74234400e+02 4.66957443e-02 4.91066777e+01 1.29481829e+03\n",
      "  7.81756837e+06 1.96039222e+00 4.00720543e+00 4.72614185e+05\n",
      "  3.19120931e-02 4.45693222e+00 3.03843302e+07 4.12420775e+01\n",
      "  2.72346058e+01 5.52023130e+05 1.23183940e+02 1.67339497e+02\n",
      "  4.84534579e+05 1.13810854e+01 3.08095196e+07 1.74218531e+00\n",
      "  7.33389774e+06 2.18909488e+08 5.36859033e+06 2.52549274e+08\n",
      "  1.24976298e+04 2.38559944e+00 2.79857039e+00 1.03038206e+00\n",
      "  3.36111947e+00 5.18237091e+05 4.22872927e+05 1.17631261e+01\n",
      "  6.80434622e+06 1.34101849e+02 3.80325540e+06 3.85496367e+06\n",
      "  7.04835109e+01 1.53684934e+00 1.79808934e+06 1.05846530e+03\n",
      "  8.57714780e+08 8.82345350e+05 1.41810946e+07 1.63818204e+01\n",
      "  1.50525096e+00 9.82302661e-01 5.36799648e+05 5.47870375e+06\n",
      "  4.31427369e+08 4.38380674e-01 4.00720543e+00 2.98020333e+07\n",
      "  1.09655647e+00 6.32631900e+00 7.89306913e+06 5.05791715e+01\n",
      "  3.17324712e-01 2.69722233e+09 1.13952629e+07 3.03480810e+07\n",
      "  2.51663184e-01 1.16529621e+01 3.24073653e+08 7.95770431e+06\n",
      "  3.53211566e-04 8.25900181e+08 6.42868529e-04 6.03519040e+04\n",
      "  7.07355727e+00 3.57017469e+00 7.67212094e+08 4.36828427e+00\n",
      "  6.92839838e+07 4.71005225e+00 2.56395299e-01 7.44176603e-01\n",
      "  1.59709868e+00 2.58427049e-01 3.36528433e+08 1.48545277e+06\n",
      "  4.01690889e+06 1.50184541e+06 7.05483602e-01 8.77242926e+01\n",
      "  4.00720543e+00 1.87536374e+06 8.73955684e-01 9.07569938e+00\n",
      "  3.21565560e+00 2.96000492e+07 1.71039293e+01 5.82093679e+06\n",
      "  6.03562754e+06 5.07621437e+02 3.88226332e+07 4.68916459e+07\n",
      "  4.41786866e+05 1.48865156e+08 1.27859161e+02 1.00877264e+00\n",
      "  2.13028152e-01 1.19464230e-01 6.16592927e+00 1.49866110e+07]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (700,700) (4,700) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m theta_optimized, cost_list \u001b[38;5;241m=\u001b[39m half_life_regression(X_train, Y_train, t_train, regularization \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m )\n",
      "Cell \u001b[1;32mIn[138], line 36\u001b[0m, in \u001b[0;36mhalf_life_regression\u001b[1;34m(X, y, delta, regularization, learning_rate, lambda_param, alpha_param, num_iterations)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Cálculo de la pérdida y el gradiente\u001b[39;00m\n\u001b[0;32m     35\u001b[0m loss     \u001b[38;5;241m=\u001b[39m hh_loss_function(y, predicted_p, estimated_h, delta, theta, regularization, lambda_param, alpha_param)\n\u001b[1;32m---> 36\u001b[0m gradient \u001b[38;5;241m=\u001b[39m gradient_partial(y, predicted_p, estimated_h, delta, X, theta, lambda_param, alpha_param)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Actualización de pesos con AdaGrad\u001b[39;00m\n\u001b[0;32m     39\u001b[0m theta, csg \u001b[38;5;241m=\u001b[39m adagrad_update(theta, gradient, learning_rate, csg)\n",
      "Cell \u001b[1;32mIn[133], line 20\u001b[0m, in \u001b[0;36mgradient_partial\u001b[1;34m(p, predicted_p, estimated_h, delta, x, theta, lambda_param, alpha_param)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient_partial\u001b[39m( p, predicted_p, estimated_h, delta, x, theta, lambda_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, alpha_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m ):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Calcula la derivada parcial de la función de pérdida con respecto a cada peso theta_k.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    - gradient : vector de derivadas parciales con respecto a cada theta_k\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     term1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m ( predicted_p \u001b[38;5;241m-\u001b[39m p ) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog( \u001b[38;5;241m2\u001b[39m ) \u001b[38;5;241m*\u001b[39m predicted_p \u001b[38;5;241m*\u001b[39m ( \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m( \u001b[38;5;241m-\u001b[39mdelta \u001b[38;5;241m/\u001b[39m estimated_h ) ) \u001b[38;5;241m*\u001b[39m x\n\u001b[0;32m     21\u001b[0m     term2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m alpha_param \u001b[38;5;241m*\u001b[39m ( estimated_h \u001b[38;5;241m+\u001b[39m delta \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2( p ) ) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog( \u001b[38;5;241m2\u001b[39m ) \u001b[38;5;241m*\u001b[39m estimated_h \u001b[38;5;241m*\u001b[39m x\n\u001b[0;32m     22\u001b[0m     term3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m lambda_param \u001b[38;5;241m*\u001b[39m theta\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (700,700) (4,700) "
     ]
    }
   ],
   "source": [
    "theta_optimized, cost_list = half_life_regression(X_train, Y_train, t_train, regularization = 'l2' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76004cb-7ae7-45e4-adae-a4304b948a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae801a-a27d-41d7-99b7-fd48c1b75478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03924b90-967b-4566-be3b-3fd617f342c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293ac8e-b584-4fa2-bd27-609b3e53f48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d8d29-02ea-46b3-87ff-1bf159f2ac60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd74daa-120c-42ab-b495-116846959a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_theta(D, alpha, lambda_, eta, theta, X, p, Delta, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Optimize the values of theta for the spaced repetition model with safeguards against overflow and underflow.\n",
    "\n",
    "    Parameters:\n",
    "    - D (int): Number of data instances.\n",
    "    - alpha (float): Weight for the half-life term in the loss function.\n",
    "    - lambda_ (float): Regularization parameter.\n",
    "    - eta (float): Learning rate.\n",
    "    - theta (np.array): Initial theta values.\n",
    "    - X (np.array): Feature vectors for each data instance.\n",
    "    - p (np.array): Observed recall rates.\n",
    "    - Delta (np.array): Lag times since each item was last practiced.\n",
    "    - max_iter (int): Maximum number of iterations for the optimization.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Optimized theta values.\n",
    "    \"\"\"\n",
    "    for iteration in range(max_iter):\n",
    "        grad_theta = np.zeros_like(theta)\n",
    "        for t in range(D):\n",
    "            # Safe computation of predicted half-life and probability\n",
    "            theta_x = np.clip(theta.dot(X[t]), -10, 10)  # Prevent overflow in exponent\n",
    "            h_hat = 2 ** theta_x\n",
    "            p_hat = 2 ** np.clip(-Delta[t] / h_hat, -10, 10)  # Prevent underflow/overflow\n",
    "\n",
    "            # Compute theoretical half-life\n",
    "            if p[t] > 0:\n",
    "                h = -Delta[t] / np.log2(p[t])\n",
    "            else:\n",
    "                h = 0  # Assign a default value when p is 0\n",
    "\n",
    "            # Compute the gradients\n",
    "            for k in range(len(theta)):\n",
    "                term1 = 2 * (p[t] - p_hat) * np.log(2) * p_hat * (2 ** (-Delta[t] / h_hat)) * X[t][k]\n",
    "                term2 = 2 * alpha * (h_hat + Delta[t] / np.log2(p[t])) * np.log(2) * h_hat * X[t][k]\n",
    "                term3 = 2 * lambda_ * theta[k]\n",
    "                grad_theta[k] += term1 + term2 + term3\n",
    "\n",
    "        # Update theta\n",
    "        theta -= eta * grad_theta / D  # Average gradient over all instances\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d741418b-b39d-45fd-b166-925071230749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_theta_adagrad(D, alpha, lambda_, eta, theta, X, p, Delta, max_iter=1000, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Optimize the values of theta for the spaced repetition model using AdaGrad.\n",
    "\n",
    "    Parameters:\n",
    "    - D (int): Number of data instances.\n",
    "    - alpha (float): Weight for the half-life term in the loss function.\n",
    "    - lambda_ (float): Regularization parameter.\n",
    "    - eta (float): Learning rate.\n",
    "    - theta (np.array): Initial theta values.\n",
    "    - X (np.array): Feature vectors for each data instance.\n",
    "    - p (np.array): Observed recall rates.\n",
    "    - Delta (np.array): Lag times since each item was last practiced.\n",
    "    - max_iter (int): Maximum number of iterations for the optimization.\n",
    "    - epsilon (float): Small constant to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Optimized theta values.\n",
    "    \"\"\"\n",
    "    # Initialize gradient accumulation\n",
    "    grad_accumulation = np.zeros_like(theta)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        grad_theta = np.zeros_like(theta)\n",
    "        for t in range(D):\n",
    "            # Safe computation of predicted half-life and probability\n",
    "            theta_x = np.clip(theta.dot(X[t]), -10, 10)  # Prevent overflow in exponent\n",
    "            h_hat = 2 ** theta_x\n",
    "            p_hat = 2 ** np.clip(-Delta[t] / h_hat, -10, 10)  # Prevent underflow/overflow\n",
    "\n",
    "            # Compute theoretical half-life\n",
    "            if p[t] > 0:\n",
    "                h = -Delta[t] / np.log2(p[t])\n",
    "            else:\n",
    "                h = 0  # Assign a default value when p is 0\n",
    "\n",
    "            # Compute the gradients\n",
    "            for k in range(len(theta)):\n",
    "                term1 = 2 * (p[t] - p_hat) * np.log(2) * p_hat * (2 ** (-Delta[t] / h_hat)) * X[t][k]\n",
    "                term2 = 2 * alpha * (h_hat + Delta[t] / np.log2(p[t])) * np.log(2) * h_hat * X[t][k]\n",
    "                term3 = 2 * lambda_ * theta[k]\n",
    "                grad_theta[k] += term1 + term2 + term3\n",
    "\n",
    "        # Accumulate the square of gradients\n",
    "        grad_accumulation += grad_theta ** 2\n",
    "\n",
    "        # Update theta using AdaGrad adjustment\n",
    "        adjusted_eta = eta / (np.sqrt(grad_accumulation) + epsilon)\n",
    "        theta -= adjusted_eta * grad_theta / D\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "18e6ba58-1721-4ea1-86e1-7f60606b2873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ESTA ES LA REAL FUNCION\n",
    "\n",
    "def optimize_theta_adagrad(D, alpha, lambda_, eta, theta, X, p, Delta, max_iter=1000, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Optimize the values of theta for the spaced repetition model using AdaGrad,\n",
    "    and calculate the cost function explicitly at each iteration.\n",
    "\n",
    "    Parameters:\n",
    "    - D (int): Number of data instances.\n",
    "    - alpha (float): Weight for the half-life term in the loss function.\n",
    "    - lambda_ (float): Regularization parameter.\n",
    "    - eta (float): Learning rate.\n",
    "    - theta (np.array): Initial theta values.\n",
    "    - X (np.array): Feature vectors for each data instance.\n",
    "    - p (np.array): Observed recall rates.\n",
    "    - Delta (np.array): Lag times since each item was last practiced.\n",
    "    - max_iter (int): Maximum number of iterations for the optimization.\n",
    "    - epsilon (float): Small constant to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Optimized theta values.\n",
    "    - list: Cost per iteration.\n",
    "    \"\"\"\n",
    "    # Initialize gradient accumulation and cost tracking\n",
    "    grad_accumulation = np.zeros_like(theta)\n",
    "    cost_history = []\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        grad_theta = np.zeros_like(theta)\n",
    "        cost = 0\n",
    "\n",
    "        for t in range(D):\n",
    "            # Safe computation of predicted half-life and probability\n",
    "            theta_x = np.clip(theta.dot(X[t]), -10, 10)  # Prevent overflow in exponent\n",
    "            h_hat = 2 ** theta_x\n",
    "            p_hat = 2 ** np.clip(-Delta[t] / h_hat, -10, 10)  # Prevent underflow/overflow\n",
    "\n",
    "            # Compute theoretical half-life\n",
    "            if p[t] > 0:\n",
    "                h = -Delta[t] / np.log2(p[t])\n",
    "            else:\n",
    "                h = 0  # Assign a default value when p is 0\n",
    "\n",
    "            # Update cost\n",
    "            cost += (p[t] - p_hat) ** 2 + alpha * (h - h_hat) ** 2\n",
    "\n",
    "            # Compute the gradients\n",
    "            for k in range(len(theta)):\n",
    "                term1 = 2 * (p[t] - p_hat) * np.log(2) * p_hat * (2 ** (-Delta[t] / h_hat)) * X[t][k]\n",
    "                term2 = 2 * alpha * (h_hat + Delta[t] / np.log2(p[t])) * np.log(2) * h_hat * X[t][k]\n",
    "                term3 = 2 * lambda_ * theta[k]\n",
    "                grad_theta[k] += term1 + term2 + term3\n",
    "\n",
    "        # Add regularization term to cost\n",
    "        cost += lambda_ * np.sum(theta ** 2)\n",
    "        cost_history.append(cost / D)  # Average cost per instance\n",
    "\n",
    "        # Accumulate the square of gradients\n",
    "        grad_accumulation += grad_theta ** 2\n",
    "\n",
    "        # Update theta using AdaGrad adjustment\n",
    "        adjusted_eta = eta / (np.sqrt(grad_accumulation) + epsilon)\n",
    "        theta -= adjusted_eta * grad_theta / D\n",
    "\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c7662bbd-6494-4f44-be17-3ab0e42ff362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplp pequeño\n",
    "\n",
    "# Creating a sample DataFrame to use with the optimize_theta function\n",
    "data = {\n",
    "    \"feature1\": [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    \"feature2\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"feature3\": [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    \"recall_rate\": [0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "    \"lag_time\": [1, 2, 3, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Converting DataFrame columns to numpy arrays\n",
    "X = df[[\"feature1\", \"feature2\", \"feature3\"]].values\n",
    "p = df[\"recall_rate\"].values\n",
    "Delta = df[\"lag_time\"].values\n",
    "\n",
    "# Set the parameters for optimization\n",
    "D = len(df)  # Number of data instances\n",
    "alpha = 0.5\n",
    "lambda_ = 0.1\n",
    "eta = 0.01\n",
    "theta = np.random.randn(3)  # Initial theta values for a 3-feature model\n",
    "\n",
    "# Using the optimize_theta function\n",
    "optimized_theta = optimize_theta(D, alpha, lambda_, eta, theta, X, p, Delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1bd1c554-7e9e-4450-a2bc-af232bf706df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.12879734, -0.65156256,  0.41833509])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_theta, costo_h = optimize_theta_adagrad(D, alpha, lambda_, eta, theta, X, p, Delta)\n",
    "optimized_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "daea6c45-1616-4f26-98d3-583a7940613e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.687350451498462,\n",
       " 2.6860919635366534,\n",
       " 2.685190738215561,\n",
       " 2.6844529674772537,\n",
       " 2.6838135541449026,\n",
       " 2.683241595510166,\n",
       " 2.6827195932191605,\n",
       " 2.6822365122155922,\n",
       " 2.68178486651717,\n",
       " 2.6813593021584743,\n",
       " 2.6809558333626287,\n",
       " 2.680571398022611,\n",
       " 2.6802035831752513,\n",
       " 2.679850447216697,\n",
       " 2.6795104002073695,\n",
       " 2.679182120632304,\n",
       " 2.678864495902597,\n",
       " 2.678556578812707,\n",
       " 2.678257555016295,\n",
       " 2.6779667182930114,\n",
       " 2.6776834514397745,\n",
       " 2.6774072112978984,\n",
       " 2.677137516871791,\n",
       " 2.6768739397929737,\n",
       " 2.676616096587156,\n",
       " 2.676363642344311,\n",
       " 2.6761162654925723,\n",
       " 2.6758736834493866,\n",
       " 2.675635638976315,\n",
       " 2.675401897103128,\n",
       " 2.675172242516113,\n",
       " 2.6749464773277074,\n",
       " 2.6747244191615236,\n",
       " 2.6745058994998905,\n",
       " 2.6742907622512093,\n",
       " 2.674078862502392,\n",
       " 2.6738700654279635,\n",
       " 2.673664245332424,\n",
       " 2.6734612848064945,\n",
       " 2.6732610739811107,\n",
       " 2.673063509865654,\n",
       " 2.672868495759105,\n",
       " 2.672675940724475,\n",
       " 2.672485759118463,\n",
       " 2.6722978701693436,\n",
       " 2.6721121975971998,\n",
       " 2.671928669271427,\n",
       " 2.6717472169011165,\n",
       " 2.671567775754533,\n",
       " 2.671390284404446,\n",
       " 2.6712146844964075,\n",
       " 2.6710409205375463,\n",
       " 2.670868939703679,\n",
       " 2.67069869166282,\n",
       " 2.670530128413447,\n",
       " 2.6703632041360077,\n",
       " 2.6701978750563997,\n",
       " 2.6700340993202367,\n",
       " 2.6698718368768892,\n",
       " 2.6697110493723804,\n",
       " 2.669551700050339,\n",
       " 2.6693937536602537,\n",
       " 2.669237176372417,\n",
       " 2.6690819356989266,\n",
       " 2.6689280004202667,\n",
       " 2.668775340516987,\n",
       " 2.6686239271060095,\n",
       " 2.668473732381265,\n",
       " 2.668324729558223,\n",
       " 2.6681768928220575,\n",
       " 2.6680301972791645,\n",
       " 2.6678846189117227,\n",
       " 2.667740134535124,\n",
       " 2.667596721758016,\n",
       " 2.667454358944779,\n",
       " 2.667313025180257,\n",
       " 2.6671727002365824,\n",
       " 2.667033364541935,\n",
       " 2.6668949991511015,\n",
       " 2.666757585717724,\n",
       " 2.666621106468102,\n",
       " 2.666485544176448,\n",
       " 2.666350882141512,\n",
       " 2.666217104164465,\n",
       " 2.66608419452797,\n",
       " 2.6659521379763644,\n",
       " 2.665820919696875,\n",
       " 2.665690525301808,\n",
       " 2.665560940811647,\n",
       " 2.665432152639012,\n",
       " 2.6653041475734134,\n",
       " 2.6651769127667633,\n",
       " 2.6650504357195963,\n",
       " 2.6649247042679525,\n",
       " 2.664799706570901,\n",
       " 2.664675431098634,\n",
       " 2.6645518666211405,\n",
       " 2.6644290021973864,\n",
       " 2.6643068271650083,\n",
       " 2.6641853311304615,\n",
       " 2.6640645039596262,\n",
       " 2.66394433576881,\n",
       " 2.6638248169161765,\n",
       " 2.6637059379935204,\n",
       " 2.66358768981842,\n",
       " 2.6634700634267077,\n",
       " 2.663353050065278,\n",
       " 2.6632366411851818,\n",
       " 2.663120828435022,\n",
       " 2.663005603654608,\n",
       " 2.6628909588688847,\n",
       " 2.6627768862820935,\n",
       " 2.66266337827218,\n",
       " 2.6625504273854093,\n",
       " 2.6624380263312153,\n",
       " 2.6623261679772297,\n",
       " 2.6622148453445194,\n",
       " 2.6621040516029897,\n",
       " 2.6619937800669926,\n",
       " 2.6618840241910626,\n",
       " 2.661774777565846,\n",
       " 2.6616660339141633,\n",
       " 2.661557787087216,\n",
       " 2.661450031060947,\n",
       " 2.6613427599325123,\n",
       " 2.6612359679169026,\n",
       " 2.661129649343661,\n",
       " 2.6610237986537366,\n",
       " 2.660918410396445,\n",
       " 2.660813479226518,\n",
       " 2.6607089999012876,\n",
       " 2.6606049672779264,\n",
       " 2.66050137631082,\n",
       " 2.6603982220490043,\n",
       " 2.6602954996336976,\n",
       " 2.6601932042959096,\n",
       " 2.660091331354142,\n",
       " 2.6599898762121468,\n",
       " 2.6598888343567717,\n",
       " 2.6597882013558665,\n",
       " 2.659687972856264,\n",
       " 2.6595881445818215,\n",
       " 2.6594887123315205,\n",
       " 2.6593896719776353,\n",
       " 2.6592910194639496,\n",
       " 2.6591927508040345,\n",
       " 2.65909486207958,\n",
       " 2.658997349438766,\n",
       " 2.6589002090947056,\n",
       " 2.6588034373239067,\n",
       " 2.6587070304648064,\n",
       " 2.658610984916336,\n",
       " 2.6585152971365185,\n",
       " 2.6584199636411325,\n",
       " 2.6583249810023952,\n",
       " 2.658230345847688,\n",
       " 2.6581360548583204,\n",
       " 2.6580421047683354,\n",
       " 2.657948492363336,\n",
       " 2.657855214479354,\n",
       " 2.6577622680017505,\n",
       " 2.65766964986414,\n",
       " 2.6575773570473524,\n",
       " 2.6574853865784234,\n",
       " 2.6573937355296033,\n",
       " 2.6573024010174016,\n",
       " 2.6572113802016597,\n",
       " 2.657120670284634,\n",
       " 2.657030268510125,\n",
       " 2.656940172162609,\n",
       " 2.6568503785664057,\n",
       " 2.656760885084865,\n",
       " 2.6566716891195705,\n",
       " 2.656582788109568,\n",
       " 2.656494179530612,\n",
       " 2.656405860894439,\n",
       " 2.6563178297480468,\n",
       " 2.656230083672993,\n",
       " 2.65614262028473,\n",
       " 2.6560554372319323,\n",
       " 2.655968532195857,\n",
       " 2.65588190288971,\n",
       " 2.6557955470580357,\n",
       " 2.6557094624761186,\n",
       " 2.6556236469493983,\n",
       " 2.655538098312898,\n",
       " 2.655452814430674,\n",
       " 2.655367793195267,\n",
       " 2.6552830325271755,\n",
       " 2.6551985303743386,\n",
       " 2.6551142847116314,\n",
       " 2.6550302935403716,\n",
       " 2.6549465548878373,\n",
       " 2.6548630668067976,\n",
       " 2.6547798273750556,\n",
       " 2.6546968346949953,\n",
       " 2.65461408689315,\n",
       " 2.6545315821197626,\n",
       " 2.6544493185483784,\n",
       " 2.654367294375431,\n",
       " 2.6542855078198424,\n",
       " 2.6542039571226317,\n",
       " 2.654122640546535,\n",
       " 2.6540415563756254,\n",
       " 2.653960702914959,\n",
       " 2.6538800784902064,\n",
       " 2.653799681447303,\n",
       " 2.6537195101521145,\n",
       " 2.6536395629900973,\n",
       " 2.6535598383659744,\n",
       " 2.6534803347034073,\n",
       " 2.6534010504446863,\n",
       " 2.653321984050429,\n",
       " 2.6532431339992737,\n",
       " 2.6531644987875795,\n",
       " 2.653086076929153,\n",
       " 2.653007866954952,\n",
       " 2.652929867412819,\n",
       " 2.6528520768672035,\n",
       " 2.6527744938989013,\n",
       " 2.652697117104794,\n",
       " 2.6526199450975922,\n",
       " 2.6525429765055892,\n",
       " 2.6524662099724177,\n",
       " 2.6523896441568047,\n",
       " 2.652313277732344,\n",
       " 2.652237109387263,\n",
       " 2.652161137824195,\n",
       " 2.6520853617599665,\n",
       " 2.652009779925366,\n",
       " 2.6519343910649518,\n",
       " 2.651859193936825,\n",
       " 2.6517841873124333,\n",
       " 2.651709369976376,\n",
       " 2.651634740726198,\n",
       " 2.6515602983722077,\n",
       " 2.6514860417372708,\n",
       " 2.651411969656648,\n",
       " 2.6513380809777924,\n",
       " 2.6512643745601876,\n",
       " 2.6511908492751592,\n",
       " 2.6511175040057102,\n",
       " 2.6510443376463537,\n",
       " 2.6509713491029423,\n",
       " 2.6508985372925085,\n",
       " 2.6508259011431052,\n",
       " 2.6507534395936476,\n",
       " 2.6506811515937634,\n",
       " 2.650609036103636,\n",
       " 2.65053709209386,\n",
       " 2.6504653185452964,\n",
       " 2.65039371444893,\n",
       " 2.650322278805722,\n",
       " 2.6502510106264836,\n",
       " 2.6501799089317304,\n",
       " 2.6501089727515543,\n",
       " 2.650038201125493,\n",
       " 2.6499675931024003,\n",
       " 2.649897147740319,\n",
       " 2.649826864106358,\n",
       " 2.649756741276573,\n",
       " 2.64968677833584,\n",
       " 2.649616974377746,\n",
       " 2.6495473285044673,\n",
       " 2.6494778398266554,\n",
       " 2.64940850746333,\n",
       " 2.649339330541767,\n",
       " 2.6492703081973885,\n",
       " 2.6492014395736576,\n",
       " 2.649132723821974,\n",
       " 2.649064160101573,\n",
       " 2.6489957475794244,\n",
       " 2.6489274854301272,\n",
       " 2.6488593728358203,\n",
       " 2.648791408986083,\n",
       " 2.648723593077834,\n",
       " 2.6486559243152525,\n",
       " 2.648588401909673,\n",
       " 2.648521025079503,\n",
       " 2.648453793050133,\n",
       " 2.648386705053848,\n",
       " 2.6483197603297404,\n",
       " 2.6482529581236323,\n",
       " 2.648186297687981,\n",
       " 2.6481197782818113,\n",
       " 2.648053399170622,\n",
       " 2.6479871596263154,\n",
       " 2.6479210589271176,\n",
       " 2.6478550963575,\n",
       " 2.647789271208101,\n",
       " 2.6477235827756584,\n",
       " 2.647658030362935,\n",
       " 2.6475926132786376,\n",
       " 2.6475273308373604,\n",
       " 2.6474621823595035,\n",
       " 2.6473971671712073,\n",
       " 2.647332284604288,\n",
       " 2.647267533996168,\n",
       " 2.64720291468981,\n",
       " 2.647138426033653,\n",
       " 2.6470740673815465,\n",
       " 2.6470098380926923,\n",
       " 2.6469457375315804,\n",
       " 2.6468817650679215,\n",
       " 2.6468179200766015,\n",
       " 2.646754201937606,\n",
       " 2.6466906100359737,\n",
       " 2.646627143761733,\n",
       " 2.646563802509848,\n",
       " 2.646500585680164,\n",
       " 2.6464374926773475,\n",
       " 2.6463745229108344,\n",
       " 2.646311675794778,\n",
       " 2.6462489507479967,\n",
       " 2.6461863471939213,\n",
       " 2.646123864560539,\n",
       " 2.646061502280351,\n",
       " 2.6459992597903197,\n",
       " 2.6459371365318156,\n",
       " 2.6458751319505756,\n",
       " 2.6458132454966514,\n",
       " 2.6457514766243606,\n",
       " 2.6456898247922482,\n",
       " 2.6456282894630316,\n",
       " 2.645566870103562,\n",
       " 2.6455055661847777,\n",
       " 2.645444377181655,\n",
       " 2.6453833025731806,\n",
       " 2.6453223418422906,\n",
       " 2.6452614944758404,\n",
       " 2.645200759964553,\n",
       " 2.6451401378029944,\n",
       " 2.645079627489519,\n",
       " 2.6450192285262304,\n",
       " 2.6449589404189524,\n",
       " 2.644898762677177,\n",
       " 2.644838694814039,\n",
       " 2.6447787363462707,\n",
       " 2.6447188867941596,\n",
       " 2.644659145681531,\n",
       " 2.644599512535689,\n",
       " 2.6445399868873936,\n",
       " 2.6444805682708212,\n",
       " 2.6444212562235374,\n",
       " 2.6443620502864507,\n",
       " 2.644302950003783,\n",
       " 2.644243954923041,\n",
       " 2.6441850645949816,\n",
       " 2.644126278573572,\n",
       " 2.6440675964159674,\n",
       " 2.6440090176824684,\n",
       " 2.643950541936506,\n",
       " 2.643892168744591,\n",
       " 2.6438338976762954,\n",
       " 2.6437757283042247,\n",
       " 2.643717660203975,\n",
       " 2.6436596929541194,\n",
       " 2.643601826136165,\n",
       " 2.6435440593345336,\n",
       " 2.6434863921365332,\n",
       " 2.6434288241323185,\n",
       " 2.64337135491488,\n",
       " 2.6433139840800037,\n",
       " 2.6432567112262517,\n",
       " 2.64319953595493,\n",
       " 2.6431424578700673,\n",
       " 2.6430854765783858,\n",
       " 2.6430285916892764,\n",
       " 2.642971802814774,\n",
       " 2.6429151095695307,\n",
       " 2.6428585115707954,\n",
       " 2.642802008438381,\n",
       " 2.6427455997946487,\n",
       " 2.6426892852644825,\n",
       " 2.6426330644752616,\n",
       " 2.642576937056839,\n",
       " 2.6425209026415226,\n",
       " 2.642464960864044,\n",
       " 2.6424091113615473,\n",
       " 2.642353353773557,\n",
       " 2.6422976877419555,\n",
       " 2.642242112910975,\n",
       " 2.642186628927162,\n",
       " 2.642131235439358,\n",
       " 2.6420759320986873,\n",
       " 2.642020718558527,\n",
       " 2.64196559447449,\n",
       " 2.6419105595044066,\n",
       " 2.6418556133083024,\n",
       " 2.6418007555483785,\n",
       " 2.641745985888993,\n",
       " 2.641691303996641,\n",
       " 2.641636709539937,\n",
       " 2.6415822021895923,\n",
       " 2.641527781618401,\n",
       " 2.641473447501223,\n",
       " 2.6414191995149525,\n",
       " 2.6413650373385216,\n",
       " 2.6413109606528637,\n",
       " 2.6412569691409016,\n",
       " 2.6412030624875387,\n",
       " 2.641149240379627,\n",
       " 2.641095502505965,\n",
       " 2.641041848557266,\n",
       " 2.640988278226157,\n",
       " 2.6409347912071466,\n",
       " 2.6408813871966226,\n",
       " 2.6408280658928263,\n",
       " 2.640774826995843,\n",
       " 2.6407216702075806,\n",
       " 2.640668595231756,\n",
       " 2.6406156017738867,\n",
       " 2.6405626895412624,\n",
       " 2.640509858242941,\n",
       " 2.640457107589726,\n",
       " 2.640404437294161,\n",
       " 2.640351847070505,\n",
       " 2.6402993366347234,\n",
       " 2.6402469057044753,\n",
       " 2.6401945539990965,\n",
       " 2.640142281239582,\n",
       " 2.640090087148581,\n",
       " 2.6400379714503757,\n",
       " 2.6399859338708724,\n",
       " 2.639933974137585,\n",
       " 2.639882091979623,\n",
       " 2.6398302871276815,\n",
       " 2.639778559314021,\n",
       " 2.639726908272462,\n",
       " 2.6396753337383667,\n",
       " 2.639623835448631,\n",
       " 2.6395724131416687,\n",
       " 2.6395210665574025,\n",
       " 2.639469795437246,\n",
       " 2.6394185995241006,\n",
       " 2.6393674785623333,\n",
       " 2.6393164322977745,\n",
       " 2.6392654604776995,\n",
       " 2.63921456285082,\n",
       " 2.639163739167274,\n",
       " 2.639112989178608,\n",
       " 2.639062312637775,\n",
       " 2.6390117092991163,\n",
       " 2.638961178918353,\n",
       " 2.6389107212525778,\n",
       " 2.6388603360602385,\n",
       " 2.6388100231011316,\n",
       " 2.638759782136391,\n",
       " 2.638709612928479,\n",
       " 2.638659515241169,\n",
       " 2.638609488839547,\n",
       " 2.6385595334899894,\n",
       " 2.638509648960161,\n",
       " 2.6384598350190047,\n",
       " 2.638410091436726,\n",
       " 2.6383604179847886,\n",
       " 2.638310814435903,\n",
       " 2.638261280564016,\n",
       " 2.638211816144305,\n",
       " 2.638162420953163,\n",
       " 2.638113094768194,\n",
       " 2.638063837368201,\n",
       " 2.6380146485331784,\n",
       " 2.6379655280443037,\n",
       " 2.637916475683928,\n",
       " 2.6378674912355637,\n",
       " 2.637818574483881,\n",
       " 2.6377697252146985,\n",
       " 2.637720943214968,\n",
       " 2.6376722282727805,\n",
       " 2.637623580177338,\n",
       " 2.6375749987189616,\n",
       " 2.637526483689076,\n",
       " 2.637478034880205,\n",
       " 2.6374296520859586,\n",
       " 2.637381335101028,\n",
       " 2.6373330837211766,\n",
       " 2.637284897743233,\n",
       " 2.6372367769650857,\n",
       " 2.6371887211856677,\n",
       " 2.63714073020496,\n",
       " 2.637092803823972,\n",
       " 2.637044941844743,\n",
       " 2.636997144070329,\n",
       " 2.6369494103048026,\n",
       " 2.636901740353237,\n",
       " 2.636854134021704,\n",
       " 2.6368065911172667,\n",
       " 2.6367591114479705,\n",
       " 2.636711694822836,\n",
       " 2.636664341051854,\n",
       " 2.6366170499459796,\n",
       " 2.63656982131712,\n",
       " 2.6365226549781324,\n",
       " 2.636475550742818,\n",
       " 2.6364285084259107,\n",
       " 2.636381527843073,\n",
       " 2.636334608810892,\n",
       " 2.636287751146869,\n",
       " 2.636240954669416,\n",
       " 2.636194219197846,\n",
       " 2.63614754455237,\n",
       " 2.63610093055409,\n",
       " 2.63605437702499,\n",
       " 2.6360078837879364,\n",
       " 2.6359614506666604,\n",
       " 2.635915077485771,\n",
       " 2.635868764070723,\n",
       " 2.635822510247837,\n",
       " 2.635776315844277,\n",
       " 2.6357301806880473,\n",
       " 2.6356841046079955,\n",
       " 2.635638087433795,\n",
       " 2.635592128995945,\n",
       " 2.6355462291257665,\n",
       " 2.635500387655395,\n",
       " 2.635454604417771,\n",
       " 2.635408879246642,\n",
       " 2.635363211976553,\n",
       " 2.6353176024428384,\n",
       " 2.6352720504816247,\n",
       " 2.6352265559298145,\n",
       " 2.6351811186250926,\n",
       " 2.6351357384059098,\n",
       " 2.635090415111489,\n",
       " 2.635045148581807,\n",
       " 2.6349999386576073,\n",
       " 2.6349547851803723,\n",
       " 2.63490968799234,\n",
       " 2.6348646469364847,\n",
       " 2.6348196618565183,\n",
       " 2.6347747325968878,\n",
       " 2.6347298590027584,\n",
       " 2.6346850409200253,\n",
       " 2.634640278195297,\n",
       " 2.6345955706758963,\n",
       " 2.634550918209853,\n",
       " 2.634506320645899,\n",
       " 2.634461777833468,\n",
       " 2.6344172896226854,\n",
       " 2.6343728558643664,\n",
       " 2.6343284764100123,\n",
       " 2.6342841511118054,\n",
       " 2.6342398798226045,\n",
       " 2.6341956623959395,\n",
       " 2.6341514986860077,\n",
       " 2.6341073885476733,\n",
       " 2.6340633318364537,\n",
       " 2.6340193284085283,\n",
       " 2.6339753781207227,\n",
       " 2.6339314808305105,\n",
       " 2.63388763639601,\n",
       " 2.633843844675975,\n",
       " 2.6338001055297955,\n",
       " 2.633756418817492,\n",
       " 2.6337127843997097,\n",
       " 2.633669202137723,\n",
       " 2.633625671893414,\n",
       " 2.633582193529292,\n",
       " 2.6335387669084667,\n",
       " 2.633495391894662,\n",
       " 2.633452068352203,\n",
       " 2.6334087961460138,\n",
       " 2.6333655751416165,\n",
       " 2.6333224052051234,\n",
       " 2.6332792862032344,\n",
       " 2.633236218003241,\n",
       " 2.6331932004730065,\n",
       " 2.633150233480979,\n",
       " 2.6331073168961794,\n",
       " 2.633064450588198,\n",
       " 2.6330216344271946,\n",
       " 2.632978868283888,\n",
       " 2.632936152029566,\n",
       " 2.6328934855360626,\n",
       " 2.6328508686757734,\n",
       " 2.6328083013216443,\n",
       " 2.6327657833471596,\n",
       " 2.632723314626359,\n",
       " 2.6326808950338125,\n",
       " 2.632638524444635,\n",
       " 2.6325962027344674,\n",
       " 2.6325539297794887,\n",
       " 2.6325117054564005,\n",
       " 2.6324695296424303,\n",
       " 2.6324274022153262,\n",
       " 2.6323853230533567,\n",
       " 2.6323432920353014,\n",
       " 2.632301309040455,\n",
       " 2.6322593739486195,\n",
       " 2.632217486640101,\n",
       " 2.6321756469957145,\n",
       " 2.63213385489677,\n",
       " 2.6320921102250727,\n",
       " 2.6320504128629283,\n",
       " 2.632008762693128,\n",
       " 2.631967159598953,\n",
       " 2.631925603464172,\n",
       " 2.6318840941730324,\n",
       " 2.631842631610265,\n",
       " 2.631801215661075,\n",
       " 2.6317598462111436,\n",
       " 2.631718523146621,\n",
       " 2.6316772463541307,\n",
       " 2.631636015720755,\n",
       " 2.631594831134048,\n",
       " 2.6315536924820186,\n",
       " 2.631512599653136,\n",
       " 2.6314715525363215,\n",
       " 2.631430551020954,\n",
       " 2.63138959499686,\n",
       " 2.6313486843543137,\n",
       " 2.631307818984035,\n",
       " 2.6312669987771855,\n",
       " 2.6312262236253674,\n",
       " 2.631185493420618,\n",
       " 2.631144808055414,\n",
       " 2.631104167422662,\n",
       " 2.6310635714157,\n",
       " 2.6310230199282905,\n",
       " 2.630982512854624,\n",
       " 2.630942050089314,\n",
       " 2.6309016315273945,\n",
       " 2.630861257064315,\n",
       " 2.6308209265959444,\n",
       " 2.6307806400185636,\n",
       " 2.6307403972288634,\n",
       " 2.6307001981239453,\n",
       " 2.630660042601316,\n",
       " 2.6306199305588884,\n",
       " 2.630579861894975,\n",
       " 2.63053983650829,\n",
       " 2.6304998542979456,\n",
       " 2.6304599151634496,\n",
       " 2.6304200190046987,\n",
       " 2.630380165721987,\n",
       " 2.6303403552159947,\n",
       " 2.6303005873877883,\n",
       " 2.630260862138819,\n",
       " 2.6302211793709227,\n",
       " 2.6301815389863132,\n",
       " 2.6301419408875857,\n",
       " 2.63010238497771,\n",
       " 2.630062871160029,\n",
       " 2.630023399338262,\n",
       " 2.6299839694164917,\n",
       " 2.6299445812991777,\n",
       " 2.6299052348911394,\n",
       " 2.629865930097565,\n",
       " 2.629826666824,\n",
       " 2.6297874449763556,\n",
       " 2.6297482644608996,\n",
       " 2.629709125184253,\n",
       " 2.6296700270533977,\n",
       " 2.629630969975663,\n",
       " 2.6295919538587347,\n",
       " 2.629552978610641,\n",
       " 2.6295140441397615,\n",
       " 2.6294751503548204,\n",
       " 2.6294362971648857,\n",
       " 2.629397484479367,\n",
       " 2.629358712208012,\n",
       " 2.6293199802609113,\n",
       " 2.6292812885484844,\n",
       " 2.629242636981492,\n",
       " 2.6292040254710254,\n",
       " 2.629165453928504,\n",
       " 2.629126922265683,\n",
       " 2.629088430394638,\n",
       " 2.629049978227777,\n",
       " 2.6290115656778257,\n",
       " 2.6289731926578392,\n",
       " 2.628934859081188,\n",
       " 2.6288965648615634,\n",
       " 2.6288583099129754,\n",
       " 2.62882009414975,\n",
       " 2.628781917486525,\n",
       " 2.628743779838253,\n",
       " 2.6287056811201945,\n",
       " 2.628667621247925,\n",
       " 2.628629600137324,\n",
       " 2.628591617704577,\n",
       " 2.628553673866174,\n",
       " 2.6285157685389122,\n",
       " 2.6284779016398856,\n",
       " 2.62844007308649,\n",
       " 2.6284022827964195,\n",
       " 2.6283645306876666,\n",
       " 2.6283268166785163,\n",
       " 2.62828914068755,\n",
       " 2.628251502633643,\n",
       " 2.6282139024359563,\n",
       " 2.6281763400139444,\n",
       " 2.6281388152873513,\n",
       " 2.6281013281762036,\n",
       " 2.6280638786008157,\n",
       " 2.6280264664817863,\n",
       " 2.6279890917399937,\n",
       " 2.627951754296599,\n",
       " 2.6279144540730424,\n",
       " 2.627877190991044,\n",
       " 2.627839964972597,\n",
       " 2.6278027759399736,\n",
       " 2.627765623815717,\n",
       " 2.6277285085226474,\n",
       " 2.6276914299838507,\n",
       " 2.6276543881226875,\n",
       " 2.6276173828627813,\n",
       " 2.62758041412803,\n",
       " 2.6275434818425945,\n",
       " 2.6275065859308975,\n",
       " 2.627469726317631,\n",
       " 2.627432902927742,\n",
       " 2.6273961156864445,\n",
       " 2.6273593645192084,\n",
       " 2.6273226493517616,\n",
       " 2.627285970110095,\n",
       " 2.627249326720445,\n",
       " 2.6272127191093126,\n",
       " 2.6271761472034463,\n",
       " 2.627139610929847,\n",
       " 2.627103110215768,\n",
       " 2.6270666449887123,\n",
       " 2.627030215176429,\n",
       " 2.626993820706917,\n",
       " 2.62695746150842,\n",
       " 2.626921137509428,\n",
       " 2.626884848638673,\n",
       " 2.6268485948251286,\n",
       " 2.6268123759980133,\n",
       " 2.626776192086781,\n",
       " 2.6267400430211283,\n",
       " 2.626703928730989,\n",
       " 2.6266678491465343,\n",
       " 2.6266318041981673,\n",
       " 2.6265957938165307,\n",
       " 2.6265598179324963,\n",
       " 2.6265238764771714,\n",
       " 2.626487969381893,\n",
       " 2.626452096578229,\n",
       " 2.626416257997973,\n",
       " 2.6263804535731525,\n",
       " 2.6263446832360158,\n",
       " 2.6263089469190426,\n",
       " 2.626273244554935,\n",
       " 2.626237576076613,\n",
       " 2.6262019414172304,\n",
       " 2.6261663405101543,\n",
       " 2.6261307732889763,\n",
       " 2.626095239687503,\n",
       " 2.626059739639765,\n",
       " 2.62602427308001,\n",
       " 2.625988839942695,\n",
       " 2.625953440162502,\n",
       " 2.625918073674321,\n",
       " 2.6258827404132608,\n",
       " 2.625847440314635,\n",
       " 2.6258121733139754,\n",
       " 2.6257769393470234,\n",
       " 2.6257417383497286,\n",
       " 2.625706570258248,\n",
       " 2.6256714350089503,\n",
       " 2.6256363325384067,\n",
       " 2.625601262783397,\n",
       " 2.625566225680902,\n",
       " 2.6255312211681128,\n",
       " 2.625496249182418,\n",
       " 2.62546130966141,\n",
       " 2.6254264025428835,\n",
       " 2.625391527764831,\n",
       " 2.6253566852654457,\n",
       " 2.6253218749831193,\n",
       " 2.6252870968564417,\n",
       " 2.6252523508241987,\n",
       " 2.625217636825369,\n",
       " 2.6251829547991337,\n",
       " 2.625148304684858,\n",
       " 2.6251136864221074,\n",
       " 2.6250790999506375,\n",
       " 2.6250445452103963,\n",
       " 2.6250100221415194,\n",
       " 2.624975530684335,\n",
       " 2.6249410707793586,\n",
       " 2.6249066423672938,\n",
       " 2.6248722453890325,\n",
       " 2.624837879785654,\n",
       " 2.6248035454984158,\n",
       " 2.624769242468769,\n",
       " 2.624734970638346,\n",
       " 2.624700729948958,\n",
       " 2.6246665203426054,\n",
       " 2.6246323417614636,\n",
       " 2.6245981941478904,\n",
       " 2.624564077444428,\n",
       " 2.62452999159379,\n",
       " 2.6244959365388754,\n",
       " 2.6244619122227535,\n",
       " 2.624427918588679,\n",
       " 2.624393955580075,\n",
       " 2.624360023140542,\n",
       " 2.6243261212138562,\n",
       " 2.624292249743966,\n",
       " 2.6242584086749945,\n",
       " 2.624224597951235,\n",
       " 2.624190817517152,\n",
       " 2.6241570673173826,\n",
       " 2.624123347296733,\n",
       " 2.6240896574001784,\n",
       " 2.6240559975728615,\n",
       " 2.62402236776009,\n",
       " 2.623988767907348,\n",
       " 2.6239551979602767,\n",
       " 2.6239216578646856,\n",
       " 2.6238881475665528,\n",
       " 2.6238546670120115,\n",
       " 2.623821216147369,\n",
       " 2.6237877949190875,\n",
       " 2.623754403273796,\n",
       " 2.623721041158281,\n",
       " 2.6236877085194923,\n",
       " 2.6236544053045394,\n",
       " 2.6236211314606894,\n",
       " 2.623587886935371,\n",
       " 2.6235546716761666,\n",
       " 2.6235214856308198,\n",
       " 2.6234883287472295,\n",
       " 2.623455200973449,\n",
       " 2.623422102257689,\n",
       " 2.623389032548314,\n",
       " 2.6233559917938436,\n",
       " 2.6233229799429463,\n",
       " 2.62328999694445,\n",
       " 2.6232570427473303,\n",
       " 2.6232241173007145,\n",
       " 2.6231912205538817,\n",
       " 2.6231583524562603,\n",
       " 2.6231255129574307,\n",
       " 2.623092702007118,\n",
       " 2.6230599195551996,\n",
       " 2.623027165551696,\n",
       " 2.6229944399467797,\n",
       " 2.622961742690767,\n",
       " 2.622929073734121,\n",
       " 2.622896433027448,\n",
       " 2.622863820521501,\n",
       " 2.6228312361671775,\n",
       " 2.6227986799155163,\n",
       " 2.6227661517177006,\n",
       " 2.6227336515250537,\n",
       " 2.6227011792890456,\n",
       " 2.62266873496128,\n",
       " 2.622636318493509,\n",
       " 2.6226039298376165,\n",
       " 2.622571568945634,\n",
       " 2.622539235769726,\n",
       " 2.6225069302621975,\n",
       " 2.6224746523754896,\n",
       " 2.6224424020621835,\n",
       " 2.6224101792749925,\n",
       " 2.6223779839667722,\n",
       " 2.6223458160905064,\n",
       " 2.6223136755993193,\n",
       " 2.6222815624464646,\n",
       " 2.6222494765853375,\n",
       " 2.6222174179694573,\n",
       " 2.6221853865524807,\n",
       " 2.6221533822881997,\n",
       " 2.62212140513053,\n",
       " 2.6220894550335268,\n",
       " 2.622057531951369,\n",
       " 2.622025635838369,\n",
       " 2.6219937666489703,\n",
       " 2.621961924337741,\n",
       " 2.621930108859382,\n",
       " 2.6218983201687203,\n",
       " 2.6218665582207086,\n",
       " 2.6218348229704302,\n",
       " 2.6218031143730918,\n",
       " 2.6217714323840307,\n",
       " 2.6217397769587025,\n",
       " 2.6217081480526927,\n",
       " 2.6216765456217126,\n",
       " 2.6216449696215927,\n",
       " 2.6216134200082903,\n",
       " 2.621581896737886,\n",
       " 2.621550399766579,\n",
       " 2.6215189290506964,\n",
       " 2.6214874845466842,\n",
       " 2.6214560662111057,\n",
       " 2.6214246740006515,\n",
       " 2.6213933078721277,\n",
       " 2.621361967782462,\n",
       " 2.621330653688703,\n",
       " 2.621299365548011,\n",
       " 2.6212681033176737,\n",
       " 2.6212368669550905,\n",
       " 2.6212056564177804,\n",
       " 2.6211744716633802,\n",
       " 2.62114331264964,\n",
       " 2.621112179334429,\n",
       " 2.621081071675731,\n",
       " 2.6210499896316435,\n",
       " 2.6210189331603795,\n",
       " 2.6209879022202704,\n",
       " 2.6209568967697536,\n",
       " 2.620925916767383,\n",
       " 2.62089496217183,\n",
       " 2.6208640329418724,\n",
       " 2.620833129036402,\n",
       " 2.6208022504144237,\n",
       " 2.620771397035052,\n",
       " 2.6207405688575114,\n",
       " 2.6207097658411387,\n",
       " 2.62067898794538,\n",
       " 2.62064823512979,\n",
       " 2.6206175073540328,\n",
       " 2.6205868045778837,\n",
       " 2.6205561267612203,\n",
       " 2.620525473864034,\n",
       " 2.620494845846422,\n",
       " 2.6204642426685867,\n",
       " 2.620433664290837,\n",
       " 2.6204031106735926,\n",
       " 2.6203725817773718,\n",
       " 2.6203420775628046,\n",
       " 2.6203115979906224,\n",
       " 2.6202811430216633,\n",
       " 2.620250712616869,\n",
       " 2.6202203067372833,\n",
       " 2.6201899253440555,\n",
       " 2.620159568398437,\n",
       " 2.620129235861782,\n",
       " 2.6200989276955484,\n",
       " 2.6200686438612935,\n",
       " 2.620038384320677,\n",
       " 2.620008149035461,\n",
       " 2.619977937967508,\n",
       " 2.6199477510787785,\n",
       " 2.6199175883313357,\n",
       " 2.619887449687343,\n",
       " 2.6198573351090593,\n",
       " 2.619827244558846,\n",
       " 2.619797177999162,\n",
       " 2.619767135392562,\n",
       " 2.619737116701704,\n",
       " 2.619707121889336,\n",
       " 2.619677150918312,\n",
       " 2.6196472037515734,\n",
       " 2.6196172803521662,\n",
       " 2.6195873806832255,\n",
       " 2.6195575047079878,\n",
       " 2.6195276523897815,\n",
       " 2.6194978236920283,\n",
       " 2.619468018578253,\n",
       " 2.619438237012063,\n",
       " 2.619408478957168,\n",
       " 2.619378744377368,\n",
       " 2.619349033236557,\n",
       " 2.619319345498723,\n",
       " 2.6192896811279445,\n",
       " 2.6192600400883927,\n",
       " 2.6192304223443297,\n",
       " 2.619200827860115,\n",
       " 2.6191712566001915,\n",
       " 2.6191417085290967,\n",
       " 2.6191121836114593,\n",
       " 2.6190826818119968,\n",
       " 2.6190532030955165,\n",
       " 2.619023747426917,\n",
       " 2.6189943147711867,\n",
       " 2.6189649050933985,\n",
       " 2.618935518358718,\n",
       " 2.6189061545323984,\n",
       " 2.6188768135797797,\n",
       " 2.6188474954662917,\n",
       " 2.618818200157448,\n",
       " 2.6187889276188527,\n",
       " 2.6187596778161963,\n",
       " 2.6187304507152547,\n",
       " 2.618701246281888,\n",
       " 2.618672064482047,\n",
       " 2.618642905281763,\n",
       " 2.6186137686471556,\n",
       " 2.6185846545444305,\n",
       " 2.618555562939872,\n",
       " 2.6185264937998554,\n",
       " 2.6184974470908386,\n",
       " 2.618468422779359,\n",
       " 2.618439420832042,\n",
       " 2.6184104412155946,\n",
       " 2.6183814838968056,\n",
       " 2.618352548842548,\n",
       " 2.6183236360197766,\n",
       " 2.618294745395527,\n",
       " 2.6182658769369183,\n",
       " 2.618237030611151,\n",
       " 2.6182082063855043,\n",
       " 2.61817940422734,\n",
       " 2.6181506241041,\n",
       " 2.6181218659833077]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costo_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "32ef5c6f-0189-4868-8922-21e153dbc225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data      = pd.read_csv( 'subset_1000.csv' )\n",
    "pred_vars = [ 'right', 'wrong', 'bias', 't' ]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( data[ pred_vars ], \n",
    "                                                     data[ 'p' ], \n",
    "                                                     test_size    = 0.30,\n",
    "                                                     random_state = 2023 )\n",
    "\n",
    "delta = X_train[ 't' ].values\n",
    "X_train = X_train.drop( columns = [ 't' ] )\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values\n",
    "X_test = X_test.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "# X_train = X_train.T\n",
    "Y_train = Y_train.reshape( 700, )\n",
    "delta = delta.reshape( 700, )\n",
    "\n",
    "# X_test = X_test.T\n",
    "Y_test = Y_test.reshape( 300, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c2b20abb-f151-434b-ad23-5a6c1a5228bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700,)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e0626167-30c2-419e-97b6-086cee73b58d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.95120453, -0.15090183,  0.58920047])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the parameters for optimization\n",
    "D = 700  # Number of data instances\n",
    "alpha = 0.5\n",
    "lambda_ = 0.1\n",
    "eta = 0.01\n",
    "theta = np.random.randn(3)  # Initial theta values for a 3-feature model\n",
    "\n",
    "# Using the optimize_theta function\n",
    "optimized_theta = optimize_theta_adagrad(D, alpha, lambda_, eta, theta, X_train, Y_train, delta )\n",
    "optimized_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc9c6a-9394-4dfd-9620-468bd23a04bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
