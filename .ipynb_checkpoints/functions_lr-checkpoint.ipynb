{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed19c81-07ab-4501-a8cb-9fecb8092e50",
   "metadata": {},
   "source": [
    "# FUNCTIONS FOR LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317f810-8267-4982-9a7a-b94c69c5c264",
   "metadata": {},
   "source": [
    "## 1. Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ab5075-8b38-4655-a8b7-0ec50ec0f71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f24b0bc0-205d-4966-95d5-953c5ce6244a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data        = pd.read_csv( 'subset_1000.csv' )\n",
    "dummies     = pd.get_dummies( data[ 'lexeme' ], prefix = 'cat', dtype=float )\n",
    "dummies_col = dummies.columns.to_list()\n",
    "df          = pd.concat( [ data, dummies ], axis = 1 )\n",
    "\n",
    "pred_vars = [ 'right', 'wrong', 'bias' ]\n",
    "dummies_  = dummies_col + pred_vars\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( df[ dummies_ ], \n",
    "                                                     df[ 'p' ], \n",
    "                                                     test_size    = 0.30,\n",
    "                                                     random_state = 2023 )\n",
    "\n",
    "X_train = X_train.values\n",
    "Y_train = Y_train.values\n",
    "X_test = X_test.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "X_train = X_train.T\n",
    "Y_train = Y_train.reshape( 1, X_train.shape[ 1 ] )\n",
    "\n",
    "X_test = X_test.T\n",
    "Y_test = Y_test.reshape( 1, X_test.shape[ 1 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931628c-202c-4deb-9520-97e218e9a7d1",
   "metadata": {},
   "source": [
    "## 2. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ce4d2c-cdd6-4d3e-bafd-0c1609e262dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid( x ):\n",
    "    \n",
    "    '''\n",
    "    Objetivo:\n",
    "    \n",
    "        - Pasar la combinación lineal de características\n",
    "          ponderadas (x) a través de la función sigmoide\n",
    "          para obtener una predicción probabilística.\n",
    "          \n",
    "    Input:\n",
    "    \n",
    "        - x: combinación lineal de características ponderadas\n",
    "        \n",
    "    Output:\n",
    "    \n",
    "        - Predicción probabilística\n",
    "    '''\n",
    "    \n",
    "    a = 1 / ( 1 + np.exp( -x ) )\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142e5fd9-901e-4aad-81ce-e6239834011c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_cost_function( m, a, y_train ):\n",
    "    \n",
    "    '''\n",
    "    Objetivo:\n",
    "    \n",
    "        - Se trata de la función de costo de la una \n",
    "          regresión logística.\n",
    "          \n",
    "    Input:\n",
    "    \n",
    "        - m       : número de filas de la la matriz \n",
    "                    x_train\n",
    "        - a       : predicción probabilística resultado\n",
    "                    de la función sigmoide\n",
    "        - y_train : array con los valores target para el\n",
    "                    conjunto de entrenamiento\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "        Valor de costo de una regresión logística\n",
    "    '''\n",
    "    \n",
    "    cost = -( 1 / m ) * np.sum( y_train * np.log( a ) + ( 1 - y_train ) * np.log( 1 - a ) )\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e28479-1c5e-4a89-9944-22b130c6b470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent( m, a, b, w, x_train, y_train, learning_rate ):\n",
    "    \n",
    "    '''\n",
    "    Objetivo:\n",
    "    \n",
    "        - Se trata de la función para implementar el \n",
    "          descenso del gradiente\n",
    "          \n",
    "    Input:\n",
    "    \n",
    "        - m            : número de filas de la la matriz \n",
    "                         x_train\n",
    "        - a            : predicción probabilística resultado\n",
    "                         de la función sigmoide\n",
    "        - b            : valor del bias (intercepto)\n",
    "        - w            : valor de los weights (coeficientes)\n",
    "        - x_train      : array con los valores de conjunto de\n",
    "                         entrenamiento\n",
    "        - y_train      : array con los valores target para el\n",
    "                         conjunto de entrenamiento\n",
    "        -learning_rate : velocidad de convergencia (tamaño de \n",
    "                         pasos) para gradient descent\n",
    "                    \n",
    "    Output:\n",
    "    \n",
    "        - Valores actualizados para el bias (b) y los weights\n",
    "          (w) después de cada iteración\n",
    "    '''\n",
    "    \n",
    "    dw = ( 1/m ) * np.dot( a - y_train, x_train.T )\n",
    "    db = ( 1/m ) * np.sum( a - y_train )\n",
    "\n",
    "    w = w - learning_rate * dw.T\n",
    "    b = b - learning_rate * db     \n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef921975-76b8-4369-ac9e-072c4bab4855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_model( x_train, y_train, learning_rate, tol = 1e-7, n_iter = 100 ):\n",
    "    \n",
    "    '''\n",
    "    Objetivo:\n",
    "    \n",
    "        - Implementar un modelo de regresión logística\n",
    "        \n",
    "    Input:\n",
    "    \n",
    "        - x_train       : predictores conjunto de entrenamiento\n",
    "        - y_train       : target del conjunto de entrenamiento\n",
    "        - learning_rate : velocidad de convergencia (tamaño de \n",
    "                          pasos) para gradient descent\n",
    "        - tol           : diferencia mínima tolerable entre los\n",
    "                          valores de costo para considerar que el\n",
    "                          modelo ha convergido\n",
    "        - n_iter        : cada cuántas iteraciones se imprime el\n",
    "                          valor de costo\n",
    "                          \n",
    "    Output: \n",
    "    \n",
    "        - Valor final para el bias (b) y los weights (w) después\n",
    "          de la última iteración. Además, la lista con todos los \n",
    "          valores de costo.\n",
    "    '''\n",
    "    \n",
    "    m = x_train.shape[ 1 ]   # n_rows\n",
    "    n = x_train.shape[ 0 ]   # n_columns\n",
    "    w = np.zeros( ( n, 1 ) ) # weights: matriz vacía. Coeficientes. \n",
    "    b = 0                    # bias. Intercepto\n",
    "    \n",
    "    cost_list = []\n",
    "    prev_cost = float( 'inf' )\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        z = np.dot( w.T, x_train ) + b\n",
    "        a = sigmoid( z )   \n",
    "        \n",
    "        # Cost function\n",
    "        cost = lr_cost_function( m, a, y_train )\n",
    "        \n",
    "        # Gradient descent\n",
    "        w, b = gradient_descent( m, a, b, w, x_train, y_train, learning_rate ) \n",
    "        \n",
    "        # cost_list\n",
    "        cost_list.append( cost )\n",
    "        \n",
    "        if iteration % n_iter == 0:\n",
    "            print( f'En la iteración { iteration } el costo es { cost }' )\n",
    "            \n",
    "        if abs( prev_cost - cost ) < tol:\n",
    "            print( f'Convergencia alcanzada en la iteración { iteration }. Costo: { cost }' )\n",
    "            break\n",
    "\n",
    "        prev_cost = cost\n",
    "        iteration += 1\n",
    "    \n",
    "    return w, b, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f33caad-20dd-4a13-8334-6e8c77653744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def accuracy( x_test, y_test, w, b ):\n",
    "    \n",
    "#     '''\n",
    "#     Objetivo: \n",
    "#         - Evaluar el desempeño del modelo con la métrica\n",
    "#           Accuracy.\n",
    "          \n",
    "#     Input:\n",
    "#         - x_test : predictores del conjunto de entrenamiento\n",
    "#         - y_test : predictores del conjunto de prueba\n",
    "#         - w      : weights resultantes del modelo entrenado\n",
    "#         - b      : bias resultante del modelo entrenado\n",
    "        \n",
    "#     Output:\n",
    "#         - Métrica Accuracy\n",
    "#     '''\n",
    "    \n",
    "#     z   = np.dot( w.T, x_test ) + b\n",
    "#     a   = sigmoid( z )\n",
    "#     a   = a > 0.5\n",
    "#     a   = np.array( a, dtype = 'int64' )\n",
    "#     acc = ( 1 - np.sum( np.absolute( a - y_test ) ) / y_test.shape[ 1 ] ) * 100\n",
    "    \n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4838269-add4-468c-96ba-5017f0af8d5f",
   "metadata": {},
   "source": [
    "## 3. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d533b2-7279-4fa5-9cf4-d8649c6ab37c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la iteración 0 el costo es 0.6931471805599454\n",
      "En la iteración 10000 el costo es 0.342945228952292\n",
      "En la iteración 20000 el costo es 0.3330561260373781\n",
      "En la iteración 30000 el costo es 0.3252557314511962\n",
      "En la iteración 40000 el costo es 0.3191136862168527\n",
      "En la iteración 50000 el costo es 0.3142606542179178\n",
      "En la iteración 60000 el costo es 0.3104032002859797\n",
      "En la iteración 70000 el costo es 0.307322875884503\n",
      "En la iteración 80000 el costo es 0.30486123333537\n",
      "En la iteración 90000 el costo es 0.3028965720595452\n",
      "En la iteración 100000 el costo es 0.3013248617427829\n",
      "En la iteración 110000 el costo es 0.30005361478599096\n",
      "Convergencia alcanzada en la iteración 117580. Costo: 0.29924164787838653\n"
     ]
    }
   ],
   "source": [
    "learning_rate   = 0.0005\n",
    "n_iter          = 10000\n",
    "W, B, cost_list = lr_model( X_train, Y_train, learning_rate = learning_rate, n_iter = n_iter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3641720d-d7ce-4d0f-9d27-aca5315a17ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression Model is 88.28942545333334\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy(X_test, Y_test, W, B)\n",
    "print( f'Accuracy of Logistic Regression Model is { accuracy }' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
